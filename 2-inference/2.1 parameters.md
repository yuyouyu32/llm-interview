# Inference Parameters
> 本章节记录 LLMs 推理参数相关面试QA题。

## 1. Inference Templates

### Q. 你认为好的prompt范式是什么？
> **Company**: 字节 | **Round**: 算法工程师 一面 | **Date**: 2025-08-27 | **Tags**: [Prompt, Few-shot, Chain-of-Thought]

**核心特征**  
一个好的 Prompt 范式，通常需要具备以下几点：  
- **明确性 (Clarity)**：指令清晰、不含歧义，避免模型“胡乱发挥”。  
- **结构化 (Structure)**：有固定的输入/输出格式，便于模型对齐任务模式。  
- **可扩展性 (Scalability)**：能平滑支持 zero-shot、few-shot、甚至 CoT 等增强方式。  
- **上下文利用 (Context Awareness)**：善用提示中的背景信息，避免冗余和无关内容。  

**常见范式**  
- **Instruction-based Prompt**：直接下达任务指令，适合 zero-shot。  
- **Few-shot Prompt**：通过给出多个示例，帮助模型模仿任务模式。  
- **Chain-of-Thought (CoT)**：引导模型逐步推理，提升复杂任务的可解释性与准确率。  
- **角色扮演式 Prompt (Role-playing)**：通过设定身份/语气/风格，引导模型在特定 persona 下完成任务。  
- **模板化 Prompt**：采用标准化格式，如 `输入 → 步骤 → 输出`，提高一致性。  

**优化思路**  
- 保持输入简洁，突出关键信息。  
- 用显式约束（如“只输出 JSON 格式”）减少偏离。  
- 对复杂问题分解，引导逐步思考，而不是“一步到位”。  
- 在实际应用中，可以结合 **系统提示 + 用户提示 + 示例提示**，形成层次化结构。  

<mark>总结：好的 Prompt 范式应当“清晰、结构化、可扩展”，结合 Instruction、Few-shot 和 CoT 等模式，根据任务难度灵活选择，确保模型输出稳定可靠。</mark>



### Q. Qwen3 是如何实现混合思考的？
> **Company**: 字节topseed | **Round**: 算法工程师 一面 | **Date**: 2025-08-04 | **Tags**: [RL, GRPO, MOE]

**核心机制**  
Qwen3 的混合思考（Hybrid Reasoning, 快/慢思考）不是通过修改模型结构实现的，而是**依赖对话模板 (Chat Template) 的控制**来完成的。  
在官方实现中，有一个参数 `enable_thinking`：  
- 当 `enable_thinking=True` 时，模型默认会生成 `<think> ... </think>` 的内部思考内容；  
- 当 `enable_thinking=False` 时，系统会在 `<|im_start|>assistant` 后 **注入一个空白的 `<think></think>`**，告诉模型“思考过程已结束，可以直接回复”。  

这样就实现了**同一个模型在两种模式下切换**：需要时输出思考过程，不需要时直接输出答案。


**为什么用“空白思考注入”？**  
- 如果单纯用 `<think>` token 作为开始标记，那么模型只能依赖外部显式注入来触发思考，无法学会**自主决定何时思考**。  
- Qwen3 的方案是让模型默认会思考，而 “空白注入” 代表提前结束思考。这样模型在训练中能同时学习“完整思考”与“空思考”两种情况。  
- 基于这种机制，就能进一步支持 **软启停**：用户只需在 prompt 中添加 `/think` 或 `/no_think`，模型就能逐轮切换思考模式。  

<mark>Qwen3 通过在对话模板中注入空白 `<think></think>` 的方式来控制混合思考，既保证了模型默认具备思考能力，又允许通过参数或提示实现“快/慢思考”切换。<mark>


### Q. Qwen3-2507 为什么取消了混合思考模式？
> **Company**: 字节topseed | **Round**: 算法工程师 一面 | **Date**: 2025-08-04 | **Tags**: [RL, GRPO, MOE]

阿里在新版 Qwen3-2507 中将“混合思考模式”（Hybrid Mode）拆分为两个明确模型版本：思考（thinking）模式和非思考（non-thinking）模式，其背后有以下几个关键原因：
1. **评测表现提升**  社区反馈显示，在某些任务上（如数学运算或创意写作），关闭思考模式反而效果更佳。Qwen3-235B-A22B 的 non-thinking 模式表现优于之前混合版本，评测成绩显著提升。许多用户直接指出“去掉混合模式后效果更好”。

2. **混合模式带来的性能代价** Reddit 用户分析表示：“混合模式带来的切换逻辑消耗了较大性能成本”。将思考逻辑嵌入单一模型会影响性能和稳定性，因此拆分更易优化。

3. **部署复杂 & 用户偏好不明确** 混合思考虽然灵活，但部分用户偏向稳定选择具体模式——简单问题快速回答，无需思考；复杂任务则开启思考。拆分后更符合 API 驱动场景，也减少部署成本。

4. **存储成本下降而不敏感** 如讨论所说，SSD 容量便宜，存储两个模型（thinking 与 non-thinking）并无太大成本。在效果改善的前提下，拆分是更优选择。

<mark>Qwen3-2507 取消混合思考模式，转为拆分模型，是因为非思考版本在部分任务上效果更优，混合逻辑带来性能开销且部署复杂，拆分能更好匹配不同场景需求。</mark>

## 2. Inference Superparameters

### Q. 解码策略中 Top-k 与 Top-p 有什么区别？二者结合使用时哪个优先？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [解码策略, Top-k, Top-p]

- **Top-k 采样**  
  - **定义**：从概率分布中取前 k 个概率最高的 token，再归一化后随机采样。  
  - **特点**：  
    - 候选集大小固定。  
    - k 小 → 输出更确定但容易单一；k 大 → 输出多样但可能带来噪声。  

- **Top-p (Nucleus Sampling)**  
  - **定义**：选择最小的 token 集合，使累计概率 ≥ p，再在其中采样。  
  - **特点**：  
    - 候选集大小动态调整。  
    - 在分布集中的时候候选数少（更确定），分布分散时候选数多（更多样）。  

- **区别总结**  
  | 策略   | 控制方式      | 候选数 | 优点 | 缺点 |
  |--------|---------------|--------|------|------|
  | Top-k  | 固定数量 k    | 固定   | 简单直观，可控性强 | 不自适应，分布变化时效果差 |
  | Top-p  | 累积概率阈值 p| 动态   | 自适应概率分布，文本更自然 | p 难调，可能包含低概率词 |

- **二者结合使用**  
  - **组合方式**：  
    1. 先用 Top-k 取前 k 个最高概率 token（作为候选池上限）。  
    2. 再在这 k 个 token 中选择累计概率 ≥ p 的最小子集。  
    3. 从该子集采样生成下一个 token。  
  - **优先级**：  
    - **Top-k 是硬上限**，避免极端低概率 token 进入候选池。  
    - **Top-p 是自适应调节**，保证候选概率覆盖度，同时避免候选过多。  
  - **实践经验**：常用设置 k=50, p=0.9 → 既安全又灵活。  

<mark>核心：Top-k 固定候选数量，Top-p 固定概率阈值；Top-k 更可控，Top-p 更自适应。结合使用时 **先 Top-k 再 Top-p**，Top-k 提供安全边界，Top-p 提供动态灵活性。</mark>


### Q. LLMs 的解码策略有哪些？
> **Company**: 京东 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-01 ｜ **Tags**: [LLMs, 解码策略, 推理]

**1. Greedy Search（贪心搜索）**  
- 每一步都选概率最大的 token。  
- **优点**：速度快，实现简单。  
- **缺点**：容易陷入局部最优，生成结果单一。  
- **应用**：要求高效、对多样性要求低的场景。  

**2. Beam Search（束搜索）**  
- 保留 top-k 条候选路径，逐步扩展。  
- **优点**：在一定程度上平衡质量和搜索空间，提升整体连贯性。  
- **缺点**：计算开销大，多样性不足。  
- **应用**：翻译、摘要等需要句子整体质量的任务。  

**3. Sampling（采样类方法）**  
- **Random Sampling**：直接按概率分布采样下一个 token。  
- **Top-k Sampling**：只在概率最高的 k 个 token 中采样。  
- **Top-p (Nucleus) Sampling**：只在累计概率达到 p 的候选集内采样。  
- **优点**：能生成更具多样性和创造性的文本。  
- **缺点**：随机性大，可能生成不稳定。  
- **应用**：对话、创意写作等。  

**4. Temperature Scaling（温度调整）**  
- 在 softmax 前对 logits 除以温度 $T$：  
  - $T < 1$ → 分布更尖锐，模型更“保守”。  
  - $T > 1$ → 分布更平滑，模型更“随机”。  
- 常与 Sampling 结合使用。  

**5. Contrastive Search（对比搜索）**  
- 结合概率最大化和多样性约束，避免模式化和重复生成。  
- **优点**：兼顾流畅性和多样性，在长文本生成中表现较好。  
- **代表应用**：学术写作、长对话。  

**6. Typical Decoding（典型解码）**  
- 选择“信息量接近平均熵”的 token，避免生成过于平庸或过于罕见的词。  
- **优点**：提升生成质量和多样性平衡。  

**7. 总结**  
<mark>LLMs 常见解码策略有 Greedy、Beam、Sampling（Top-k/Top-p）、Temperature、Contrastive、Typical 等。选择策略需在**效率、连贯性、多样性**之间权衡：Greedy/Beam 适合精确任务，Sampling/Temperature 适合创意任务，Contrastive/Typical 更适合长文本生成。</mark>
