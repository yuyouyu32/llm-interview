# Inference Application
> 本章节记录 LLMs 推理应用相关面试QA题。

### 1. LLMs inference speed

### Q. 假设使用 Greedy Search，每次输出 1 个 token 需要 1ms，生成 200 个 token 时总耗时是 >200ms 还是 <200ms 还是 =200ms？为什么？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [解码策略, Greedy Search, KV Cache, 推理性能]

- **情况分析**  
  - 在自回归生成中，每个新 token 的计算依赖于前面所有 token。  
  - 如果没有 **KV Cache**，每次都要重复计算前缀 → 时间复杂度约 $O(n^2)$。  
  - 有了 **KV Cache**，前缀的 key/value 可以缓存，下一个 token 只需要算一次新 step 的 attention → 时间复杂度降为 $O(n)$。  

- **为什么是大于 200ms**  
  - **第 1 个 token**：推理开销较小。  
  - **随着 token 增多**：每一步计算量会递增（即使用 KV Cache，也要和所有历史 token 的 key/value 做 attention 计算）。  
  - 因此生成第 200 个 token 的时间 > 生成第 1 个 token 的时间。  
  - 整体来看，平均下来 **单 token 时间 > 1ms**。  
  - 所以生成 200 个 token 的总时长 **> 200ms**。  

- **结论**  
  - 有 KV Cache：复杂度 $O(n)$，总耗时大于 200ms，但远小于无 KV Cache 的 $O(n^2)$。  
  - 没有 KV Cache：总耗时远大于 200ms，增长更快。  

<mark>核心：生成 200 个 token 的耗时 **大于 200ms**，因为即使有 KV Cache，每步都要和历史 token 做 attention 计算，单 token 时延会随序列长度增加而上升。</mark>


### Q. 如何提升大模型推理的吞吐率？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [吞吐率优化, Flash Attention, GPU 分区]

- **提升吞吐率的常见方法**  
  1. **KV Cache**：缓存历史 Key/Value，避免重复计算前缀，降低复杂度 O(n²) → O(n)。  
  2. **Flash Attention**：优化注意力计算，减少显存占用、提高计算效率。  
  3. **张量并行 / 流水并行 / ZeRO**：分布式切分计算与显存。  
  4. **动态批处理 (Dynamic Batching)**：多个请求合并一起做推理，提升 GPU 利用率。  
  5. **Speculative Decoding**：小模型预测，大模型验证，加快生成。  
   


<mark>核心：吞吐率提升既要算法层（Flash Attention → O(L) 存储、访存友好），也要系统层（GPU 分区与并行调度 → 更高硬件利用率）。</mark>