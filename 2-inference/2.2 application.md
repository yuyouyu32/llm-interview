# Inference Application
> 本章节记录 LLMs 推理应用相关面试QA题。

## 1. LLMs inference speed

### Q. 假设使用 Greedy Search，每次输出 1 个 token 需要 1ms，生成 200 个 token 时总耗时是 >200ms 还是 <200ms 还是 =200ms？为什么？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [解码策略, Greedy Search, KV Cache, 推理性能]

- **情况分析**  
  - 在自回归生成中，每个新 token 的计算依赖于前面所有 token。  
  - 如果没有 **KV Cache**，每次都要重复计算前缀 → 时间复杂度约 $O(n^2)$。  
  - 有了 **KV Cache**，前缀的 key/value 可以缓存，下一个 token 只需要算一次新 step 的 attention → 时间复杂度降为 $O(n)$。  

- **为什么是大于 200ms**  
  - **第 1 个 token**：推理开销较小。  
  - **随着 token 增多**：每一步计算量会递增（即使用 KV Cache，也要和所有历史 token 的 key/value 做 attention 计算）。  
  - 因此生成第 200 个 token 的时间 > 生成第 1 个 token 的时间。  
  - 整体来看，平均下来 **单 token 时间 > 1ms**。  
  - 所以生成 200 个 token 的总时长 **> 200ms**。  

- **结论**  
  - 有 KV Cache：复杂度 $O(n)$，总耗时大于 200ms，但远小于无 KV Cache 的 $O(n^2)$。  
  - 没有 KV Cache：总耗时远大于 200ms，增长更快。  

<mark>核心：生成 200 个 token 的耗时 **大于 200ms**，因为即使有 KV Cache，每步都要和历史 token 做 attention 计算，单 token 时延会随序列长度增加而上升。</mark>


### Q. 如何提升大模型推理的吞吐率？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [吞吐率优化, Flash Attention, GPU 分区]

- **提升吞吐率的常见方法**  
  1. **KV Cache**：缓存历史 Key/Value，避免重复计算前缀，降低复杂度 O(n²) → O(n)。  
  2. **Flash Attention**：优化注意力计算，减少显存占用、提高计算效率。  
  3. **张量并行 / 流水并行 / ZeRO**：分布式切分计算与显存。  
  4. **动态批处理 (Dynamic Batching)**：多个请求合并一起做推理，提升 GPU 利用率。  
  5. **Speculative Decoding**：小模型预测，大模型验证，加快生成。  
   


<mark>核心：吞吐率提升既要算法层（Flash Attention → O(L) 存储、访存友好），也要系统层（GPU 分区与并行调度 → 更高硬件利用率）。</mark>


## 2. COT(Chain of Thought)

### Q. 讲讲大模型的思维链（Chain of Thought, CoT）技术？
> **Company**: 快手 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-01 ｜ **Tags**: [思维链, Chain of Thought, CoT, 多步推理]


**让模型在给出最终答案前，先生成“中间推理步骤/草稿”（rationales），以分解复杂任务（数学、逻辑、多跳问答、代码）并降低一步到位的难度。**

**为什么有效**  
- 将难问题拆成若干可验证的简单子步骤；  
- 通过可见/不可见的“草稿”稳定梯度与搜索过程；  
- 便于后验验证与纠错（对每步做检查，而不是只校验最终答案）。

**常见用法（Prompting）**  
- *Few-shot CoT*：给出少量带推理步骤的示例，诱导模型仿照推理。  
- *Zero-shot CoT*：在提示词加入“let's think step by step”等，触发自发推理。  
- *Self-Consistency*：对同一问题采样多条思维链，**投票**选答案（提高鲁棒性）。  
- *Least-to-Most*：先让模型列子问题与计划，再逐一求解。  
- *Chain-of-Verification (CoVe)*：先产出答案，再生成核查清单逐步自检与修正。

**进阶：把 CoT 和“搜索/工具”结合**  
- *ReAct*：交替“推理（Reason）”与“行动（Act）”，边想边用工具（检索、计算器、代码执行）。  
- *PAL / PoT*：让模型把推理转化为可执行代码，交给解释器跑，返回结果再推理。  
- *Tree/Graph-of-Thought (ToT/GoT)*：把多条思维链视为搜索树/图，**分支探索 + 评分选择**，比单链更稳。  
- *外部知识/检索结合*：检索-读证据-写步骤-再检索，减少“无依据的长篇胡诌”。

**训练范式（从“会想”到“想得对”）**  
- *SFT on Rationales*：用带步骤的数据做监督微调，让模型学会输出步骤。  
- *Process Supervision / PRM（过程奖励模型）*：不是只奖最终对错，而是**对每一步**给奖励/扣分。  
- *Outcome + Process RL*：结合最终答案正确性与步骤质量做强化学习（如基于组内归一化的 GRPO，多采样、拒绝采样筛优）。  
- *DPO/StepDPO*：用偏好比较直接优化（整体/逐步偏好），减少 RL 管道复杂度。  
- *Distillation*：用强模型的思维链教小模型（压缩为“短链/要点链”降低推理成本）。

**部署与工程实践**  
- *何时触发 CoT*：做**难度/不确定性门控**（如对数熵/置信度超阈值才启用），避免对简单题“过度思考”。  
- *采样策略*：CoT 通常需略高温度采样以获得多样链，再用投票/评分挑最佳。  
- *成本控制*：限制链长（max rationale tokens）、分段生成与早停；对常见模式**缓存**中间结论。  
- *可验证性*：对关键步骤引入外部验证（单元测试、约束校验、工具执行、引用证据）。  
- *安全与风格*：系统提示强调“先证据、后结论、不过度奉承、遇不确定要声明”，避免“看似有理的长链幻觉”。

**优缺点与常见坑**  
- **优点**：显著提升复杂推理正确率、可追溯、可验证、便于蒸馏。  
- **缺点**：Token 成本高，易冗长；若缺证据，可能生成“长篇但错误”的链；评测需过程与结果双维度。  
- **缓解**：短链蒸馏、门控触发、工具核验、投票/搜索、过程奖励训练、总结式/要点式 CoT（更短更稳）。

**适用场景**  
- 数学/逻辑/程序推导、多跳问答、流程规划、复杂代码与数据推理、需要可解释与可审计的业务场景。

<mark>思维链让模型“先想清楚再回答”。结合搜索与工具、用过程监督/偏好或 RL 优化步骤质量，并通过门控与投票控制成本，能稳定提升复杂推理的可靠性与可解释性。</mark>

### Q. 思维链咋压缩？
**Company**: 腾讯 ｜ **Round**: WXG青云算法工程师 一面 ｜ **Date**: 2025-09-05 ｜ **Tags**: [思维链, Chain of Thought, CoT, 压缩]

**1. 背景问题**  
- 思维链 (Chain of Thought, CoT) 能提升模型的推理能力，但带来的问题是 **推理过程过长**，增加了推理时延与 token 成本。  
- “思维链压缩”指如何在保持模型推理效果的前提下，减少中间推理步骤或其显式输出。  

**2. 常见压缩方法**  
1. **训练阶段压缩**  
   - **Distillation 蒸馏**：先用长 CoT 教师模型生成详细推理，再让学生模型学习压缩版（short CoT 或直接到答案）。  
   - **Selective CoT**：只保留关键中间步骤，丢弃冗余推理。  
   - **抽象化表达**：将多步推理转为更紧凑的符号/公式表示。  

2. **推理阶段压缩**  
   - **思维链截断**：生成 CoT 到一定长度后强制收束，要求模型输出最终答案。  
   - **答案优先生成**：先预测答案，再回填少量关键推理步骤（减少 token）。  
   - **自适应压缩**：根据问题难度动态决定思维链长短，简单题直接给答案，复杂题才展开。  

3. **表示层压缩**  
   - **隐式 CoT**：不显式输出所有步骤，而是在模型内部通过 latent reasoning 完成推理，只在外部展示简化版链条。  
   - **向量化存储**：将长推理链嵌入为压缩向量，供后续推理使用，而不是逐字保留。  

**3. 效果与权衡**  
- 压缩能显著减少 token 开销和延迟，但可能损失可解释性。  
- 在实际系统中常采用 **混合策略**：训练时蒸馏压缩，推理时根据任务需求动态决定输出多少推理步骤。  

<mark>思维链压缩的核心是通过蒸馏、截断、自适应生成或隐式表征，把冗长的推理过程压缩成更短、更关键的步骤，从而降低推理成本同时保持推理正确性。</mark>