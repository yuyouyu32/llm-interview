# DeepSeek LLMs
> 此章节记录 DeepSeek 系列大模型相关的面试题目（偏宏观，细节算法详见具体章节）

### Q. DeepSeek-R1 相比其他大模型做了哪些优化？
> **Company**: 美团 | **Round**: 大模型算法工程师 一面 | **Date**: 2025-08-31 | **Tags**: [DeepSeek-R1, LLM, 优化]

**1. 训练范式：融入多阶段训练＋Cold-Start 数据**  
- **R1-Zero**：首次提出基于 **纯强化学习（RL）** 的训练，不依赖监督微调（SFT）阶段，模型自发展现强大的推理能力。  
- **DeepSeek-R1**：在 RL 前加入少量高质量的 “cold-start” 数据进行预热，再结合迭代 RL 和监督微调，显著提升模型输出的可读性和用户友好性。  

**2. 推理链强化与思考时间延展**  
- 模型设计鼓励更长的推理链（Chain-of-Thought），即让模型在回答前“多思考”，提高复杂任务的正确性:contentReference[oaicite:2]{index=2}。  
- 在 RL 框架中，还引入类似于 **GRPO（Group Relative Policy Optimization）** 的方法—在同一 Prompt 下生成多响应，通过组内奖励归一化替代 Critic，既节省计算资源，又降低训练方差。

**3. 成本控制 + 开源透明**  
- **高性价比**：号称训练与推理成本远低于行业标杆模型，比如GPT-4 或 OpenAI-o1；部分报告指出其训练成本仅为 GPT-4 的一小部分，推理开销大幅下降。  
- **完全开源**：采用 MIT 许可证，提供模型代码与权重，支持社区复用与商业化。

**4. 精度与推理能力的显著提升**  
- 在多项推理与数学任务中表现优秀，如 AIME、MATH-500、LiveCodeBench、Codeforces 等，优于众多公开开源模型，并与 OpenAI-o1-1217 不相上。  
- 在多语言、专业领域如复杂眼科医学推理上，也展现出优越水平，超越 Gemini、OpenAI-o1 与 o3-mini 等竞争对手。

**5. 支持蒸馏与量化部署**  
- 提供多个 distilled 版本（如 1.5B, 7B, 8B, 14B, 32B, 70B），使用 Qwen 与 Llama 系列模型架构，可用于边缘部署或推理加速。  
- 针对推理部署场景提供量化工具：4-bit 量化几乎不损失性能，适用于常规 GPU；并提出更高效的 3-bit 量化方案（DQ3_K_M）以支持低资源环境部署。

| 优化方向          | 关键特性与优势                                                     |
|------------------|------------------------------------------------------------------|
| 多阶段训练结构      | Cold-Start + RL + SFT 多环节协同优化模型可读性与推理质量                     |
| 推理链与 RL 架构   | 长链思考与 GRPO 带来更强推理能力与训练效率                               |
| 成本与开源策略      | 低成本、MIT 授权、社区友好，有利于广泛传播与二次开发                          |
| 性能对标强模型      | 在数学、代码、医学推理任务中，达到或超越 o1，开源模型中表现领先                        |
| 部署灵活与精度保留  | 支持蒸馏与轻量化量化部署，适配不同算力与场景，不损伤模型能力                   |

<mark>**总结**：DeepSeek-R1 引入 Cold-Start 加 RL、GRPO 策略、推理链增强等创新机制，实现低成本、高性能的推理模型，兼具开源开放与工业部署友好性，是当前 reasoning-oriented LLM 的典型代表。<mark>

### Q. DeepSeek-R1 是怎么训练得到的，给我一个完整的训练流程？
> **Company**: 美团 | **Round**: 大模型算法工程师 一面 | **Date**: 2025-09-03 | **Tags**: [DeepSeek-R1, LLM, 训练流程]

![GRPO](./imgs/DeepSeek-R1.png)

DeepSeek-R1 在 DeepSeek-V3 基础上，采用 **多阶段训练 + 冷启动数据 + GRPO RL + 合成数据 + 蒸馏** 等策略，逐层优化模型的推理能力、表达质量与部署友好性。其性能达到 OpenAI-o1 水平，且训练成本大幅降低。

---

**阶段 1: 初始化 & 冷启动 SFT**  
- 基于 **DeepSeek-V3 Base**（已预训练，在数学、编码等任务上性能优异）作为起点。  
- 使用少量高质量的“Cold-Start”数据（包括推理过程 + 总结），通过 **监督微调（SFT）**，提升可读性与格式一致性。

**阶段 2: 无监督 RL 微调（DeepSeek-R1-Zero）**  
- 不依赖任何监督数据，采用 **GRPO（Group Relative Policy Optimization）** 强化学习方法训练模型。  
  - 奖励设计包括 **准确性奖励**（如数学题答案正确）和 **格式奖励**（如正确输出推理 trace）。  
- 生成推理能力显著增强，但可读性与语言一致性欠佳（如中英混杂）。

**阶段 3: 语言一致性 RL + 合成数据生成**
- 用**DeepSeek-V3 Base + Cold-Start SFT + RORL**对“推理类 Prompts”批量采样，并结合 **规则 + DeepSeek-V3 作为裁判**做 **拒绝采样**，筛出高质量、带完整 CoT 的样本。
- **Non-Reasoning 数据（200K）**：由 **DeepSeek-V3（未走 RL）** 通过 **CoT Prompting** 直接生成。

**阶段 4: 合成数据 SFT**  
- 将上述 800K 条数据用于 **SFT DeepSeek-V3 Base**，训练 2 个 epoch，实现质量与推理能力的双提升。

**阶段 5: 最终强化学习微调（DeepSeek-R1）**  
- 基于 SFT 模型，再次应用 **GRPO RL**：  
  - 推理任务使用规则型奖励（accuracy, formatting）；  
  - 非推理任务引入模型型 reward（helpfulness, harmlessness）。  
- 最终产出 **DeepSeek-R1**，性能与 OpenAI-o1 可比。

**阶段 6: 蒸馏与部署优化**  
- 使用 R1 生成的 800K 合成数据对 Qwen / LLaMA 等架构进行 **蒸馏训练**，生成多个规模模型（1.5B/7B/8B/14B/32B/70B）。  
- 这些 distilled 模型支持资源节省、灵活部署，保持推理性能。

| 阶段 | 方法 | 核心目标 |
|------|------|----------|
| 1 | Cold-Start SFT | 提升可读性与一致格式 |
| 2 | GRPO RL (R1-Zero) | 强化推理能力生成“思考流程” |
| 3 | 探入语言一致性 + 数据合成 | 修正模型语言混杂，丰富数据 |
| 4 | 合成数据 SFT | 平衡推理能力与语言流畅性 |
| 5 | GRPO RL with mixed rewards | 继续强化推理，同时兼顾帮助性与无害性 |
| 6 | 蒸馏 | 产出多尺寸模型，适配不同部署需求 |

<mark>**总结**：DeepSeek-R1 的训练是一个由浅入深的多阶段策略：从少量冷启动数据微调，到“无监督自我强化训练”，再结合语言一致性优化与合成大规模数据，最后通过蒸馏实现高性能模型家族。其思维链 自发增强 + 可读性优化 和 强推理能力 的融合，是其对标 OpenAI-o1 的关键所在。<mark>
