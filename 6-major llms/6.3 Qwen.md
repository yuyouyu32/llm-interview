# Qwen LLMs
> 此章节记录 Qwen 系列大模型相关的面试题目（偏宏观，细节算法详见具体章节）

### Q. Qwen系列的LLMs和传统的transformer模型架构上有什么区别？
> **Company**: 美团 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-05 ｜ **Tags**: [Qwen, 大模型, 架构]

**1. 背景**  
- 传统 Transformer 架构：标准的 Encoder-Decoder 或 Decoder-only 结构，核心是多头注意力 (MHA)、前馈网络 (FFN)、残差连接和 LayerNorm。  
- Qwen 系列：阿里提出的新一代大模型，在保持 Transformer 主干的同时，针对训练稳定性、推理效率和多语言能力做了优化。  

**2. 主要差异点**  
1. **Attention 机制改进**  
   - 传统：标准 MHA，复杂度随序列长度二次增长。  
   - Qwen：引入 **Group Query Attention (GQA)** / **Multi-Query Attention (MQA)**，减少 KV 投影数，提高推理效率和显存利用。  

2. **位置编码 (Positional Encoding)**  
   - 传统：绝对位置编码或 RoPE。  
   - Qwen：采用 **改进版 RoPE**（旋转位置编码）并支持**动态扩展上下文长度**，在长文本场景下更稳定。  

3. **归一化方式**  
   - 传统：常用 Post-LN 或 Pre-LN。  
   - Qwen：采用 **RMSNorm**，在大规模训练中更稳定，减少梯度爆炸风险。  

4. **激活函数**  
   - 传统：常用 ReLU/GELU。  
   - Qwen：采用 **SwiGLU**，在保留表达力的同时降低计算开销。  

5. **多语言与多模态优化**  
   - Qwen 模型在训练阶段对 **中文、英文等多语言语料**做特别优化，embedding 处理更适合 CJK（中日韩）字符。  
   - Qwen-VL / Qwen2-VL 进一步在图像-文本对齐上增加专门的投影层，增强多模态能力。  

**3. 整体架构优势**  
- 更高效：通过 GQA/MQA 在推理时减少计算和显存需求。  
- 更稳定：RMSNorm + SwiGLU 改善大规模训练稳定性。  
- 更灵活：改进 RoPE 支持长上下文扩展。  
- 更实用：专门优化多语言和多模态场景。  

<mark>Qwen 系列在 Transformer 基础上做了“轻改”：用 GQA/MQA 优化注意力，RMSNorm+SwiGLU 保证训练稳定，改进 RoPE 支持长上下文，并针对多语言/多模态增强，整体比传统架构更高效、更实用。</mark>


### Q. Qwen 系列大模型的迭代历程？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01 ｜ **Tags**: [Qwen, 大模型, 迭代]


- **Qwen1.5（2024 年初）**  
  - **架构**：纯 Decoder，首次引入 **GQA（分组查询注意力）**，提升注意力效率；MoE 采用“共享专家 + 专属专家”混合。  
  - **改进点**：  
    - 专家负载不均匀、利用率不高。  
    - 上下文窗口 32K，相比 GPT-4-128K 较弱。  
  - **训练数据**：18T tokens。  

- **Qwen2.5（2024 年 9 月）**  
  - **架构升级**：  
    - **全系列 GQA**，KV 缓存减少 40%，推理吞吐提升 30%。  
    - **上下文扩展至 128K**，引入 **Dual Chunk Attention (DCA)** 捕获长程依赖。  
    - MoE 细粒度化，引入任务感知的门控网络。  
  - **训练策略**：三阶段预训练（通用→注入数学/代码→NTK 扩展），并加入 **两阶段 RLHF（离线 DPO + 在线奖励学习）**。  
  - **效果**：在 72B 模型上超越 Llama3-70B。  

- **Qwen3（2025 年 4 月）**  
  - **架构突破**：  
    - **QK-Norm** 替代 QKV-bias，缓解注意力头标量差异问题，提升训练稳定性。  
    - **MoE 专家独立化**，引入 **Global-Batch Load Balancing Loss**，均衡专家负载。  
    - 支持 **动态思维模式切换（深度推理 ↔ 高效响应）**。  
  - **训练规模**：36T tokens（2.5 的 2 倍），覆盖 119 种语言。  
  - **模型规模**：推出 Qwen3-235B（激活 22B）、Qwen3-Coder-480B（专精代码）。  
  - **能力提升**：数学推理 +30%，代码生成准确率 +25%；上下文扩展到 **1M tokens**（YaRN）。  

- **Qwen3-2507（2025 年 7 月）**  
  - **架构分化**：首次 **双模型独立部署**：  
    - Thinking 版：深度逻辑推理，适合数学/科学/推理。  
    - Non-thinking 版：FP8 量化，速度优先，适合信息提取与通用问答。  
  - **上下文长度**：支持 **256K**，超越 Claude3 (200K)。  
  - **垂直模型**：Qwen3-Coder（代码任务解题率超 DeepSeek-V3）、Qwen-MT（高精度翻译）。  
  - **对齐能力**：Arena-Hard 评测全面超越 Claude Opus4，人类偏好对齐显著提升。  

| 技术点         | Qwen1.5 (2024.初) | Qwen2.5 (2024.9) | Qwen3 (2025.4)        | Qwen3-2507 (2025.7) |
|----------------|------------------|------------------|-----------------------|---------------------|
| **注意力机制** | GQA 部分应用     | 全系列 GQA       | QK-Norm 稳定训练      | 继承 QK-Norm        |
| **上下文长度** | 32K              | 128K (DCA)       | 1M (YaRN 扩展)        | 256K                |
| **MoE 架构**   | 共享专家         | 细粒度专家       | 独立专家 + 均衡损失   | 独立专家 + 均衡损失 |
| **思维模式**   | 无               | 无               | 动态切换（深推/快应） | 双模型分立（Thinking / Non-thinking） |
| **训练数据量** | 18T tokens       | 18T tokens       | 36T tokens            | 未公布（增量）      |

### Q. 训练Qwen 选择的训练阶段和Loss函数是如何决定的？
> **Company**: 淘天 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [Qwen, 大模型, 训练阶段, Loss函数]


1. **训练阶段设计**  
   - **预训练 (Pre-training)**：  
     - 大规模无监督文本（网页、书籍、代码等）。  
     - 目标是学习通用语言建模能力。  
   - **监督微调 (SFT, Supervised Fine-tuning)**：  
     - 人工或合成的高质量指令–回答对。  
     - 让模型学会遵循指令，保证基本可用性。  
   - **对齐训练 (Post-training)**：  
     - RLHF、DPO 等，基于人类或 AI 偏好优化。  
     - 提升回答的安全性、礼貌性、偏好一致性。  
   - **扩展训练**（Qwen 系列常见）：  
     - 长上下文扩展 (128K, 1M tokens)，采用 **NTK scaling / YaRN** 策略。  
     - 特定领域任务（数学、代码、翻译）再进行额外阶段的 domain SFT + RL。  

2. **Loss 函数的选择与作用**  
   - **语言建模 Loss (CE Loss)**：  
     - 预训练 & SFT 的核心损失函数。  
     - 公式：  
       $$
       \mathcal{L}_{\text{CE}} = - \sum_{t} \log P_\theta(y_t | y_{<t})
       $$  
     - 保证模型能学到 token 预测能力。  
   - **RLHF 中的 PPO Loss**：  
     - 最大化奖励模型打分，带 KL 正则：  
       $$
       \mathcal{L}_{\text{PPO}} = \mathbb{E}_t \big[\min(r_t(\theta) A_t,\; \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon) A_t ) \big]
       $$  
   - **DPO Loss**（Qwen3/3.5 中常用）：  
     - 基于偏好对的直接优化，无需奖励模型：  
       $$
       \mathcal{L}_{\text{DPO}} = - \mathbb{E}_{(x, y^+, y^-)} \log \sigma\big(\beta (\log \pi_\theta(y^+|x) - \log \pi_\theta(y^-|x))\big)
       $$  
   - **特殊任务 Loss**：  
     - 代码任务：可能加入 AST 对齐 Loss 或 pass@k。  
     - 翻译/多语言任务：可能加入对比学习 Loss。  
     - 长上下文：可能加入位置对比 Loss（帮助模型学习远距离 token 关系）。  

3. **为什么要分阶段 + 多 Loss 结合？**  
   - **分阶段**：降低训练难度，从“学语言”到“学指令”再到“学人类偏好”。  
   - **多 Loss**：保证不同能力兼顾，例如 CE 保证语言流畅性，RLHF/DPO 提升对齐性，特殊 Loss 提升领域能力。  

**一句话总结：**  
<mark>Qwen 的训练采用“预训练 → SFT → Post-training → 领域扩展”的分阶段流程，Loss 函数从通用的交叉熵到 RLHF/DPO，再到领域专用 Loss 逐步叠加，目的是在通用性、可用性、对齐性和专业性之间找到平衡。</mark>

### Q. Qwen是怎么做长度外推的？
> **Company**: 美团 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-05 ｜ **Tags**: [Qwen, 大模型, 长度外推]

**1. 背景问题**  
- Transformer 的注意力机制依赖位置编码，常见的 **RoPE (Rotary Position Embedding)** 在训练过的最大长度以外，角度增长过快，会导致模型在超长文本时失真。  
- 所以，想在推理时用更长上下文，就需要 **长度外推 (Length Extrapolation)** 技术。  

**2. Qwen 的核心方法**  
1. **RoPE 缩放 (Scaling RoPE)**  
   - 在标准 RoPE 里，token 的位置 $n$ 会映射到旋转角度：  
     $\theta = n / 10000^{2i/d}$  
   - Qwen 在此基础上加入**缩放因子 $s$**，把推理时的大位置 $n'$ 压缩到训练区间：  
     $\theta' = (n' / s) / 10000^{2i/d}$  
   - 相当于把“超长文本的坐标”映射回“训练过的坐标范围”。  

2. **NTK-aware 插值 (NTK Scaling)**  
   - 简单缩放可能会丢失细粒度区分度，Qwen 借鉴 NTK 理论，引入一种**非线性映射**：  
     $n' \to f(n') = n'^{\alpha}$，其中 $\alpha \in (0,1)$  
   - 这样长距离 token 的角度不会增长得过快，既保留相对顺序，又避免失真。  

3. **训练-推理解耦**  
   - 训练时只需在 2k~4k 的上下文长度范围内，显著降低训练成本。  
   - 推理时通过 **Scaled RoPE + NTK-aware 插值**，就能把窗口扩展到 32k、128k 甚至更大。  

**3. 效果与优势**  
- **稳定性**：在长文本生成时，困惑度 (PPL) 曲线更加平滑，不会出现突然劣化。  
- **低成本**：不用训练时就塞入几十万 token，依赖数学映射即可。  
- **泛化性**：在代码补全、长文档问答等任务上，Qwen 能够较好地维持一致性。  

**4. 和传统方法的对比**  
- **原始 RoPE**：超过训练长度就迅速失效。  
- **ALiBi (线性偏置)**：更偏向单调衰减，但缺少周期性表达。  
- **Qwen 的 RoPE Scaling**：保留了旋转位置编码的优势，同时通过缩放与插值，兼顾长程稳定性。  

<mark>Qwen 的长度外推并不是直接“硬训练长序列”，而是通过对 RoPE 做缩放和 NTK-aware 插值，把长序列位置映射回训练过的范围，从而实现低成本、稳定的超长上下文能力。</mark>


### Q. Qwen 和 Deepseek有什么区别？
**Company**: 腾讯 ｜ **Round**: TEG大模型算法工程师 一面 ｜ **Date**: 2025-09-10 ｜ **Tags**: [Qwen, DeepSeek, 大模型]

**1. Qwen 的核心创新点**  
- **Length Extrapolation / 长上下文外推**：在推理阶段使用位置编码缩放与插值映射，使模型在处理远超训练长度的上下文时依然稳定，例如 Qwen2.5-1M 支持百万级 token。  
- **多模态与大规模预训练**：覆盖多语种、视觉、音频等模态，预训练数据规模大幅扩展，增强通用性与跨任务能力。  
- **推理框架优化**：引入稀疏注意力、分块 prefill、kernel 优化与调度优化，确保长文本任务下的推理效率。  
- **架构灵活性**：既有稠密模型，也有 MoE 模型，满足不同资源环境的需求。

**2. DeepSeek 的核心创新点**  
- **高效成本控制**：通过硬件协同设计与工程优化，实现大规模训练与推理的低成本。  
- **MoE 稀疏架构**：仅激活部分专家，显著减少计算量与内存开销。  
- **Multi-Head Latent Attention (MLA)**：低秩压缩注意力机制，降低计算负担同时保持建模能力。  
- **推理与数学优化**：推出 DeepSeek-Prover 等专门用于数学推理和形式化证明的模型，强化逻辑与结构化任务表现。  
- **开放性与可扩展性**：强调开放权重与许可，并通过原生稀疏注意力等机制提升长上下文效率。

**3. 核心创新对比总结**

| 创新维度 | Qwen | DeepSeek |
|---|---|---|
| 长上下文支持 | 依赖长度外推（RoPE 缩放 + 插值），训练短序列，推理时扩展至百万级上下文 | 稀疏注意力与工程优化，原生支持更大上下文但无显式外推机制 |
| 架构设计 | 提供稠密与 MoE 并存 | 核心是 MoE 与 MLA，专注稀疏性和高效性 |
| 任务侧重 | 通用性更强，覆盖对话、代码、写作、长文档 | 在数学、逻辑推理、形式化证明上表现突出 |
| 成本优化 | 通过外推降低训练成本 | 硬件共设计 + 稀疏计算，显著降低大规模训练推理成本 |

<mark>Qwen 的核心创新在于推理阶段的长度外推技术，实现低成本的超长上下文处理；而 DeepSeek 的核心创新在于稀疏架构与硬件协同优化，用更少资源换取更强的推理与数学逻辑能力。</mark>
