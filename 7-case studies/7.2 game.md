# Game Case Questions
> 本章记录面试中和游戏相关的场景题


### Q. 假设有一个数据表有用户过去的游戏记录，所有的feature，那我应该用transformer、RNN、LSTM哪个来建模？
> **Company**: 途游游戏 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-03-12 ｜ **Tags**: [Transformer, RNN, LSTM, 游戏]

**1. 背景问题**  
- 用户过去的游戏记录天然是一个 **序列数据**（时间顺序的操作、对局特征、留存行为）。  
- 要建模这种数据，常见的序列模型包括 RNN、LSTM、Transformer。  
- 核心在于：如何平衡 **长程依赖捕捉能力**、**计算开销** 与 **样本量**。

**2. RNN**  
- **特点**：逐步递归更新隐藏状态，能处理序列，但长序列存在梯度消失/爆炸。  
- **优点**：结构简单，计算成本低。  
- **缺点**：对长依赖建模弱，难捕捉复杂用户行为模式。  
- **适用场景**：数据量较小，序列较短的任务。  

**3. LSTM**  
- **特点**：引入门控机制（输入门/遗忘门/输出门），缓解梯度消失问题。  
- **优点**：能捕捉更长的时间依赖，效果比 RNN 稳定。  
- **缺点**：计算开销比 RNN 大，对极长序列仍然有限。  
- **适用场景**：中等规模数据，序列长度在几十到几百步。  

**4. Transformer**  
- **特点**：基于自注意力 (Self-Attention)，并行计算，能捕捉任意位置的依赖。  
- **优点**：对长序列建模能力强，能利用大规模数据学到复杂模式。  
- **缺点**：对小数据集容易过拟合；计算和显存开销大。  
- **适用场景**：数据规模大、序列长（如数百上千步）、需要复杂交互关系的场景。  

**5. 工程实践建议**  
- 如果是 **小规模游戏数据**（例如一两万用户、几十条记录）：优先 **LSTM**，稳健且不易过拟合。  
- 如果是 **中等规模**（百万级用户，记录在百步内）：LSTM 或轻量 Transformer（如 Transformer Encoder with pooling）。  
- 如果是 **大规模行为序列**（上亿条日志，长序列 >1000）：Transformer 优势明显，可以结合稀疏注意力、分块建模。  
- 实际系统常结合 **特征工程 + Transformer Encoder**，在表格特征与序列特征混合场景中表现最好。  

<mark>短序列小数据用 LSTM，中长序列大规模数据用 Transformer，而 RNN 已较少单独使用，更多是教学或轻量场景。</mark>

### Q. 如果游戏要给用户推荐3个装备，这装备需要打折促销，怎么去建模（从算法和模型的角度来分析）？
> **Company**: 途游游戏 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-03-12 ｜ **Tags**: [推荐系统, 装备推荐, 促销]

**1. 背景问题**  
- 游戏中的装备推荐既要考虑 **用户兴趣/需求**，又要考虑 **运营目标**（打折促销）。  
- 本质上是一个 **推荐系统 + 策略约束优化** 问题：既要预测用户购买/点击概率，也要在候选中选出最合适的 3 个促销装备。

**2. 算法层面**  
1. **候选生成 (Candidate Generation)**  
   - 基于协同过滤 / 召回模型（如 embedding 检索），找出用户可能感兴趣的装备集合。  
   - 输入特征：用户历史行为（使用过的装备、战斗数据）、用户画像（等级、活跃度）、装备特征（类别、价格、折扣信息）。  

2. **排序模型 (Ranking)**  
   - 使用监督学习模型预测用户对每件装备的 **购买/点击概率 (CTR/CVR)**。  
   - 模型选择：GBDT、DNN、Wide&Deep、DIN（注意力机制建模用户与装备交互）。  
   - 特征工程：  
     - 用户特征（等级、付费习惯）  
     - 装备特征（基础属性、稀有度、价格、折扣率）  
     - 用户-装备交互特征（历史相似装备的购买率、使用时长）。  

3. **促销约束与优化**  
   - 在推荐 3 个装备时加入约束：必须是打折装备。  
   - 可建模为 **Top-K Ranking with Constraint**：  
     - 先过滤掉非促销装备。  
     - 对剩余候选用排序模型预测兴趣分数，取前 3。  
   - 进一步可引入 **多目标优化**：  
     - 目标1：提升用户点击/购买概率。  
     - 目标2：满足促销覆盖度（推广指定装备）。  
     - 方法：加权目标函数 / 强化学习策略优化（RL-based re-ranking）。  

**3. 模型层面**  
1. **二阶段推荐架构**  
   - **召回阶段**：Embedding 检索（DSSM/SimCLR-style 表示学习）。  
   - **排序阶段**：CTR/CVR 预测模型（如 DIN/Transformer-based Ranking）。  
   - **重排序 (Re-ranking)**：在满足促销约束的候选中选出 Top-3。  

2. **强化学习方法**  
   - 将推荐建模为序列决策：Agent 选择3个促销装备 → 用户反馈（购买/不购买） → 奖励。  
   - 奖励函数 = 用户满意度指标 + 运营收益指标。  
   - 能动态平衡用户体验与促销目标。  

3. **多任务学习**  
   - 共享底层特征网络，同时预测多个目标：购买概率、使用时长、促销转化率。  
   - 在排序时综合这些指标，提升整体效果。  

<mark>推荐促销装备可建模为“召回 + 排序 + 约束优化”的推荐系统问题，排序模型预测用户兴趣，促销约束保证推荐结果，进阶可用强化学习或多任务学习平衡用户体验与运营目标。</mark>
