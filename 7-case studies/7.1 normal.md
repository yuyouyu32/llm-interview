# Normal LLM Case Studies
> 本章记录面试中通用大模型相关的场景题


### Q. 端到端的大模型和级联模型有什么区别？各自的优缺点是什么？
> **Company**: 字节跳动 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-27 ｜ **Tags**: [端到端, 级联模型, LLMs]

**端到端大模型 (End-to-End LLMs)**  
- **特点**：把输入直接映射到输出，所有子任务由同一个大模型完成。  
- **优点**：  
  - 整体优化，误差不会在多阶段传递和放大。  
  - 使用简单，减少复杂的工程搭建。  
  - 模型有机会自动学到跨任务的全局最优表示。  
- **缺点**：  
  - 需要海量数据和算力支持才能覆盖各种子任务。  
  - 缺乏可解释性，中间过程不可控。  
  - 出错时难以定位和修复。  

**级联模型 (Cascaded Models)**  
- **特点**：把复杂任务拆解为多个子模块/子模型，逐步完成。  
- **优点**：  
  - 每个子模型可以针对特定子任务优化，资源需求更低。  
  - 模块化设计，出错时可定位和单独改进。  
  - 更容易引入专家知识或规则，增强可控性。  
- **缺点**：  
  - 误差会在多阶段间累积，后续模块可能放大前面错误。  
  - 工程复杂度高，维护成本大。  
  - 整体性能可能受限于最弱的子模块。  

**在 LLM 应用中的取舍**  
- **端到端**适合数据和算力充足、目标任务相对单一或闭环（如机器翻译、对话生成）。  
- **级联模型**更适合多步骤、可解释性要求高的应用（如信息抽取 → 推理 → 生成的复杂管线）。  

<mark>总结：端到端模型追求“一体化与全局最优”，但依赖大规模训练资源；级联模型强调“模块化与可控性”，但容易误差传递。实际应用中常见做法是两者结合，例如大模型端到端处理，再辅以规则/子模块校正。</mark>

### Q. 端到端的大模型和级联模型有什么区别？各自的优缺点是什么？（以模态为例）
> **Company**: 字节跳动 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-27 ｜ **Tags**: [端到端, 级联模型, 多模态]

**级联模型（例如语音 → 文本 → 语音）**  
- **流程**：先用 ASR 模块把语音转文本，再由文本模型处理，最后用 TTS 模块合成语音。  
- **优点**：  
  - 各模块相对成熟，可以独立优化和替换。  
  - 中间有文本表示，易于编辑、解释和人工干预。  
  - 对资源不足的场景更友好，可以重用已有 ASR/TTS 技术。  
- **缺点**：  
  - 中间转写会引入误差（ASR 错误 → 传播到后续）。  
  - 延迟更高，端到端调用链长。  
  - 无法直接捕捉语音中携带的韵律、情感等细粒度特征。  

**端到端模型（例如语音 → 语音）**  
- **流程**：直接输入语音，模型输出目标语音，中间不显式依赖文本。  
- **优点**：  
  - 更自然地建模语音到语音的映射，可以保留韵律、情感、语速等信息。  
  - 潜在推理速度更快（无需额外转写/合成）。  
  - 出错点更少，避免级联误差累积。  
- **缺点**：  
  - 需要大规模 paired 语音-语音数据，训练成本高。  
  - 缺乏显式的中间表示，不易调试和解释。  
  - 灵活性差，不便于在中间加规则或知识增强。  

**实际应用取舍**  
- 如果任务需要 **可控性、可编辑性**（如会议转写、字幕生成），往往采用 **级联方案**。  
- 如果任务更追求 **自然性和实时性**（如 AI 同声传译、语音助手对话），则更倾向 **端到端语音-语音** 模式。  

<mark>总结：模态端到端强调“语音直接到语音”，保留更多声学与情感信息；级联方式依赖“语音-文本-语音”，可解释性和可控性更强。二者的选择取决于应用场景是更需要**自然性**还是**可控性**。</mark>


### Q. 如何解决 RAG 系统生成结果幻觉的问题  
> **Company**: 腾讯 ｜ **Round**: 青云四面 ｜ **Date**: 2025-09-25 ｜ **Tags**: [RAG, 幻觉, LLMs]

**1. 模型优化**  
**1.1 领域知识注入**  
- 领域继续预训练（DAPT/CPT）：用企业知识库长语料做继续预训练，提升“看懂检索片段”的能力。  
- 有依据的指令微调（Grounded SFT）：构造 *{问题, 证据片段集合, 仅基于证据的答案, 句级引用}* 数据；加入**强负样本**（无关片段/相似干扰）训练“拒答/不确定”。  
- 适配器/LoRA 多任务：统一训练“检索可读性、引用风格、拒答模板”。

**1.2 引入 CoT 解决结构性幻觉**  
- 计划→生成（Plan-then-Generate）：先生成“步骤/槽位/字段检查清单”，再受约束地填充。  
- 程序化推理（Program-of-Thought）：把推理步骤落在可执行 DSL/工具（SQL、计算器、正则）上，结果再自然语言化。  
- 受约束解码：对结构化输出用 JSON-Schema/CFG 约束，减少凭空字段。  

**1.3 多候选生成与一致性判别**  
- 自洽投票（Self-Consistency/MBR）：生成 K 个候选，基于**证据覆盖率/蕴含分**挑选最佳：  
  $$
  y^*=\arg\max_y\ \text{Support}(y,\mathcal{C})-\lambda\cdot\text{Contradict}(y,\mathcal{C})
  $$  
- 交叉检验：候选间逐句互证/互斥打分，淘汰自相矛盾者。

**2. 优化 RAG 链路**  
**2.1 引入答案解释（可溯源）**  
- 句级引用与片段级高亮（sentence→chunk id），强制“每句有据可依”；无法支撑则**显式拒答/提示缺证**。  
- 抽取式→抽象式混合：先引用要点，再在“只看证据”的前提下改写，降低自由发挥空间。

**2.2 优化 rerank，过滤噪音**  
- 检索：BM25 + 向量检索（多路召回）→ 交叉编码器 rerank（monoT5/双塔+CE）。  
- 片段工程：去重、时间/来源过滤、分块带重叠（防截断）、多跳检索（自举改写查询）。  
- 召回质量门控：设最小相似度阈值，不达标则**触发追问/拒答**而非硬编答案。

**2.3 基础模型增训（弥补领域缺失）**  
- 用领域百科/手册/FAQ 进行继续预训练 + 指令化 SFT，使基座对领域术语、符号、表格更鲁棒。  
- 文档视图增强：训练模型理解表格/图表/代码片段的”证据对齐“指令。

**2.4 Critique Model 在线拦截**  
- 独立批判器（NLI/Fact-Checker）：对生成结果逐句做“证据蕴含/矛盾/缺失”判定，不合格触发**再检索-再生成**或拒答。  
- 规则+学习的双层护栏：黑名单（绝对禁错）、白名单（必引文献）、长度/新颖度/引用密度阈值。

**5. 系统筛选 bad case，用 RL 持续优化**  
- 数据回流：上线日志挖掘**无证据/错证据/过度扩写**样本，构造偏好对（好 vs 坏）。  
- 奖励设计（GRPO/DPO）：  
  - 事实性奖励：句级 NLI 支撑率、引用精确度@句、冲突惩罚。  
  - 行为奖励：拒答得当、追问合理、延迟/长度正则。  
  - 训练目标示意：  
    $$
    \max_\theta\ \mathbb{E}\big[\text{Faithfulness}(y,\mathcal{C})-\beta\ \text{Length}(y)-\gamma\ \text{Latency}\big]
    $$  
- 检索侧 bandit：用多臂老虎机/策略学习动态调权（词法/语义/多跳通道），以**最终事实性**为回报。

**落地检查清单（可直接用）**  
- [ ] 句级引用强制；引用缺失即拒答/追问  
- [ ] 召回混合 + CE rerank + 去重/时效过滤  
- [ ] 多候选 + 支撑率投票（MBR）  
- [ ] 批判器在线拦截 + 失败回路：再检索→再生成  
- [ ] bad case 回流 + DPO/GRPO 强化“有据可依”偏好

<mark>一句话总结：把“有据可依”变成**训练目标、生成约束与上线门控**三位一体：强检索与引用、批判式核验、RL 持续对齐，系统性压缩幻觉空间。</mark>
