# Normal LLM Case Studies
> 本章记录面试中通用大模型相关的场景题

### Q. 请你给我详细计算一个LLM他的参数量，你可以假设各种必要的数值，如词表大小、隐藏层维度、层数等。（再提供一个例子）  
> **Company**: 作业帮 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-10-21 ｜ **Tags**: [参数量计算, LLMs]

思路与公式  
- 记：词表大小 **V**，隐藏维度 **d_model**，前馈维度 **d_ff**，层数 **L**，注意力头数 **H**（每头维度 **d_k = d_model / H**）。  
- 采用常见的 Llama/Transformer 架构（RMSNorm，无偏置），前馈用 **SwiGLU**：MLP 参数约为 `3 * d_model * d_ff`（两路上投 + 一路下投）。  
- 注意力（不含偏置）：  
  - **标准多头注意力（MHA）**：`Q,K,V,O` 共 `4 * d_model^2`。  
  - **分组查询注意力（GQA）**：`Q: d_model^2，K: d_model * (n_kv * d_k)，V: 同K，O: d_model^2`。  
- 每层归一化参数：两处 RMSNorm，各 `d_model`，合计 `2 * d_model`。 
$$
\text{PerLayer} \;\approx\;
\underbrace{4\,d_{\text{model}}^2}_{\text{MHA: }Q,K,V,O}
\;+\;
\underbrace{3\,d_{\text{model}}\,d_{\text{ff}}}_{\text{SwiGLU: 两路上投+一路下投}}
\;+\;
\underbrace{2\,d_{\text{model}}}_{\text{两处RMSNorm}}
$$
若采用 GQA（\(n_{kv}\le H\)）则注意力改写为
$$
\text{Attn}_{\text{GQA}}
=\underbrace{d_{\text{model}}^2}_{Q}
+\underbrace{d_{\text{model}}\,(n_{kv}\cdot d_k)}_{K}
+\underbrace{d_{\text{model}}\,(n_{kv}\cdot d_k)}_{V}
+\underbrace{d_{\text{model}}^2}_{O}
=2\,d_{\text{model}}^2+2\,d_{\text{model}}(n_{kv}\cdot d_k).
$$
- 词嵌入：`V * d_model`；若 **不权重共享（untied）**，输出头再加一份 `V * d_model`；若 **共享（tied）**，不再单独计出头。  
- 最终层 RMSNorm：再加 `d_model`。

$$
\text{Params}_{\text{tied}}
=L\cdot \text{PerLayer}\;+\;\underbrace{V\,d_{\text{model}}}_{\text{Embedding}}\;+\;\underbrace{d_{\text{model}}}_{\text{Final Norm}}
$$


**示例 A（细算）：接近 Llama-7B（MHA 版本，便于与公开 6.7B 量级对齐）**  
设：`V=32000, d_model=4096, d_ff=11008, L=32, H=32`（MHA，K/V 规模与 Q 相同）

1) 单层注意力参数  
`4 * d_model^2 = 4 * 4096 * 4096`  
`4096 * 4096 = 16,777,216`  
`4 * 16,777,216 = 67,108,864`

1) 单层 MLP 参数（SwiGLU）  
`3 * d_model * d_ff = 3 * 4096 * 11008`  
先算 `4096 * 11008 = 45,088,768`  
再乘 3：`135,266,304`

1) 单层 RMSNorm 参数  
`2 * d_model = 8192`

1) 单层总计  
`67,108,864 + 135,266,304 + 8,192 = 202,383,360`

1) L 层 Transformer 堆叠  
`202,383,360 * 32 = 6,476,?`  
逐步：`202,383,360 * 10 = 2,023,833,600`  
`×30 = 6,071,500,800`  
再加 `×2 = 404,766,720`  
合计 `6,476,267,520`

1) 词嵌入 + 最终Norm  
- 词嵌入：`V * d_model = 32000 * 4096 = 131,072,000`  
- 最终 RMSNorm：`4096`

1) 是否权重共享  
- **权重共享（tied）**：总参数  
  `6,476,267,520 + 131,072,000 + 4,096 = 6,607,343,616 ≈ 6.61B`  
- **不共享（untied）**：再加一份输出头 `131,072,000`  
  `6,607,343,616 + 131,072,000 = 6,738,415,616 ≈ 6.74B`  
与常见 “7B≈6.7B” 对齐良好。

注：若该 7B 采用 **GQA（如 n_kv=8）**，K/V 维度缩小，会比上面 MHA 数更小；MHA 的估算更贴近公开 6.7B。

**示例 B（再给一个 13B 级别，MHA）**  
设：`V=32000, d_model=5120, d_ff=13824, L=40, H=40`（MHA）

1) 单层注意力：`4 * d_model^2 = 4 * 5120 * 5120`  
`5120*5120 = 26,214,400` → `×4 = 104,857,600`

1) 单层 MLP：`3 * 5120 * 13824`  
`5120*13824 = 70,778,880` → `×3 = 212,336,640`

1) 单层 Norm：`2 * 5120 = 10,240`

2) 单层合计：  
`104,857,600 + 212,336,640 + 10,240 = 317,204,480`

1) L 层总计：  
`317,204,480 * 40 = 12,688,179,200`

1) 词嵌入 + 最终 Norm：  
- 词嵌入：`32000 * 5120 = 163,840,000`  
- 最终 Norm：`5120`

1) 总参数  
- **tied**：`12,688,179,200 + 163,840,000 + 5,120 = 12,852,024,320 ≈ 12.85B`  
- **untied**：再加 `163,840,000` → `≈ 13.02B`

——

可选：若采用 **GQA**  
将注意力中的 `K/V` 改为 `d_model * (n_kv * d_k)`（其中 `d_k = d_model / H`，`n_kv ≤ H`）。  
例如在示例 A 中若 `n_kv=8`：  
`d_k = 4096/32 = 128`，`n_kv * d_k = 1024`，  
单层注意力从 `4 * d_model^2` 变为 `Q: 4096^2, K: 4096*1024, V: 4096*1024, O: 4096^2`，  
单层注意力合计 `16,777,216 + 4,194,304 + 4,194,304 + 16,777,216 = 41,943,040`，  
比 MHA 的 `67,108,864` 更省参数（也更省算力/显存）。

<mark>总结</mark>  
- 逐层计数：**Attention（Q/K/V/O）+ MLP（3·d_model·d_ff）+ 两处Norm（2·d_model）**，再乘层数 **L**，加上 **词嵌入** 与 **最终Norm**，若 **untied** 再加一份 `V*d_model`。  
- 以 Llama 风格（MHA）计算：  
  7B 级（`4096/11008/32/32000`）≈ **6.7B**；  
  13B 级（`5120/13824/40/32000`）≈ **12.9–13.0B**（视是否 tied）。  
- 采用 **GQA** 时，参数量（尤其 K/V）显著下降；是否 **tied** 会带来约 `V*d_model` 的差异。  



### Q. 端到端的大模型和级联模型有什么区别？各自的优缺点是什么？
> **Company**: 字节跳动 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-27 ｜ **Tags**: [端到端, 级联模型, LLMs]

**端到端大模型 (End-to-End LLMs)**  
- **特点**：把输入直接映射到输出，所有子任务由同一个大模型完成。  
- **优点**：  
  - 整体优化，误差不会在多阶段传递和放大。  
  - 使用简单，减少复杂的工程搭建。  
  - 模型有机会自动学到跨任务的全局最优表示。  
- **缺点**：  
  - 需要海量数据和算力支持才能覆盖各种子任务。  
  - 缺乏可解释性，中间过程不可控。  
  - 出错时难以定位和修复。  

**级联模型 (Cascaded Models)**  
- **特点**：把复杂任务拆解为多个子模块/子模型，逐步完成。  
- **优点**：  
  - 每个子模型可以针对特定子任务优化，资源需求更低。  
  - 模块化设计，出错时可定位和单独改进。  
  - 更容易引入专家知识或规则，增强可控性。  
- **缺点**：  
  - 误差会在多阶段间累积，后续模块可能放大前面错误。  
  - 工程复杂度高，维护成本大。  
  - 整体性能可能受限于最弱的子模块。  

**在 LLM 应用中的取舍**  
- **端到端**适合数据和算力充足、目标任务相对单一或闭环（如机器翻译、对话生成）。  
- **级联模型**更适合多步骤、可解释性要求高的应用（如信息抽取 → 推理 → 生成的复杂管线）。  

<mark>总结：端到端模型追求“一体化与全局最优”，但依赖大规模训练资源；级联模型强调“模块化与可控性”，但容易误差传递。实际应用中常见做法是两者结合，例如大模型端到端处理，再辅以规则/子模块校正。</mark>

### Q. 端到端的大模型和级联模型有什么区别？各自的优缺点是什么？（以模态为例）
> **Company**: 字节跳动 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-27 ｜ **Tags**: [端到端, 级联模型, 多模态]

**级联模型（例如语音 → 文本 → 语音）**  
- **流程**：先用 ASR 模块把语音转文本，再由文本模型处理，最后用 TTS 模块合成语音。  
- **优点**：  
  - 各模块相对成熟，可以独立优化和替换。  
  - 中间有文本表示，易于编辑、解释和人工干预。  
  - 对资源不足的场景更友好，可以重用已有 ASR/TTS 技术。  
- **缺点**：  
  - 中间转写会引入误差（ASR 错误 → 传播到后续）。  
  - 延迟更高，端到端调用链长。  
  - 无法直接捕捉语音中携带的韵律、情感等细粒度特征。  

**端到端模型（例如语音 → 语音）**  
- **流程**：直接输入语音，模型输出目标语音，中间不显式依赖文本。  
- **优点**：  
  - 更自然地建模语音到语音的映射，可以保留韵律、情感、语速等信息。  
  - 潜在推理速度更快（无需额外转写/合成）。  
  - 出错点更少，避免级联误差累积。  
- **缺点**：  
  - 需要大规模 paired 语音-语音数据，训练成本高。  
  - 缺乏显式的中间表示，不易调试和解释。  
  - 灵活性差，不便于在中间加规则或知识增强。  

**实际应用取舍**  
- 如果任务需要 **可控性、可编辑性**（如会议转写、字幕生成），往往采用 **级联方案**。  
- 如果任务更追求 **自然性和实时性**（如 AI 同声传译、语音助手对话），则更倾向 **端到端语音-语音** 模式。  

<mark>总结：模态端到端强调“语音直接到语音”，保留更多声学与情感信息；级联方式依赖“语音-文本-语音”，可解释性和可控性更强。二者的选择取决于应用场景是更需要**自然性**还是**可控性**。</mark>


### Q. 如何解决 RAG 系统生成结果幻觉的问题  
> **Company**: 腾讯 ｜ **Round**: 青云四面 ｜ **Date**: 2025-09-25 ｜ **Tags**: [RAG, 幻觉, LLMs]

**1. 模型优化**  
**1.1 领域知识注入**  
- 领域继续预训练（DAPT/CPT）：用企业知识库长语料做继续预训练，提升“看懂检索片段”的能力。  
- 有依据的指令微调（Grounded SFT）：构造 *{问题, 证据片段集合, 仅基于证据的答案, 句级引用}* 数据；加入**强负样本**（无关片段/相似干扰）训练“拒答/不确定”。  
- 适配器/LoRA 多任务：统一训练“检索可读性、引用风格、拒答模板”。

**1.2 引入 CoT 解决结构性幻觉**  
- 计划→生成（Plan-then-Generate）：先生成“步骤/槽位/字段检查清单”，再受约束地填充。  
- 程序化推理（Program-of-Thought）：把推理步骤落在可执行 DSL/工具（SQL、计算器、正则）上，结果再自然语言化。  
- 受约束解码：对结构化输出用 JSON-Schema/CFG 约束，减少凭空字段。  

**1.3 多候选生成与一致性判别**  
- 自洽投票（Self-Consistency/MBR）：生成 K 个候选，基于**证据覆盖率/蕴含分**挑选最佳：  
  $$
  y^*=\arg\max_y\ \text{Support}(y,\mathcal{C})-\lambda\cdot\text{Contradict}(y,\mathcal{C})
  $$  
- 交叉检验：候选间逐句互证/互斥打分，淘汰自相矛盾者。

**2. 优化 RAG 链路**  
**2.1 引入答案解释（可溯源）**  
- 句级引用与片段级高亮（sentence→chunk id），强制“每句有据可依”；无法支撑则**显式拒答/提示缺证**。  
- 抽取式→抽象式混合：先引用要点，再在“只看证据”的前提下改写，降低自由发挥空间。

**2.2 优化 rerank，过滤噪音**  
- 检索：BM25 + 向量检索（多路召回）→ 交叉编码器 rerank（monoT5/双塔+CE）。  
- 片段工程：去重、时间/来源过滤、分块带重叠（防截断）、多跳检索（自举改写查询）。  
- 召回质量门控：设最小相似度阈值，不达标则**触发追问/拒答**而非硬编答案。

**2.3 基础模型增训（弥补领域缺失）**  
- 用领域百科/手册/FAQ 进行继续预训练 + 指令化 SFT，使基座对领域术语、符号、表格更鲁棒。  
- 文档视图增强：训练模型理解表格/图表/代码片段的”证据对齐“指令。

**2.4 Critique Model 在线拦截**  
- 独立批判器（NLI/Fact-Checker）：对生成结果逐句做“证据蕴含/矛盾/缺失”判定，不合格触发**再检索-再生成**或拒答。  
- 规则+学习的双层护栏：黑名单（绝对禁错）、白名单（必引文献）、长度/新颖度/引用密度阈值。

**5. 系统筛选 bad case，用 RL 持续优化**  
- 数据回流：上线日志挖掘**无证据/错证据/过度扩写**样本，构造偏好对（好 vs 坏）。  
- 奖励设计（GRPO/DPO）：  
  - 事实性奖励：句级 NLI 支撑率、引用精确度@句、冲突惩罚。  
  - 行为奖励：拒答得当、追问合理、延迟/长度正则。  
  - 训练目标示意：  
    $$
    \max_\theta\ \mathbb{E}\big[\text{Faithfulness}(y,\mathcal{C})-\beta\ \text{Length}(y)-\gamma\ \text{Latency}\big]
    $$  
- 检索侧 bandit：用多臂老虎机/策略学习动态调权（词法/语义/多跳通道），以**最终事实性**为回报。

**落地检查清单（可直接用）**  
- [ ] 句级引用强制；引用缺失即拒答/追问  
- [ ] 召回混合 + CE rerank + 去重/时效过滤  
- [ ] 多候选 + 支撑率投票（MBR）  
- [ ] 批判器在线拦截 + 失败回路：再检索→再生成  
- [ ] bad case 回流 + DPO/GRPO 强化“有据可依”偏好

<mark>一句话总结：把“有据可依”变成**训练目标、生成约束与上线门控**三位一体：强检索与引用、批判式核验、RL 持续对齐，系统性压缩幻觉空间。</mark>



### Q. 如果要在中文领域做Instruction Tuning ，需要注意哪些数据问题？
> **Company**: 快手 ｜ **Round**: 算法工程师 二面 ｜ **Date**: 2025-09-28 ｜ **Tags**: [指令微调, 数据质量, 中文]

1. **语料来源与合规性**  
   中文语料多来自互联网、论坛或开放问答平台，因此首要问题是**版权和隐私合规**。必须确保数据来源清晰、授权明确，并对个人信息（如姓名、手机号、身份证号等）进行脱敏。同时要避免政治敏感、医疗处方、金融投资类数据引发的法律与伦理风险，必要时通过敏感词与领域分类模型过滤。

2. **去重与泄漏防控**  
   中文语料存在大量网络转载和复用现象。应使用 n-gram + 向量相似度双重去重，防止语料冗余和模板化现象。此外，还需检测**训练集与评测集泄漏**（如 C-Eval、MMLU-CN、CMMLU 等基准题），以免模型出现“记题”现象。

3. **文本规范化与分词兼容**  
   中文数据存在简繁混用、标点全半角不统一、异体字与表情符号混杂等问题。需进行：  
   - 简繁体统一；  
   - 标点、数字与空格规范化；  
   - 保留合适的表情或转义符号（如“😂”、“❤️”等）；  
   - 保证分词器或 BPE 对中文字符的覆盖度，避免 OOV（Out of Vocabulary）过高。

4. **中英混杂与口语化问题**  
   中文互联网语料中存在大量中英混用、拼音表达（如“nihao”、“gg”）、缩写和方言口语。可通过语种检测与规则清洗标注语言比例，并控制口语化样本比例，使模型既能理解正式中文，也能处理网络语体。

5. **指令与输出匹配（Instruction-Response Alignment）**  
   需要确保每条指令对应合理的回答，且格式统一。例如：
   - 保证输出符合任务类型（生成、分类、抽取）；
   - 明确拒答场景与拒答语气；
   - 指令-答案格式规范（如 Markdown、JSON、代码块）；
   - 对模糊任务（如摘要、翻译）标注风格或篇幅说明。

6. **任务与领域覆盖性**  
   中文任务应覆盖通用问答、写作润色、推理计算、工具使用、生活服务、文化常识等多个方向。尤其应平衡**新闻、教育、娱乐、工具性任务**的分布，防止模型对特定类型（如闲聊或作文）过拟合。

7. **安全与价值观**  
   中文指令调优要特别注意输出的**社会价值观与中立性**。模型应避免产生歧视、极端、政治敏感或地域偏见的回复。  
   可通过构造拒答样本、显式标注“中立/拒绝”行为，提升安全性。

8. **事实性与时效性**  
   中文语料更新速度快，应标注时间戳或来源链接。可引入知识验证模块或基于检索的辅助信号来检测幻觉与虚构信息。

9. **人工与自动混合评审机制**  
   构建过滤 → 自动评分 → 人工复核的多阶段质量流程，尤其在人文或教育类任务中，需人工判断逻辑连贯与礼貌性。

10. **多样性与格式一致性**  
    保持任务与表达多样，避免重复模板指令；但输出需保持结构一致，尤其在需要统一格式（如 JSON）或包含代码片段的任务中。

11. **训练比例与采样策略**  
    中文任务应控制合成与真实数据的比例（例如 8:2），并引入困难样本（hard negatives）与排序偏好数据，以促进模型泛化能力。

<mark>**一句话总结：中文 Instruction Tuning 的核心是“合规先行、清洗去重、格式规范、对齐一致、安全中立”，确保高质量多样语料支撑模型的真实理解与稳健生成。**</mark>


### Q. 有两个推荐模型A和B，离线实验中A的指标更好，但推到线上后A模型的效果比B模型差，分析原因。
> **Company**: 沐瞳 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-19 ｜ **Tags**: [推荐系统, 离线实验, 在线实验, 归因]

1. **离线评估与线上环境不一致**  
   - **数据分布漂移（Data Drift）**：离线测试集通常是历史数据，而线上用户行为实时变化（季节性、热点、活动期等），导致模型泛化失效。  
   - **特征时效性问题**：A 模型可能依赖时间敏感特征（如最近点击），而线上特征更新延迟或采样窗口不一致。  
   - **冷启动与曝光偏差**：离线评估常基于已曝光样本，而线上用户可能未见过相关内容，造成行为分布差异。

2. **指标不一致或评估体系存在偏差**  
   - **离线指标非线上目标**：例如 A 在离线 NDCG、AUC 上优于 B，但线上核心指标（CTR、CVR、GMV、留存率等）不一定正相关。  
   - **训练/测试样本选择偏差**：若离线样本采样不均衡或未模拟曝光机制，可能高估模型效果。  
   - **线上多目标权衡**：A 模型可能提升了短期点击但损害了长期留存或多样性。

3. **模型推理与工程实现差异**  
   - **特征对齐错误（Feature Misalignment）**：训练与线上 Serving 特征计算逻辑不一致（如归一化、缺失值填充、ID映射错误）。  
   - **模型延迟或召回限制**：A 模型结构更复杂，推理耗时高，导致推荐列表更新慢、样本曝光减少。  
   - **在线样本截断**：排序模型上线后与召回模块、粗排模块未充分对齐，最终曝光样本分布与训练分布不符。

4. **用户反馈与探索机制影响**  
   - **A/B 测试干扰**：实验样本量不足或实验周期过短，未覆盖用户全生命周期。  
   - **探索策略差异**：A 模型过度贪心 exploitation，缺乏 exploration（如 ε-greedy、UCB），线上新内容曝光不足。  
   - **用户心理偏移**：推荐过度精准导致用户“信息疲劳”，表现为 CTR 降低但满意度下降。

5. **外部系统因素**  
   - 线上埋点延迟、曝光日志漏采、冷链缓存不一致等工程问题会干扰真实效果判断。

<mark>**一句话总结：模型A线上变差多因数据分布、特征对齐、指标设计及系统实现差异导致，核心是离线评估与真实线上环境不一致。**</mark>


### Q. LLMs怎么捕捉长距离依赖？
> **Company**: 中兴 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-10-12 ｜ **Tags**: [长距离依赖, LLMs, Transformer]

1) **核心原理：自注意力机制（Self-Attention）**  
   Transformer 的自注意力结构使每个 token 能直接与序列中**任意位置**的 token 建立联系：  
   $$
   \text{Attention}(Q,K,V)=\text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V
   $$
   其中 \(Q,K,V\) 分别表示 query、key、value 向量。  
   不同于 RNN/LSTM 的“逐步传递”依赖，Self-Attention 是一种**全局依赖建模**方式，任意两个位置的相互作用可在一次计算中捕获，从而天然具备长距离建模能力。

2) **多头注意力（Multi-Head Attention）增强多维语义关系**  
   多头机制让模型从不同子空间关注不同的语义模式：
   - 某些头可专注于局部依赖（如词法或句法关系）；
   - 另一些头可捕捉长程依赖（如段落级逻辑或主题联系）；  
   从而在不同粒度层次上理解上下文。

3) **位置编码（Positional Encoding / Rotary Embedding）**  
   Transformer 不具备顺序感，需引入位置编码 \(PE\) 让模型感知相对/绝对位置：  
   $$
   PE_{(pos,2i)}=\sin(\frac{pos}{10000^{2i/d}}), \quad
   PE_{(pos,2i+1)}=\cos(\frac{pos}{10000^{2i/d}})
   $$
   或使用 **RoPE（旋转位置编码）** 提供相对位置信息，使得注意力能随距离变化仍保持相关性，从而增强长序列的建模。

4) **层次堆叠与上下文聚合**  
   多层 Transformer 堆叠可实现从局部到全局的信息传递：  
   - 底层捕捉短程词法依赖；  
   - 高层融合长程语义与篇章结构；  
   通过层层聚合实现“跨句、跨段”的依赖理解。

5) **扩展机制（针对超长上下文）**  
   现代 LLMs 常结合以下改进进一步增强长依赖捕获能力：  
   - **Sparse / Longformer / FlashAttention**：减少局部冗余，优化长序列注意力计算；  
   - **Segmented Context / Memory Cache**：在长文档中保留跨窗口上下文；  
   - **Attention Sink / KV Cache Reuse**：在推理中保留关键 token 的表示以实现持续上下文理解。

6) **本质优势对比 RNN/LSTM**  
   - RNN 依赖递归状态传递，长依赖衰减（梯度消失/爆炸）；  
   - Transformer 通过全局注意力和层堆叠**直接建模远距离关系**，避免梯度退化。

<mark>**一句话总结：LLMs 依靠自注意力机制实现任意位置间的全局交互，并结合多头结构与位置编码捕捉跨句甚至篇章级的长距离依赖。**</mark>
