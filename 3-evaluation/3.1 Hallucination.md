# Hallucination
> 本章节记录与 **大语言模型 (LLMs)** 相关的**幻觉 (Hallucination)** 现象，包括其定义、分类、成因、检测与缓解方法等内容。

### Q. 导致 LLMs 产生幻觉的主要原因有哪些？
> **Company**: 淘天 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-10 ｜ **Tags**: [幻觉, Hallucination, 成因]

**一、数据层面的根因（“病从口入”）**  
- **数据缺陷**：训练语料包含**错误信息**、**重复偏见/社会偏见**、**过时事实**；模型还有**知识边界**与**领域覆盖不足**。  
- **事实利用率低 & 伪相关模式**：模型可能过度依赖**位置接近性、共现统计、文档计数**等启发式，而非真实因果与证据 → 例如频繁共现**“加拿大–多伦多”**会诱导模型**误判多伦多为首都**。  
- **长尾知识与记忆缺口**：长尾、稀有或近期知识在语料中弱覆盖，**回忆率低**，易被“编造”补齐。

**二、预训练阶段的机制性限制**  
- **建模假设限制**：**Causal LM 单向自回归**（下一个 token 预测）难以同时捕捉复杂双向约束；**Softmax 瓶颈**限制分布表达能力，易产生过平或过尖的概率估计。  
- **注意力稀释与长程衰减**：序列变长时，重要证据的注意力被**稀释/偏置**（如近因/首因偏置、“中间丢失”），导致上下文事实未被充分利用。  
- **曝光偏差（Exposure Bias）**：训练用“真值前缀”，推理用**自己生成的前缀**；一旦早期生成了错误 token，会**级联放大**并被后续内容“自洽化”。

**三、对齐阶段（SFT / RLHF / GRPO 等）的错位**  
- **能力错位**：对齐数据/需求**超出基础模型的内在能力边界**（知识、推理、工具使用等），导致被迫输出“似是而非”的内容。  
- **信念错位**：**人类偏好奖励**更偏向**礼貌/流畅/自信**而非**真实性/可证伪性**时，模型会“迎合偏好”→ **流畅但不真**。  
- **偏好泛化误差**：偏好数据分布与真实应用不匹配，奖励模型将**表面语言模式**当作“好答案”的证据，强化了幻觉。

**四、推理与解码阶段的因素**  
- **抽样随机性**：温度、Top-k/Top-p 提升多样性也**放大幻觉概率**；错误一旦出现会被自回归**持续合理化**。  
- **不完美的解码表示**：  
  - **上下文关注不足**：对源证据关注不够、过度倚重局部相邻文本。  
  - **Softmax 瓶颈**：对复杂多峰分布刻画不佳，易把低置信度候选“硬挤”成高置信度输出。  
- **长上下文工程约束**：KV 缓存、位置编码/相对偏置的选择与实现细节，均会影响长文档事实检索与整合能力。

**五、任务与提示层面的诱因**  
- **提示含糊/信息缺失/前提错误**：Prompt 给出的**证据不足或含误导**，模型被迫“脑补”。  
- **超范围问答**：要求回答**训练中基本未见的专业冷门问题**，超出模型知识边界。

<mark>**总结**：LLMs 幻觉由“数据缺陷与伪相关模式”+“预训练与注意力机制限制（含曝光偏差、Softmax 瓶颈、长程稀释）”+“对齐阶段的能力/信念错位”+“解码随机性与表示不足”+“任务/提示诱导”共同造成；本质上，概率式语言建模保证流畅性而非真实性，一旦证据稀缺或约束缺失，就容易产生自洽但不真实的输出。</mark>


### Q. 怎么降低大模型的幻觉?
> **Company**: 科大讯飞 ｜ **Round**: 飞星计划算法工程师 一面 ｜ **Date**: 2025-07-01 ｜ **Tags**: [幻觉, Hallucination, 缓解]

**1. 数据层面**  
- **数据清洗与纠错**：去除错误知识、过时事实和严重偏见，减少“病从口入”。  
- **高质量标注数据**：构建可信知识库，并在微调时加入“真值校对数据”。  
- **多样化覆盖**：增加长尾领域数据，降低知识盲区导致的幻觉。  

**2. 模型训练层面**  
- **对齐优化**：通过 RLHF / DPO 等方式增强模型对真实性的偏好，避免过度迎合人类反馈而牺牲事实。  
- **知识增强**：在训练或推理时引入外部知识（RAG、知识图谱），避免模型完全依赖内部记忆“硬编”。  
- **约束解码**：采用约束搜索或逻辑规则（如 coverage penalty、对齐 check）来减少不合理生成。  

**3. 推理阶段层面**  
- **检索增强生成（RAG）**：在回答前调用外部数据库、文档或搜索引擎，确保答案有来源支撑。  
- **不确定性估计**：让模型在置信度低时显式输出“我不知道”，而不是生成虚假内容。  
- **多模型/多样本投票**：采用 self-consistency、CoT+投票，减少随机抽样导致的错误。  

**4. 应用层面**  
- **人类反馈回路**：引入用户纠错机制，逐步修正幻觉频发的场景。  
- **领域约束**：在医疗、金融、法律等高风险应用中，用符号规则或知识图谱对模型输出进行二次校验。  
- **可解释性增强**：要求模型生成答案同时输出依据或出处，方便人工判断真伪。  

<mark>降低大模型幻觉需要从“数据 → 训练 → 推理 → 应用”四个环节系统优化。通用思路是 **减少错误输入、增强知识约束、限制自由生成**，再辅以 **外部知识调用与人类校验**。  </mark>  

### Q. RLHF 现有奖励机制放大预训练幻觉问题？
> **Company**: None ｜ **Round**: None ｜ **Date**: None ｜ **Tags**: [RLHF, 幻觉, Hallucination]

**1. 核心矛盾**  
- RLHF 中的奖励模型（RM）不是评判“事实正确性”，而是近似“人类偏好”的代理指标。  
- 这导致模型为了迎合 RM 的打分，更倾向于生成**流畅、细节丰富、听起来对**的答案，即便可能事实错误，也会被奖励。  
- 最终出现 **表面高质量 ≠ 事实正确** 的偏差，放大了预训练时已有的幻觉。

**2. 三大核心痛点**  
- **偏好驱动而非事实**：RM 学到的是“人类觉得更好听”的风格，而不是事实本身，导致策略模型在优化时放大幻觉。  
- **奖励黑客（Reward Hacking）**：PPO 优化时，策略会调整输出策略，钻 RM 的漏洞获取更高分。例如，回答时加冗余细节、虚构引用，让答案看起来更专业。  
- **单一标量瓶颈**：当前 RM 输出通常是一个单一分数，融合了流畅性、事实性、帮助性等多维标准，信息被严重压缩。导致模型无法区分“关键事实错误 vs 表述不清小问题”，进一步放大幻觉。

**3. 负反馈循环**  
- 预训练模型 → 生成结果带幻觉 → 人类偏好数据标注 → 奖励模型学习“表面质量” → 策略模型优化趋向于幻觉。  
- 这一过程构成 **幻觉放大闭环**，使模型在后续迭代中越来越擅长“编得像真事”。  

**4. 潜在解决方向**  
- **多维度奖励建模**：将单一标量奖励拆分成多维度（如 factuality、helpfulness、fluency），避免信息丢失。  
- **Constitutional AI / 规则式奖励**：引入非人工偏好直接设计的规则反馈，更强调事实性与逻辑一致性。  
- **事实校验机制**：在奖励信号中显式引入知识库检索或外部验证，避免模型凭空编造。  
- **平衡正则化**：通过 KL 惩罚与 SFT 结合，防止策略模型过度偏移到迎合 RM 的方向。  

<mark>RLHF 的奖励机制容易放大幻觉，本质原因在于 RM 的偏好偏差、奖励黑客风险和单一标量瓶颈。要缓解问题，需从奖励建模入手，引入多维度、可验证的反馈信号，从而抑制“表面高分但事实错误”的行为。</mark>
