# Numerical precision 
> 本章节记录 LLMs 训练、推理中数值精度相关工具的面试题目。

## 1. 精度相关的基础知识

### Q. 解释符号位，指数，尾数的含义，并解释BF16和FP16的区别
> **Company**: None | **Round**: None | **Date**: None | **Tags**: [数值精度, fp16, bf16]

- **浮点数的组成 (IEEE 754 格式)**  
  - **符号位 (Sign bit)**：1 位，表示正/负号。0 = 正，1 = 负。  
  - **指数 (Exponent)**：决定数值范围大小，通过偏移量 (bias) 表示正负指数。  
  - **尾数 (Mantissa / Fraction)**：决定数值精度，小数点后的有效数字。  

  表示公式：  
  $$
  \text{value} = (-1)^{sign} \times 2^{exponent-bias} \times (1 + fraction)
  $$

- **如何计算数值范围 (Range)**  
  - 指数位数 = $n$，则指数能表示的最大值约为 $2^n$。  
  - 实际上指数还要减去偏移量 (bias)，所以范围是：  
    $$
    \text{范围} \approx 2^{\pm (2^{n-1})}
    $$  
  - 例如：  
    - **FP16** 指数 = 5 位 → 最大指数 ≈ 31 → 偏移量 = 15 → 最大幂次约 $2^{15} \approx 3 \times 10^4$。  
      所以 FP16 的数量级范围是 $10^{\pm 5}$。  
    - **BF16** 指数 = 8 位 → 最大指数 ≈ 255 → 偏移量 = 127 → 最大幂次 $2^{127} \approx 10^{38}$。  
      所以 BF16 范围与 FP32 一样，约 $10^{\pm 38}$。

- **如何计算精度 (Precision)**  
  - 尾数位数 = $m$，能表示的最小分辨率 ≈ $2^{-m}$。  
  - 例如：  
    - **FP16** 尾数 = 10 位 → 最小分辨率 ≈ $2^{-10} \approx 10^{-3}$。  
    - **BF16** 尾数 = 7 位 → 最小分辨率 ≈ $2^{-7} \approx 10^{-2}$。  

<mark>范围由“指数位”决定，精度由“尾数位”决定；FP16 有更多尾数位 → 精度高但范围小；BF16 有更多指数位 → 范围大但精度低。</mark>

### Q. fp8, fp16, bf16, int8, int4 分别是什么？各自能代表的范围
> **Company**: 阿里国际 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01  **Tags**: [数值精度, fp8, fp16, bf16, int8, int4]

| 格式      | 位宽 | 组成结构 (符号/指数/尾数) | 数值范围 (约)                   | 精度 (约)     | 常见用途                        |
|-----------|------|---------------------------|---------------------------------|---------------|---------------------------------|
| **FP32**  | 32bit | 1 / 8 / 23               | $10^{\pm 38}$                  | $10^{-7}$     | 标准训练，最高精度               |
| **FP16**  | 16bit | 1 / 5 / 10               | $10^{\pm 5}$                   | $10^{-3}$     | 半精度训练、推理加速             |
| **BF16**  | 16bit | 1 / 8 / 7                | $10^{\pm 38}$                  | $10^{-2}$     | 大范围训练 (避免溢出)            |
| **FP8**   | 8bit  | E4M3 或 E5M2             | $10^{\pm 2}$–$10^{\pm 5}$      | 极低          | 研究探索，长序列/低精推理        |
| **INT8**  | 8bit  | 整数 + scale             | $[-128,127]\times scale$       | 离散整数      | 推理量化（常配 scale + zero pt） |
| **INT4**  | 4bit  | 整数 + scale             | $[-8,7]\times scale$           | 极低          | 极限量化（65B 模型塞进消费卡）   |
| **NF4**   | 4bit  | Normalized Float 4       | 动态归一化范围（依赖 scale）    | 优于 int4    | **QLoRA 用的 4bit 格式**，更贴合权重分布 |
- **INT8/INT4**：裸的整数范围分别是 [-128,127] 和 [-8,7]，实际量化时必须配合 **scale (缩放因子)**，才能映射到模型权重的浮点区间。  
- **NF4**：同样依赖 scale，但它的 16 个点不是均匀分布，而是基于权重分布设计的非均匀 codebook，在 0 附近更密集，因此比 int4 更精确。
- **FP8**：目前有两种主流格式 E4M3（4 位指数，3 位尾数）和 E5M2（5 位指数，2 位尾数），前者精度稍高(适合用于前向推理激活值)，后者范围更大(适合用于 权重/梯度)。FP8 仍在研究探索阶段，尚未大规模应用。  

<mark>从 FP32 → FP16/BF16 → FP8 → INT8/INT4，显存逐步下降、速度加快，但精度和范围逐渐下降，需要量化与缩放来维持性能。</mark>

### Q. NF4 (Normalized Float 4) 是什么？为什么比普通 int4 更适合量化 LLMs？
> **Company**: 阿里国际 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01 ｜ **Tags**: [NF4, 量化, LLMs]

- **普通 int4 的表示方式**  
  - int4 只有 16 个整数点：`{-8, -7, …, 0, …, 7}`。  
  - 映射方式：$x \to \text{round}(x/scale)$，再存为 int4。  
  - 例如：权重 $0.037$，如果 scale=0.1，则  
    $0.037/0.1 = 0.37 \to round(0) = 0$，  
    解码后就是 $0 \times 0.1 = 0$ → **完全丢失小数信息**。

- **NF4 的表示方式**  
  - NF4 也是 4 bit（16 个点），但这些点不是等距整数，而是一组 **非均匀分布的浮点值**（codebook）。  
  - 示例 codebook（近似示意）：  
    ```
    {-1.0, -0.57, -0.35, -0.21, -0.12, -0.07, -0.03, 0,
      0.03, 0.07, 0.12, 0.21, 0.35, 0.57, 0.85, 1.0}
    ```
  - 仍然是 16 个可选值，但在 **0 附近更密集**。  
  - 例如：权重 $0.037$，可以映射为 `0.03`（误差 0.007）或 `0.07`（误差 0.033），  
    → **比 int4 的“只能变成 0”更精确**。

<mark>NF4 相当于提前准备了一张 16 个浮点值的“字典”，这些点贴合 LLM 权重分布（多集中在 0 附近）。因此，在相同 4bit 存储下，NF4 比 int4 更能保留数值细节，是 QLoRA 成功的关键。</mark>

### Q. 为什么在训练大模型时，通常用 BF16 而不是 FP16？  
> **Company**: None | **Round**: None | **Date**: None | **Tags**: [数值精度, bf16, fp16]
1. **训练稳定性**  
   - FP16 的指数位只有 5 位，数值范围仅 $10^{\pm 5}$，容易出现梯度溢出 (Inf) 或下溢 (变 0)。  
   - BF16 有 8 位指数（与 FP32 相同），范围达到 $10^{\pm 38}$，更接近 FP32，不需要额外的 **loss scaling** 技巧，训练过程更稳定。  

2. **兼容性好**  
   - BF16 与 FP32 的指数部分一致，互相转换非常方便：FP32 → BF16 只需截断尾数，BF16 → FP32 只需补 0。  
   - FP16 与 FP32 转换则容易出现溢出或精度丢失问题。  
   - 因此大模型训练中，BF16 能与现有 FP32 权重、优化器状态更好协作。  

3. **计算速度快**  
   - 硬件实现上，浮点乘法/加法的能耗与 **尾数位长的平方** 成正比。  
   - BF16 只有 7 位尾数，比 FP16 的 10 位更短 → 计算更快、能耗更低。  

<mark>FP16 精度更高但范围太小，训练大模型容易溢出；BF16 保留 FP32 的范围、转换方便，还能提升计算效率，因此更受大模型训练青睐。</mark>