# Word Embedding
> 本章节记录了面试中关于词嵌入(Word Embedding)的相关问题及答案。

### Q. 除了word2vec、BERT，还了解哪些文本或item的表示学习方法？
> **Company**: 美团 ｜ **Round**: 大模型算法工程师 一面 ｜ **Date**: 2025-08-31 ｜ **Tags**: [表示学习, Embedding, NLP, 推荐系统]

**1. 基于预测任务的嵌入方法**  
- **GloVe**：通过词共现矩阵 + 矩阵分解得到词向量，强调全局统计特性。  
- **FastText**：在 word2vec 基础上引入 **子词 n-gram** 表示，对 OOV（未登录词）和形态丰富语言更友好。  
- **ELMo**：基于双向 LSTM 语言模型生成 **上下文相关的动态词向量**。  

**2. 基于 Transformer 之后的改进模型**  
- **GPT 系列**：自回归语言模型，单向建模上下文，适合生成类任务。  
- **ELECTRA**：使用替换词检测任务（Replaced Token Detection）进行预训练，效率比 MLM 更高。  
- **ERNIE / NEZHA / RoBERTa**：在 BERT 框架上增强知识建模、长序列建模或大规模训练。  

**3. 对比学习类文本表示模型**  
- **SimCSE**：利用 Dropout 或无监督数据增强，构造正负样本对，通过对比学习获取句向量。  
- **E5 (Embedding from Instruction-tuned Encoders)**：在大规模任务指令化数据上训练，提升跨任务泛化。  
- **BGE (BAAI General Embedding)**：通过对比学习 + 指令微调 + 多样化语料训练得到的通用文本向量模型，  
  适用于 **语义检索、向量数据库召回、跨语言任务**，是当前业界常用的 embedding baseline。  

**4. 面向推荐/Item 表示学习的方法**  
- **Matrix Factorization (MF)**：通过用户–item 交互矩阵分解获得 embedding。  
- **DeepWalk / Node2Vec**：图随机游走 + Skip-gram，将 item 看作图节点，学习图嵌入。  
- **Graph Neural Networks (GCN, GAT, PinSage)**：利用图卷积或注意力建模 item-item、user-item 关系。  
- **DSSM / YouTube DNN**：通过深度神经网络学习用户和 item 在同一向量空间的表示，用于召回。  
- **Two-Tower / Dual Encoder**：在推荐和搜索中常用，用户塔和 item 塔分开训练，embedding 向量可高效 ANN 检索。  

**5. 序列建模与多模态表示**  
- **Transformers for Sequential Recommendation (SASRec, BERT4Rec)**：利用自注意力建模用户历史行为序列。  
- **CLIP / ALIGN**：图文对比学习，跨模态对齐，适合文本–图像检索。  
- **对比学习句向量 (SimCSE, E5, BGE)**：在语义检索和问答对齐中效果突出。  

<mark>**总结**：除了 word2vec、BERT 等经典方法，还包括基于预测的 GloVe/ELMo，Transformer 改进模型（GPT/ELECTRA/ERNIE），对比学习类的 SimCSE、E5、**BGE**，以及推荐系统中的 MF、Graph Embedding、Two-Tower 和多模态方法（CLIP、ALIGN）。其中 **BGE 属于对比学习范式下的通用文本表示模型**，广泛用于语义检索与推荐。<mark>

