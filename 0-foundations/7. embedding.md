# Word Embedding
> 本章节记录了面试中关于词嵌入(Word Embedding)的相关问题及答案。

### Q. 除了word2vec、BERT，还了解哪些文本或item的表示学习方法？
> **Company**: 美团 ｜ **Round**: 大模型算法工程师 一面 ｜ **Date**: 2025-08-31 ｜ **Tags**: [表示学习, Embedding, NLP, 推荐系统]

**1. 基于预测任务的嵌入方法**  
- **GloVe**：通过词共现矩阵 + 矩阵分解得到词向量，强调全局统计特性。  
- **FastText**：在 word2vec 基础上引入 **子词 n-gram** 表示，对 OOV（未登录词）和形态丰富语言更友好。  
- **ELMo**：基于双向 LSTM 语言模型生成 **上下文相关的动态词向量**。  

**2. 基于 Transformer 之后的改进模型**  
- **GPT 系列**：自回归语言模型，单向建模上下文，适合生成类任务。  
- **ELECTRA**：使用替换词检测任务（Replaced Token Detection）进行预训练，效率比 MLM 更高。  
- **ERNIE / NEZHA / RoBERTa**：在 BERT 框架上增强知识建模、长序列建模或大规模训练。  

**3. 对比学习类文本表示模型**  
- **SimCSE**：利用 Dropout 或无监督数据增强，构造正负样本对，通过对比学习获取句向量。  
- **E5 (Embedding from Instruction-tuned Encoders)**：在大规模任务指令化数据上训练，提升跨任务泛化。  
- **BGE (BAAI General Embedding)**：通过对比学习 + 指令微调 + 多样化语料训练得到的通用文本向量模型，  
  适用于 **语义检索、向量数据库召回、跨语言任务**，是当前业界常用的 embedding baseline。  

**4. 面向推荐/Item 表示学习的方法**  
- **Matrix Factorization (MF)**：通过用户–item 交互矩阵分解获得 embedding。  
- **DeepWalk / Node2Vec**：图随机游走 + Skip-gram，将 item 看作图节点，学习图嵌入。  
- **Graph Neural Networks (GCN, GAT, PinSage)**：利用图卷积或注意力建模 item-item、user-item 关系。  
- **DSSM / YouTube DNN**：通过深度神经网络学习用户和 item 在同一向量空间的表示，用于召回。  
- **Two-Tower / Dual Encoder**：在推荐和搜索中常用，用户塔和 item 塔分开训练，embedding 向量可高效 ANN 检索。  

**5. 序列建模与多模态表示**  
- **Transformers for Sequential Recommendation (SASRec, BERT4Rec)**：利用自注意力建模用户历史行为序列。  
- **CLIP / ALIGN**：图文对比学习，跨模态对齐，适合文本–图像检索。  
- **对比学习句向量 (SimCSE, E5, BGE)**：在语义检索和问答对齐中效果突出。  

<mark>**总结**：除了 word2vec、BERT 等经典方法，还包括基于预测的 GloVe/ELMo，Transformer 改进模型（GPT/ELECTRA/ERNIE），对比学习类的 SimCSE、E5、**BGE**，以及推荐系统中的 MF、Graph Embedding、Two-Tower 和多模态方法（CLIP、ALIGN）。其中 **BGE 属于对比学习范式下的通用文本表示模型**，广泛用于语义检索与推荐。<mark>

### Q. embedding 的维度怎么选？过大或过小有什么影响？
> **Company**: 蔚来 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-04 ｜ **Tags**: [Embedding, 维度选择]

**1. 维度选择的一般原则**  
- **任务复杂度**：语义信息越丰富（如跨领域文本、长序列上下文），通常需要更高维度。  
- **数据规模**：数据量大，可以支撑更高维度；若数据量小，过高维度会导致过拟合。  
- **计算与存储成本**：维度越高，显存和计算开销越大；需要在性能与资源之间平衡。  
- **经验值**：NLP 中常见 embedding 维度在 **128 ~ 1024**，推荐根据模型规模和任务难度选择。  

**2. 维度过大可能的问题**  
- **过拟合风险**：参数量随维度线性增加，在数据不足时容易记忆训练集。  
- **计算成本高**：矩阵乘法、注意力计算开销增大，推理速度变慢。  
- **存储压力**：embedding 表规模随维度线性增加，在百万级词表时占用显存显著。  

**3. 维度过小可能的问题**  
- **表达能力不足**：无法充分表示复杂语义或细粒度信息。  
- **特征空间拥挤**：不同 token/样本的表示不够区分，容易混淆，导致下游模型性能下降。  

**4. 实际场景建议**  
- **搜索/推荐系统**：常用 128/256 维，兼顾召回与在线性能。  
- **NLP 大模型预训练**：通常使用 768（BERT-base）、1024（BERT-large）、4096（GPT-3）等较高维度。  
- **边缘/移动端应用**：需要在压缩和性能之间权衡，常用 64/128 维。  

<mark>**总结**：embedding 维度选择需结合任务复杂度、数据规模和资源限制。过大易过拟合且计算开销大，过小则表达力不足。工业实践中常在 128~1024 之间选择，并通过实验验证找到最优维度。</mark>
