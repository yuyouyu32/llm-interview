# [Attention is all you need](https://arxiv.org/abs/1706.03762)
> 本章节记录 Attention 相关的面试题目。


### Q. Attention的公式，公式为什么要除以$\sqrt{d_k}$，为什么不是原来的值或者是立方根d_k？
> **Company**: 阿里国际 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01 ｜ **Tags**: [Attention, 归一化]

- **Scaled Dot-Product Attention 公式：**
  $$
  \text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  $$
  其中 $Q,K,V$ 的大小：$Q,K,V \in \mathbb{R}^{n \times d}$，$d_k$ 是 key 的维度。

- **为什么要除以 $\sqrt{d_k}$？**
  - 若不缩放，$QK^T$ 的期望值和方差会随 $d_k$ 增大而变大。  
  - 内积值过大 → softmax 指数函数饱和 → 导致梯度消失，训练困难。  
  - 除以 $\sqrt{d_k}$ 可以把内积的方差归一化到常数水平（大约保持在 1），避免数值不稳定。

- **为什么不是立方根或其他函数？**
  - $QK^T$ 是 $d_k$ 个独立随机变量乘积的和，方差 $\propto d_k$。  
  - 标准差（方差的平方根）才是合适的缩放因子 → 所以用 $\sqrt{d_k}$。  
  - 如果用 $d_k$ 或 $\sqrt[3]{d_k}$，缩放效果会过强或不足，数值不再平衡。

<mark>Attention 公式中除以 $\sqrt{d_k}$ 是为了归一化内积的方差，避免数值过大导致 softmax 饱和和梯度消失；因为内积的方差与 $d_k$ 成正比，标准差与 $\sqrt{d_k}$ 成正比，所以选择 $\sqrt{d_k}$ 作为缩放因子。</mark>

- 输入序列长度 = $n$，隐藏维度 = $d$。  
- 主要计算步骤：
  1. $QK^T$ → $O(n \times d \times n) = O(n^2 d)$
  2. softmax($n \times n$ 矩阵) → $O(n^2)$  
  3. softmax 矩阵 × $V$ → $O(n^2 d)$  
- **总复杂度：**
  $$
  O(n^2 d)
  $$

- **瓶颈：**
  - 当序列长度 $n$ 很大时，$O(n^2)$ 成本过高，显存也要存储 $n \times n$ 注意力矩阵。  
  - 这就是为什么有 FlashAttention、Sparse Attention、Linear Attention 等改进。  

**一句话总结：**  
<mark>Attention 的主要成本来自两个 $n \times n$ 矩阵乘法，时间复杂度是 $O(n^2 d)$，瓶颈在序列长度平方项。</mark>

### Q. Transformer 中 Attention 的本质是什么？你能从数学角度简要解释一下吗？
> **Company**: 淘天 ｜ **Round**: Agent智能体 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [Attention, Transformer]

- **直观理解**  
  Attention 的核心思想是：**在处理某个词时，不是平均看所有上下文，而是学会“更关注相关的词”**。  
  它相当于一个“加权信息聚合器”，相关性强的词权重大，不相关的权重小。

- **数学公式**  
  给定输入序列 $X \in \mathbb{R}^{n \times d}$，Attention 的计算过程为：

  1. 先通过可学习矩阵得到 **Query, Key, Value**：
     $$
     Q = XW_Q,\quad K = XW_K,\quad V = XW_V
     $$
  2. 计算相似度（相关性）：
     $$
     \text{score}(Q,K) = \frac{QK^\top}{\sqrt{d_k}}
     $$
     - 这是 Query 与 Key 的点积，相似度越高说明更相关。  
     - 除以 $\sqrt{d_k}$ 是为了防止数值过大导致梯度消失/爆炸。  
  3. 归一化成权重（注意力分布）：
     $$
     \alpha = \text{softmax}\big(\text{score}(Q,K)\big)
     $$
  4. 用权重对 Value 做加权求和，得到最终输出：  
     $$
     \text{Attention}(Q,K,V) = \alpha V
     $$

- **总结**  
  - Query 表示“我要关注什么”；  
  - Key 表示“你能提供什么信息”；  
  - Value 表示“真正的信息内容”；  
  - 点积 + softmax 就是“分配注意力权重”。  

**一句话总结：**  
<mark>Attention 的本质是 **基于 Query-Key 相似度，对 Value 进行加权求和**，从而实现信息的动态聚合和建模长程依赖。</mark>


### Q. KV Cache 存的是什么？它的大小跟什么因素有关？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [KV Cache, 推理优化, Transformer]

- **KV Cache 存的内容**  
  - 在 Transformer 解码时，每一层自注意力都会生成 **Key (K)** 和 **Value (V)** 张量。  
  - KV Cache 就是把历史 token 的 **K/V 表征缓存下来**，下次解码新 token 时无需重新计算前缀。  
  - **作用**：避免重复计算，加速自回归生成。  

- **KV Cache 的大小与以下因素有关**  
  1. **序列长度 $L$**：存储的历史 token 越多，KV Cache 越大。  
  2. **层数 $N$**：每一层都要存一份 K/V，层数越多占用越大。  
  3. **注意力头数 $H$**：每个头单独存储 K/V。  
  4. **隐藏维度 $d_{model}$ 与 head 维度 $d_{head}$**：  
     - 一般 $d_{head} = d_{model}/H$。  
     - K 的大小：$[L, H, d_{head}]$，V 的大小同样。  
  5. **Batch size $B$**：多个样本推理时，KV Cache 按批次线性增长。  

- **公式近似**  
  - 单层单样本 KV Cache 大小 ≈ $2 \times L \times H \times d_{head}$ （K 和 V 各一份）。  
  - 总大小 ≈ $B \times N \times 2 \times L \times H \times d_{head}$。  

- **结论**  
  - KV Cache 存的是 **历史 token 的 Key/Value 表征**。  
  - 大小主要由 **序列长度、层数、注意力头数、隐藏维度、batch size** 决定。  

<mark>核心：KV Cache = 历史 Key/Value 张量；大小随 **序列长度 L × 层数 N × 注意力头数 H × head 维度 × Batch size** 线性增长。</mark>

### Q. MQA、MLA、Dual Attention、Sparse Attention 的区别与应用场景？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [注意力机制, MQA, MLA, Dual Attention, Sparse Attention]

- **MQA (Multi-Query Attention)**  
  - **思想**：多个 Query，但 **共享同一份 Key/Value**。  
  - **优点**：减少 KV 存储和计算开销（KV Cache 从 H 个头缩减到 1 份）。  
  - **应用**：大规模推理（如 LLaMA、PaLM），提升推理效率，显存开销大幅下降。  
  - **缺点**：表达能力略弱于标准 Multi-Head Attention (MHA)。  

- **MLA (Multi-Head Latent Attention, 多头潜在注意力)**  
  - **思想**：在 Query/Key/Value 计算中引入 **低维潜在空间 (latent)**，通过共享和压缩进一步减少计算。  
  - **效果**：  
    - 比 MQA 更进一步压缩 KV 表征。  
    - 结合低秩近似，进一步优化内存和计算。  
  - **应用**：部分大模型推理优化中（如 DeepSeek 提到的 MLA）。  

- **Dual Attention**  
  - **思想**：引入 **两种注意力机制并行或交替使用**，常见模式：  
    1. **内容注意力 (content-based)** + **位置/相对位置注意力**。  
    2. **全局注意力** + **局部注意力**。  
  - **应用**：  
    - Transformer-XL、Longformer 等长序列建模。  
    - 图神经网络中 (node-level & relation-level)。  
  - **作用**：兼顾全局依赖和局部细节。  

- **Sparse Attention**  
  - **思想**：在 Attention 矩阵中只计算部分连接，降低复杂度。  
  - **典型模式**：  
    - **局部窗口 (local window)**：只看相邻 token。  
    - **Stride/Block pattern**：固定步长连接。  
    - **全局 token**：如 [CLS] 特殊 token 保持全局可见。  
  - **复杂度**：从 O(L²) 降到 O(L√L) 或 O(L log L)。  
  - **应用**：Longformer、BigBird、Sparse Transformer，常用于 **长文本/长序列建模**。  

- **对比总结**  

| 方法           | 核心思路 | 优点 | 缺点 | 应用场景 |
|----------------|----------|------|------|----------|
| **MQA**        | 多 Query，共享 KV | KV Cache 显存小，推理快 | 表达能力下降 | 大模型推理加速 |
| **MLA**        | 低维 latent 压缩 KV | 更进一步减少内存/计算 | 潜在表示需额外训练 | 新型推理优化 |
| **Dual Attention** | 结合两种注意力（如全局+局部） | 全局依赖+局部细节 | 结构更复杂 | 长序列、图建模 |
| **Sparse Attention** | 稀疏连接，降低复杂度 | 复杂度低，支持长文本 | 信息可能丢失 | Longformer、BigBird |

<mark>核心：MQA/MLA 主要解决 **推理阶段显存与吞吐率**，Dual Attention/Sparse Attention 主要解决 **长序列依赖与复杂度**。根据应用场景选择合适机制。</mark>

### Q. 手撕 MHA（Multi-Head Attention）公式与 PyTorch 实现（矩阵化、不可直接用官方 MHA）

**符号与形状**  
- 批量大小 `B`，序列长 `Lq/Lk`，模型维度 `d_model`，头数 `h`，每头维度 `d = d_model / h`  
- 输入：`X_q∈ℝ^{B×Lq×d_model}`, `X_k∈ℝ^{B×Lk×d_model}`, `X_v∈ℝ^{B×Lk×d_model}`  
- 投影：`W_Q,W_K,W_V∈ℝ^{d_model×d_model}`，输出投影 `W_O∈ℝ^{d_model×d_model}`

**公式（矩阵化）**  
1) 线性映射并按头分块  
\[
Q = X_q W_Q,\; K = X_k W_K,\; V = X_v W_V \;\in \mathbb{R}^{B\times L\_\* \times d\_{model}}
\]  
重排为多头：`reshape → (B, h, L*, d)`。

2) 缩放点积注意力（带可选掩码 `M`，masked 位置加 `-∞`）  
\[
S = \frac{QK^\top}{\sqrt{d}} \in \mathbb{R}^{B\times h\times L_q\times L_k},\quad
S \leftarrow S + M,\quad
A = \mathrm{softmax}(S)
\]  
\[
H = A V \in \mathbb{R}^{B\times h\times L_q\times d}
\]

3) 头拼接与输出投影  
\[
\mathrm{MHA}(X\_q,X\_k,X\_v) = \mathrm{concat}(H\_1,\dots,H\_h) W_O \in \mathbb{R}^{B\times L_q\times d\_{model}}
\]

**时间复杂度**  
- 主成本为注意力分数：`O(B · h · Lq · Lk · d)`；当 `Lq=Lk=L` 时为 `O(B · h · L² · d)`。

---

### 纯 PyTorch 实现（不调用 `nn.MultiheadAttention`）

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0, bias: bool = True):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.h = num_heads
        self.d = d_model // num_heads

        self.wq = nn.Linear(d_model, d_model, bias=bias)
        self.wk = nn.Linear(d_model, d_model, bias=bias)
        self.wv = nn.Linear(d_model, d_model, bias=bias)
        self.wo = nn.Linear(d_model, d_model, bias=bias)
        self.drop = nn.Dropout(dropout)

    def _split_heads(self, x):
        # x: (B, L, d_model) -> (B, h, L, d)
        B, L, _ = x.shape
        x = x.view(B, L, self.h, self.d).transpose(1, 2)
        return x

    def _merge_heads(self, x):
        # x: (B, h, L, d) -> (B, L, d_model)
        B, h, L, d = x.shape
        return x.transpose(1, 2).contiguous().view(B, L, h * d)

    @staticmethod
    def _apply_mask(scores, attn_mask=None, causal=False):
        # scores: (B, h, Lq, Lk)
        if causal:
            Lq, Lk = scores.size(-2), scores.size(-1)
            causal_mask = torch.triu(torch.ones(Lq, Lk, device=scores.device, dtype=torch.bool), diagonal=1)
            scores = scores.masked_fill(causal_mask, float('-inf'))
        if attn_mask is not None:
            # attn_mask broadcast 到 (B, 1, Lq, Lk) 或 (B, h, Lq, Lk)
            scores = scores + attn_mask
        return scores

    def forward(self, x_q, x_k, x_v, attn_mask=None, causal=False, need_weights=False):
        """
        x_q: (B, Lq, d_model), x_k/x_v: (B, Lk, d_model)
        attn_mask: 可选，加性掩码，屏蔽处应为 -inf（或大负数），形状可广播到 (B, h, Lq, Lk)
        causal: 是否因果掩码（decoder自回归）
        need_weights: 是否返回注意力权重均值（B, Lq, Lk）
        """
        B = x_q.size(0)

        Q = self._split_heads(self.wq(x_q))  # (B,h,Lq,d)
        K = self._split_heads(self.wk(x_k))  # (B,h,Lk,d)
        V = self._split_heads(self.wv(x_v))  # (B,h,Lk,d)

        # 注意：使用 matmul，而非循环；缩放放在分数计算前后均可
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d ** 0.5)  # (B,h,Lq,Lk)
        scores = self._apply_mask(scores, attn_mask=attn_mask, causal=causal)
        attn = F.softmax(scores, dim=-1)
        attn = self.drop(attn)

        H = torch.matmul(attn, V)  # (B,h,Lq,d)
        out = self.wo(self._merge_heads(H))  # (B,Lq,d_model)

        if need_weights:
            # 平均各头注意力（可用于可视化/调试）
            return out, attn.mean(dim=1)
        return out

# 测试
B, Lq, Lk, d_model, h = 2, 16, 16, 512, 8
x = torch.randn(B, Lq, d_model)
mha = MultiHeadAttention(d_model, h, dropout=0.1)

# 自注意力（Q=K=V）
y = mha(x, x, x, causal=False)          # encoder self-attn
y_causal = mha(x, x, x, causal=True)    # decoder self-attn（自回归）

# 交叉注意力（Q 来自 decoder，K/V 来自 encoder）
dec = torch.randn(B, Lq, d_model)
enc = torch.randn(B, Lk, d_model)
y_cross = mha(dec, enc, enc)

```