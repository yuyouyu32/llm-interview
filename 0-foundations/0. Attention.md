# [Attention is all you need](https://arxiv.org/abs/1706.03762)
> 本章节记录 Attention 相关的面试题目。

### Q. Transformer的自注意力机制及相比RNN的优势？
> **Company**: 同花顺 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-04-15 ｜ **Tags**: [Attention, Transformer, RNN]

- **Transformer的自注意力机制：**
  - 自注意力（Self-Attention）允许序列中的每个位置与其他所有位置建立依赖关系。
  - 对输入序列 $X$，通过 $Q = XW_Q, K = XW_K, V = XW_V$ 得到 Query、Key、Value。
  - 计算公式：
    $$
    \text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    $$
  - 每个 token 的表示会融合全局上下文信息，而不仅仅依赖相邻位置。

- **相比RNN的优势：**
  1. **并行化计算**：  
     RNN 按时间步递归处理，无法并行；Transformer 的自注意力一次性计算整个序列，训练速度更快。
  2. **长程依赖建模能力强**：  
     RNN 受梯度消失/爆炸影响，难以捕捉远距离依赖；自注意力可直接计算任意位置间的关系。
  3. **信息交互灵活**：  
     每个 token 能与序列中所有 token 建立联系，而 RNN 仅通过前后顺序传递信息。
  4. **表达能力更强**：  
     多头注意力（Multi-Head Attention）可在不同子空间建模多种关系特征，比单一隐藏状态更丰富。
  5. **更适合大规模预训练**：  
     Transformer 架构配合并行训练和 GPU/TPU 硬件优势，在大数据场景下表现更优。

<mark>自注意力机制让 Transformer 具备并行计算与强大的长程依赖建模能力，相比 RNN 在效率和表达力上都更具优势。</mark>


### Q. Attention的公式，公式为什么要除以$\sqrt{d_k}$，为什么不是原来的值或者是立方根d_k？
> **Company**: 阿里国际 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01 ｜ **Tags**: [Attention, 归一化]

- **Scaled Dot-Product Attention 公式：**
  $$
  \text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  $$
  其中 $Q,K,V$ 的大小：$Q,K,V \in \mathbb{R}^{n \times d}$，$d_k$ 是 key 的维度。

- **为什么要除以 $\sqrt{d_k}$？**
  - 若不缩放，$QK^T$ 的期望值和方差会随 $d_k$ 增大而变大。  
  - 内积值过大 → softmax 指数函数饱和 → 导致梯度消失，训练困难。  
  - 除以 $\sqrt{d_k}$ 可以把内积的方差归一化到常数水平（大约保持在 1），避免数值不稳定。

- **为什么不是立方根或其他函数？**
  - $QK^T$ 是 $d_k$ 个独立随机变量乘积的和，方差 $\propto d_k$。  
  - 标准差（方差的平方根）才是合适的缩放因子 → 所以用 $\sqrt{d_k}$。  
  - 如果用 $d_k$ 或 $\sqrt[3]{d_k}$，缩放效果会过强或不足，数值不再平衡。

<mark>Attention 公式中除以 $\sqrt{d_k}$ 是为了归一化内积的方差，避免数值过大导致 softmax 饱和和梯度消失；因为内积的方差与 $d_k$ 成正比，标准差与 $\sqrt{d_k}$ 成正比，所以选择 $\sqrt{d_k}$ 作为缩放因子。</mark>

### Q. Attention的时间复杂度是多少？瓶颈在哪里？
> **Company**: 淘天 ｜ **Round**: Agent智能体 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [Attention, Transformer]

- 输入序列长度 = $n$，隐藏维度 = $d$。  
- 主要计算步骤：
  1. $QK^T$ → $O(n \times d \times n) = O(n^2 d)$
  2. softmax($n \times n$ 矩阵) → $O(n^2)$  
  3. softmax 矩阵 × $V$ → $O(n^2 d)$  
- **总复杂度：**
  $$
  O(n^2 d)
  $$

- **瓶颈：**
  - 当序列长度 $n$ 很大时，$O(n^2)$ 成本过高，显存也要存储 $n \times n$ 注意力矩阵。  
  - 这就是为什么有 FlashAttention、Sparse Attention、Linear Attention 等改进。  

**一句话总结：**  
<mark>Attention 的主要成本来自两个 $n \times n$ 矩阵乘法，时间复杂度是 $O(n^2 d)$，瓶颈在序列长度平方项。</mark>

### Q. Transformer 中 Attention 的本质是什么？你能从数学角度简要解释一下吗？
> **Company**: 淘天 ｜ **Round**: Agent智能体 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [Attention, Transformer]

- **直观理解**  
  Attention 的核心思想是：**在处理某个词时，不是平均看所有上下文，而是学会“更关注相关的词”**。  
  它相当于一个“加权信息聚合器”，相关性强的词权重大，不相关的权重小。

- **数学公式**  
  给定输入序列 $X \in \mathbb{R}^{n \times d}$，Attention 的计算过程为：

  1. 先通过可学习矩阵得到 **Query, Key, Value**：
     $$
     Q = XW_Q,\quad K = XW_K,\quad V = XW_V
     $$
  2. 计算相似度（相关性）：
     $$
     \text{score}(Q,K) = \frac{QK^\top}{\sqrt{d_k}}
     $$
     - 这是 Query 与 Key 的点积，相似度越高说明更相关。  
     - 除以 $\sqrt{d_k}$ 是为了防止数值过大导致梯度消失/爆炸。  
  3. 归一化成权重（注意力分布）：
     $$
     \alpha = \text{softmax}\big(\text{score}(Q,K)\big)
     $$
  4. 用权重对 Value 做加权求和，得到最终输出：  
     $$
     \text{Attention}(Q,K,V) = \alpha V
     $$

- **总结**  
  - Query 表示“我要关注什么”；  
  - Key 表示“你能提供什么信息”；  
  - Value 表示“真正的信息内容”；  
  - 点积 + softmax 就是“分配注意力权重”。  

**一句话总结：**  
<mark>Attention 的本质是 **基于 Query-Key 相似度，对 Value 进行加权求和**，从而实现信息的动态聚合和建模长程依赖。</mark>


### Q. KV Cache 存的是什么？它的大小跟什么因素有关？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [KV Cache, 推理优化, Transformer]

- **KV Cache 存的内容**  
  - 在 Transformer 解码时，每一层自注意力都会生成 **Key (K)** 和 **Value (V)** 张量。  
  - KV Cache 就是把历史 token 的 **K/V 表征缓存下来**，下次解码新 token 时无需重新计算前缀。  
  - **作用**：避免重复计算，加速自回归生成。  

- **KV Cache 的大小与以下因素有关**  
  1. **序列长度 $L$**：存储的历史 token 越多，KV Cache 越大。  
  2. **层数 $N$**：每一层都要存一份 K/V，层数越多占用越大。  
  3. **注意力头数 $H$**：每个头单独存储 K/V。  
  4. **隐藏维度 $d_{model}$ 与 head 维度 $d_{head}$**：  
     - 一般 $d_{head} = d_{model}/H$。  
     - K 的大小：$[L, H, d_{head}]$，V 的大小同样。  
  5. **Batch size $B$**：多个样本推理时，KV Cache 按批次线性增长。  

- **公式近似**  
  - 单层单样本 KV Cache 大小 ≈ $2 \times L \times H \times d_{head}$ （K 和 V 各一份）。  
  - 总大小 ≈ $B \times N \times 2 \times L \times H \times d_{head}$。  

- **结论**  
  - KV Cache 存的是 **历史 token 的 Key/Value 表征**。  
  - 大小主要由 **序列长度、层数、注意力头数、隐藏维度、batch size** 决定。  

<mark>核心：KV Cache = 历史 Key/Value 张量；大小随 **序列长度 L × 层数 N × 注意力头数 H × head 维度 × Batch size** 线性增长。</mark>

### Q. 为什么Cache一般只存KV，不存Q？
> **Company**: 快手 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-01 ｜ **Tags**: [KV Cache, 推理优化, Transformer]

**1. 背景**  
- 在 Transformer 解码阶段，每次生成新 token 时需要计算自注意力：  

$$
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$  

- 如果每次都重新计算所有历史的 \(K, V\)，会导致推理复杂度随序列长度线性增长。  
- 因此在推理时会用 **KV Cache** 缓存历史 key 和 value，避免重复计算。  

**2. 为什么不存 Q？**  
- **Q 是当前 token 的查询向量**，只依赖于新输入 token 的 hidden state：  
  - 每次解码时，模型只需要计算当前一步的 Q（单个向量），开销很小。  
- **K 和 V 是历史信息**：  
  - 必须完整保存，因为新 token 的 Q 需要与所有历史 K 交互（做点积计算注意力权重），并根据历史 V 得到上下文。  
- 存储 Q 没意义：  
  - 旧的 Q 不会再被使用，因为注意力是“新 Q 对旧 K,V”的匹配，而不是“新 Q 对旧 Q”。  
  - 存 Q 反而增加了内存开销。  

**3. 计算和存储对比**  
- **存 KV**：空间消耗 \(O(L \cdot d)\)，避免重复计算，显著加速推理。  
- **不存 Q**：每次只算一步的 Q，计算量 \(O(d)\)，相对很小。  

<mark>核心：历史 K,V 必须缓存以供后续计算，而 Q 每步只依赖当前输入且不会复用，所以只存 KV，不存 Q。</mark>

### Q. MQA、MLA、Dual Attention、Sparse Attention 的区别与应用场景？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [注意力机制, MQA, MLA, Dual Attention, Sparse Attention]

- **MQA (Multi-Query Attention)**  
  - **思想**：多个 Query，但 **共享同一份 Key/Value**。  
  - **优点**：减少 KV 存储和计算开销（KV Cache 从 H 个头缩减到 1 份）。  
  - **应用**：大规模推理（如 LLaMA、PaLM），提升推理效率，显存开销大幅下降。  
  - **缺点**：表达能力略弱于标准 Multi-Head Attention (MHA)。  

- **MLA (Multi-Head Latent Attention, 多头潜在注意力)**  
  - **思想**：在 Query/Key/Value 计算中引入 **低维潜在空间 (latent)**，通过共享和压缩进一步减少计算。  
  - **效果**：  
    - 比 MQA 更进一步压缩 KV 表征。  
    - 结合低秩近似，进一步优化内存和计算。  
  - **应用**：部分大模型推理优化中（如 DeepSeek 提到的 MLA）。  

- **Dual Attention**  
  - **思想**：引入 **两种注意力机制并行或交替使用**，常见模式：  
    1. **内容注意力 (content-based)** + **位置/相对位置注意力**。  
    2. **全局注意力** + **局部注意力**。  
  - **应用**：  
    - Transformer-XL、Longformer 等长序列建模。  
    - 图神经网络中 (node-level & relation-level)。  
  - **作用**：兼顾全局依赖和局部细节。  

- **Sparse Attention**  
  - **思想**：在 Attention 矩阵中只计算部分连接，降低复杂度。  
  - **典型模式**：  
    - **局部窗口 (local window)**：只看相邻 token。  
    - **Stride/Block pattern**：固定步长连接。  
    - **全局 token**：如 [CLS] 特殊 token 保持全局可见。  
  - **复杂度**：从 O(L²) 降到 O(L√L) 或 O(L log L)。  
  - **应用**：Longformer、BigBird、Sparse Transformer，常用于 **长文本/长序列建模**。  

- **对比总结**  

| 方法           | 核心思路 | 优点 | 缺点 | 应用场景 |
|----------------|----------|------|------|----------|
| **MQA**        | 多 Query，共享 KV | KV Cache 显存小，推理快 | 表达能力下降 | 大模型推理加速 |
| **MLA**        | 低维 latent 压缩 KV | 更进一步减少内存/计算 | 潜在表示需额外训练 | 新型推理优化 |
| **Dual Attention** | 结合两种注意力（如全局+局部） | 全局依赖+局部细节 | 结构更复杂 | 长序列、图建模 |
| **Sparse Attention** | 稀疏连接，降低复杂度 | 复杂度低，支持长文本 | 信息可能丢失 | Longformer、BigBird |

<mark>核心：MQA/MLA 主要解决 **推理阶段显存与吞吐率**，Dual Attention/Sparse Attention 主要解决 **长序列依赖与复杂度**。根据应用场景选择合适机制。</mark>

### Q. 讲讲 GQA（Grouped-Query Attention）
> **Company**: 京东 | **Round**: 算法工程师 二面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [GQA, 注意力机制, Transformer]

**1. 背景**  
- 标准 **Multi-Head Attention (MHA)**：每个头都有独立的 Query/Key/Value 投影，能捕捉不同子空间的依赖，但计算和显存开销大。  
- **MQA (Multi-Query Attention)**：多个 Query 共享同一组 Key/Value，大幅减少显存和计算，但会牺牲部分表达能力。  
- **GQA (Grouped-Query Attention)**：介于二者之间的折中方案。  

**2. GQA 的核心思想**  
- 将多个 Query 头划分为若干组，每组 Query **共享同一组 Key/Value**。  
- 即：  
  - Query 仍然保持多样性（多个子空间）。  
  - Key/Value 数量减少（存储和计算负担下降）。  
- 公式上：  
  - MHA: 每个 head 有独立 $Q_i, K_i, V_i$。  
  - MQA: 所有 head 共享 $K, V$。  
  - GQA: 每组 head 共享 $K, V$，但组间独立。  

**3. 优点**  
- **效率与效果折中**：相比 MHA 显存和计算显著减少，但比 MQA 保留更多表达能力。  
- **加速推理**：存储的 KV Cache 更小（因为组数远小于 head 数），在长上下文推理时速度更快。  
- **工程友好**：在不大幅降低性能的情况下，提高大模型的可扩展性。  

**4. 应用与效果**  
- 已被广泛应用于 GPT-4、LLaMA 2/3、Qwen2 等大模型中。  
- 实践表明：  
  - 性能接近 MHA。  
  - 内存占用更接近 MQA。  
  - 在长文本生成和大规模推理时尤其高效。  

**5. 总结**  
<mark>GQA（Grouped-Query Attention）是介于 MHA 和 MQA 之间的折中方案：多个 Query 头分组共享 Key/Value。它兼顾表达能力和推理效率，已成为大模型主流配置。</mark>


### Q. MHA有什么用？为什么需要多头？
> **Company**: 快手 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-01 ｜ **Tags**: [MHA, Attention, Transformer]

**1. MHA 的作用**  
- **MHA（Multi-Head Attention，多头注意力）**是在 Transformer 中的核心机制。  
- 通过多个注意力头并行计算，可以让模型在不同的“投影子空间”中学习信息交互：  

$$
\text{MHA}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
$$  

其中：  
$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$  

- 每个头都有独立的参数矩阵 \(W_i^Q, W_i^K, W_i^V\)，因此能从不同角度理解序列依赖。  

**2. 为什么需要多头？**  
- **单头注意力的局限**：只能从一个空间去计算注意力，捕捉的依赖关系有限。  
- **多头的优势**：  
  - **多视角表达**：不同头可关注不同位置关系（如短距离依赖、长距离依赖）。  
  - **子空间学习**：通过线性投影到不同维度子空间，增强表示能力。  
  - **信息丰富**：多个头的结果拼接后，能提供更全面的上下文特征。  

**3. 工程实践要点**  
- **头数与维度平衡**：常见配置是 \(d_{model}=512, h=8\)，即每个头维度 \(64\)。  
- **效果**：多头注意力能提升模型对复杂依赖的建模能力，是 Transformer 性能的关键来源。  

<mark>核心：MHA 通过多个注意力头并行学习不同的依赖关系，使模型能从多角度捕捉序列特征，从而增强表达能力。</mark>



### Q. 手撕 MHA（Multi-Head Attention）公式与 PyTorch 实现（矩阵化、不可直接用官方 MHA）
> **Company**: 快手 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-05 ｜ **Tags**: [MHA, Attention, Transformer, PyTorch]

**公式（矩阵化）**  
设输入为 \(X \in \mathbb{R}^{B\times T\times d_{\text{model}}}\)，头数为 \(h\)，单头维度 \(d_k=\frac{d_{\text{model}}}{h}\)：
$$
Q = X W^Q,\quad K = X W^K,\quad V = X W^V,\qquad W^{Q,K,V}\in\mathbb{R}^{d_{\text{model}}\times d_{\text{model}}}
$$
将 \(Q,K,V\) 变形为 \((B,h,T,d_k)\) 后按头并行计算注意力：
$$
\text{head}_i=\mathrm{softmax}\!\left(\frac{Q_i K_i^\top}{\sqrt{d_k}}\right) V_i,\qquad Q_i,K_i,V_i\in\mathbb{R}^{B\times T\times d_k}
$$
拼接各头并做输出投影：
$$
\mathrm{MHA}(X)=\mathrm{Concat}(\text{head}_1,\ldots,\text{head}_h)\,W^O,\qquad W^O\in\mathbb{R}^{d_{\text{model}}\times d_{\text{model}}}
$$

**PyTorch 实现（纯矩阵化，支持可选 padding/causal mask）**
```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_size, num_heads, dropout=0.0):
        """
        多头注意力机制的实现。
        Args:
            hidden_size (int): 输入特征的维度，也即 hidden_state 的最后一维。
            num_heads (int): 注意力头的数量。
            dropout (float): dropout 的概率，默认为 0.0。
        """
        super(MultiHeadAttention, self).__init__()

        assert hidden_size % num_heads == 0, "hidden_size 必须能被 num_heads 整除"

        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads  # 每个头的维度

        # 定义线性变换层，用于生成 Q, K, V
        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)

        self.dropout = nn.Dropout(dropout)

        # 输出线性层
        self.out_projection = nn.Linear(hidden_size, hidden_size)

    def forward(self, hidden_state, attention_mask=None):

        """
        前向传播函数。
        Args:
            hidden_state (torch.Tensor): 输入的 hidden_state，形状为 [batch_size, seq_len, hidden_size]。
            attention_mask (torch.Tensor, optional): 注意力掩码，用于屏蔽某些位置，形状为 [batch_size, seq_len]。默认为 None。
        Returns:
            torch.Tensor: 注意力输出，形状为 [batch_size, seq_len, hidden_size]。

        """
        batch_size, seq_len, _ = hidden_state.size()

        # 1. 通过线性层得到 Q, K, V
        query = self.query(hidden_state)  # [batch_size, seq_len, hidden_size]
        key = self.key(hidden_state)      # [batch_size, seq_len, hidden_size]
        value = self.value(hidden_state)    # [batch_size, seq_len, hidden_size]

        # 2. 将 Q, K, V 拆分成多头
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]
        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)      # [batch_size, num_heads, seq_len, head_dim]
        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)    # [batch_size, num_heads, seq_len, head_dim]

        # 3. 计算注意力权重
        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [batch_size, num_heads, seq_len, seq_len]

        # 应用 attention mask
        if attention_mask is not None:
            attention_weights = attention_weights.masked_fill(attention_mask[:, None, None, :] == 0, float('-inf'))

        attention_weights = torch.softmax(attention_weights, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]
        attention_weights = self.dropout(attention_weights)

        # 4. 计算上下文向量
        context = torch.matmul(attention_weights, value)  # [batch_size, num_heads, seq_len, head_dim]

        # 5. 将多头合并
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)  # [batch_size, seq_len, hidden_size]

        # 6. 通过输出线性层
        output = self.out_projection(context)  # [batch_size, seq_len, hidden_size]
        return output
        
if __name__ == '__main__':
    # 示例
    batch_size = 2
    seq_len = 10
    hidden_size = 256
    num_heads = 8

    # 创建一个 MHA 实例
    mha = MultiHeadAttention(hidden_size, num_heads)

    # 创建一个随机的 hidden_state
    hidden_state = torch.randn(batch_size, seq_len, hidden_size)

    # 创建一个 attention mask (可选)
    attention_mask = torch.ones(batch_size, seq_len)
    attention_mask[:, 5:] = 0  # 屏蔽掉每个 batch 中 seq_len 的后 5 个位置

    # 通过 MHA 层
    output = mha(hidden_state, attention_mask)

    # 打印输出形状
    print("输出形状:", output.shape)  # torch.Size([2, 10, 256])
```
<mark>核心：用单次线性取得 QKV → 分头矩阵化算 softmax( QKᵀ/√dₖ )V → 拼接后线性投影，全流程无显式循环且不依赖官方 MHA。</mark>