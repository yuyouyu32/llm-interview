# LLMs Frameworks
> 本章节记录 LLMs 相关框架(encoders/decoders)的面试题目。

### Q. BERT和大模型的区别
> **Company**: 美团 | **Round**: 大模型算法工程师 一面 | **Date**: 2025-08-31 | **Tags**: [BERT, LLMs, Transformer]

**BERT 的特点**  
- **模型定位**：典型的双向 Transformer 编码器，用于获取上下文相关的文本表示。  
- **训练目标**：Masked Language Modeling (MLM) + Next Sentence Prediction (NSP)。  
- **输入输出**：输入文本 → 编码为固定向量表示，主要用于分类、匹配、抽取等理解类任务。  
- **规模**：参数量相对较小（百M级），依赖预训练 + 下游微调。  
- **适用场景**：NLP 下游理解任务（文本分类、NER、QA 检索等）。  

**大模型 (LLMs, Large Language Models) 的特点**  
- **模型定位**：以 GPT、LLaMA、GLM 等为代表的自回归 Transformer，具备通用语言生成与推理能力。  
- **训练目标**：自回归语言建模 (Next Token Prediction)，可自然扩展到对话、推理、工具调用。  
- **输入输出**：输入 Prompt → 输出自然语言序列，可处理开放域任务。  
- **规模**：参数量巨大（B 级甚至 T 级），具备涌现能力，支持 few-shot / zero-shot 学习。  
- **适用场景**：对话、写作、代码生成、智能体、跨模态扩展等。  

**主要区别**  
1. **模型架构**：BERT = Encoder-only；LLMs = Decoder-only / Encoder-Decoder。  
2. **训练目标**：BERT 用 MLM，LLMs 用自回归预测。  
3. **能力边界**：BERT 偏向表示学习，依赖下游任务微调；LLMs 具备通用生成与推理能力。  
4. **规模与性能**：BERT 规模有限，缺乏涌现能力；LLMs 参数更大，支持跨任务迁移与 zero-shot。  
5. **应用范围**：BERT 多用于理解类任务，LLMs 同时覆盖理解与生成。  

<mark>**总结**：BERT 是典型的预训练语言理解模型，规模较小，偏向下游任务表征学习；而大模型（LLMs）规模庞大、训练范式不同，具备更强的生成和推理能力，应用范围更广。<mark>


### Q. BERT 预训练和 GPT 预训练的区别
> **Company**: 浪潮 | **Round**: 大模型算法工程师 一面 | **Date**: 2025-08-29 | **Tags**: [BERT, GPT, 预训练]

**1. 模型架构差异**  
- **BERT**：Encoder-only 架构，双向 Transformer。  
- **GPT**：Decoder-only 架构，单向 Transformer。  

**2. 预训练目标**  
- **BERT**  
  - **MLM (Masked Language Modeling)**：随机 mask 部分 token，让模型预测被 mask 的词。  
  - **NSP (Next Sentence Prediction)**：判断两句是否相邻（BERT 原版使用，RoBERTa 等后续工作去掉）。  
- **GPT**  
  - **自回归语言建模 (Next Token Prediction)**：给定前文，预测下一个 token。  
  - 训练目标更贴近生成式任务。  

**3. 训练数据与范式**  
- **BERT**：更偏向理解类任务的预训练（QA、分类、NER），常在大规模 Wikipedia + BookCorpus 等数据上训练。  
- **GPT**：在大规模网页、代码、书籍数据上进行自回归预训练，更适合生成、对话、推理等任务。  

**4. 下游适配方式**  
- **BERT**：常用 **Fine-tuning**，即在下游任务加一层分类器或任务头，做参数更新。  
- **GPT**：既可 Fine-tune，也支持 **Prompting / In-Context Learning**，zero/few-shot 泛化能力强。  

**5. 应用边界**  
- **BERT**：偏向语义理解任务，如文本分类、信息抽取、阅读理解。  
- **GPT**：偏向通用生成与推理，可覆盖理解 + 生成的广泛任务。  

<mark>**总结**：BERT 预训练基于 MLM + NSP，目标是获取深层次语义表示，主要用于理解任务；GPT 预训练基于自回归预测，天然适配生成与推理，扩展性与泛化能力更强。架构上 BERT 用 Encoder-only，GPT 用 Decoder-only，这也决定了它们在大模型时代的应用分工。<mark>


### Q. BERT 怎么针对梯度消失做的优化
> **Company**: 美团 | **Round**: 大模型算法工程师 一面 | **Date**: 2025-08-31 | **Tags**: [BERT, Transformer, 梯度消失]

**背景**  
- 在深层神经网络中，梯度消失会导致参数更新缓慢甚至停滞。  
- BERT 作为多层 Transformer（Base 12 层 / Large 24 层），在训练中必须解决梯度消失和梯度不稳定问题。  

**优化手段**  

1. **残差连接 (Residual Connection)**  
   - 每个子层输入与输出相加，保证梯度在反向传播时有“捷径”，避免梯度过度衰减。  
   - 类似 ResNet 思想，使深层网络可训练。  

2. **层归一化 (LayerNorm)**  
   - 在每个子层残差之后应用 LayerNorm，稳定梯度分布。  
   - 避免深层堆叠导致的梯度放大或消失问题。  

3. **权重初始化策略**  
   - 使用 **Xavier/He 初始化**，保证前向传播时激活值方差稳定。  
   - BERT 论文中对 Transformer 参数初始化有专门设置，确保训练初期收敛稳定。  

4. **预训练目标设计**  
   - MLM（Masked Language Modeling）和 NSP（Next Sentence Prediction）提供了丰富监督信号，梯度更新均匀，降低了梯度稀疏和不稳定的风险。  

5. **优化器与学习率调度**  
   - 使用 **AdamW 优化器**：结合权重衰减与自适应梯度，缓解梯度消失。  
   - **Warmup + Linear Decay**：先小学习率热身，再逐步衰减，避免训练初期梯度过小或过大。  

**总结**  
- **核心机制**：Residual + LayerNorm 保证梯度流动稳定。  
- **辅助机制**：合适的初始化、AdamW + Warmup 学习率调度，进一步缓解梯度消失。  
- **结果**：BERT 能稳定训练数十层 Transformer，而不会像早期 RNN 那样严重受梯度消失困扰。  

<mark>**一句话总结**：BERT 通过残差连接、LayerNorm、合理初始化和优化器调度等手段，有效缓解了深层 Transformer 中的梯度消失问题。<mark>

### Q. 为什么现在不用 Encoder–Decoder 架构的大模型了？
> **Company**: 美团 | **Round**: 大模型算法工程师 一面 | **Date**: 2025-08-31 | **Tags**: [Encoder-Decoder, LLMs, Transformer, 并行训练]

**1. 历史背景**  
- **Encoder–Decoder 架构**：最初在机器翻译中大放异彩（如 Transformer 原始论文中的 Seq2Seq），Encoder 编码输入，Decoder 逐步生成输出。  
- **典型代表**：BART、T5、mBART。  

**2. 为什么 LLM 转向 Decoder-only 架构**  

1. **训练目标更统一**  
   - Decoder-only 只需做自回归预测 (Next Token Prediction)，训练范式单一、与大规模语料天然契合。  
   - Encoder–Decoder 需要同时优化 Encoder 和 Decoder，设计上更复杂。  

2. **推理效率更高**  
   - Decoder-only 可以利用 KV Cache，仅保存过去的隐状态，推理复杂度 \(O(1)\) per token。  
   - Encoder–Decoder 必须先跑完整个 Encoder，再逐步交互 Decoder，推理路径更长，延迟更大。  

3. **训练并行与资源利用**  
   - **Encoder–Decoder 的并行难点**：  
     - 两个模块参数量级相近，训练时必须同时维护 Encoder 和 Decoder，**显存消耗更大**。  
     - Encoder 输出需广播给 Decoder，增加 **跨设备通信开销**。  
     - 数据并行、张量并行、流水线并行需要分别对 Encoder 与 Decoder 分区，**调度更复杂**。  
   - **Decoder-only 的优势**：  
     - 模型结构单一，适合 **张量并行 (TP)**、**流水线并行 (PP)**、**专家并行 (MoE/EP)** 等现代大规模分布式训练方案。  
     - 在 GPU 集群中，易于切分计算图，**通信模式简单**，大幅提升可扩展性。  

4. **工程与生态支持**
   - GPT 系列验证了 Decoder-only 的可扩展性，业界的优化工具链（如 Megatron-LM、DeepSpeed、vLLM）均围绕 Decoder-only 架构，形成正向循环。 
   - 这让开发者和硬件厂商更倾向于继续强化 Decoder-only 的生态。  

**3. Encoder–Decoder 仍有价值的场景**  
- **翻译、摘要**：输入与输出长度相近时，Encoder–Decoder 可以更高效建模。  
- **跨模态建模**：图像/语音作为 Encoder 输入，文本由 Decoder 输出，是常见范式。  
- **小规模定制模型**：在垂直领域任务中仍有应用。  

<mark>**总结**：从训练并行的角度看，Encoder–Decoder 架构需要同时维护两套大模型，通信与显存开销都更高，切分和调度更复杂；而 Decoder-only 结构单一，天然契合张量并行、流水线并行等分布式训练方式，更适合数百亿、千亿参数规模的大模型。<mark>

