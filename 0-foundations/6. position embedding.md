# Position Embedding
> 本章节记录了面试中关于位置嵌入(Position Embedding)的相关问题及答案。

### Q. Transformer架构中，为什么要使用Position Embedding？有哪些常见的实现方式？
> **Company**: 美团 ｜ **Round**: 大模型算法工程师 一面 ｜ **Date**: 2025-08-31 ｜ **Tags**: [Position Embedding, Transformer]

**为什么需要 Position Embedding**  
- **自注意力机制的特点**：  
  Transformer 的 Self-Attention 对序列输入是 **置换不变的**，即模型无法区分「ABC」和「CAB」的顺序。  
- **作用**：  
  - 为序列引入**位置信息**，使模型能够建模 **顺序依赖** 和 **相对位置关系**。  
  - 让模型具备理解 **语法结构**（NLP）和 **空间顺序**（CV中的序列建模）的能力。  

**常见实现方式**  

1. **Absolute Position Embedding（绝对位置编码）**  
   - **Sinusoidal Positional Encoding**（Transformer 原始论文）：  
     使用固定的正弦/余弦函数生成位置编码：  
     \[
     PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right),\quad
     PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
     \]  
     - **优点**：无参数、可外推到比训练序列更长的长度。  
     - **缺点**：表达能力有限，不能灵活学习任务相关的位置信息。  

   - **Learnable Position Embedding**：  
     为每个位置学习一个可训练向量（类似 word embedding）。  
     - **优点**：表达灵活，适合固定长度任务。  
     - **缺点**：序列超长时难以外推。  

2. **Relative Position Encoding（相对位置编码）**  
   - 在 Attention Score 中直接引入位置差：  
     \[
     \text{Attention}(Q,K,V) = \text{Softmax}\Big(\frac{Q(K+R)^T}{\sqrt{d}}\Big)V
     \]  
   - 代表工作：Transformer-XL、T5、DeBERTa。  
   - **优点**：建模相对位置信息，更符合自然语言结构，可泛化到长序列。  
   - **缺点**：实现更复杂，计算开销稍大。  

3. **Rotary Position Embedding (RoPE)**  
   - 将位置信息以旋转矩阵形式融入 Query/Key：  
     - 常见于 LLaMA、GPT-NeoX 等模型。  
   - **优点**：天然支持相对位置建模，外推能力好，适合长上下文。  
   - **缺点**：实现复杂度较高，但已成为主流选择。  

4. **其他变体**  
   - **ALiBi (Attention with Linear Biases)**：直接在 Attention Score 中加随距离增长的线性偏置；推理时支持无限长度外推。  
   - **混合方法**：结合绝对与相对（如 DeBERTa 的 Disentangled Position Encoding）。  

**适用场景总结**  
- **Sinusoidal**：简单高效，常用于原始 Transformer/机器翻译。  
- **Learnable Absolute**：适合固定长度任务（分类、句子对匹配）。  
- **Relative PE**：适合 NLP 序列建模（语言建模、对话）。  
- **RoPE/ALiBi**：长文本/大模型常用，主流大模型（LLaMA、ChatGLM 等）多采用。  

<mark>**总结**：Position Embedding 解决了 Transformer 无法建模顺序的问题。常见方法有固定正弦、可学习向量、相对位置编码、RoPE 与 ALiBi 等，选择取决于任务需求与序列长度。<mark>

### Q. RoPE（Rotary Position Embedding）在解决超长上下文有哪些优化的工作？
> **Company**: 美团 | **Round**: 大模型算法工程师 一面 | **Date**: 2025-08-31 | **Tags**: [RoPE, Position Embedding, 长上下文]

**1. RoPE 的核心机制**  
- RoPE 将位置信息通过 **旋转变换** 融入 Q/K 向量：  
  \[
  \text{RoPE}(x_{2i},x_{2i+1}) = 
  \begin{bmatrix}
  x_{2i}\cos\theta - x_{2i+1}\sin\theta \\
  x_{2i}\sin\theta + x_{2i+1}\cos\theta
  \end{bmatrix}
  \]  
- 其中 \(\theta \propto 10000^{-2i/d}\)，实现了 **相对位置信息编码**，天然支持长序列外推。  
- 但原始 RoPE 仍会在超长上下文（远超训练长度）出现退化。  

**2. 针对超长上下文的优化工作**  

1. **NTK-aware RoPE Scaling（LLaMA-2/3）**  
   - 通过调整旋转角度 \(\theta\) 的缩放，使得 RoPE 的频率分布在长序列下更合理。  
   - 原理：模拟 **神经切核（NTK）等效性**，在外推时减少注意力退化。  
   - 效果：上下文长度可由 2K → 32K+。  

2. **YaRN（Yet another RoPE extensioN）**  
   - 提出 **分段缩放策略**，在不同位置段采用不同的缩放函数，保证近距离保持精度、远距离不过度失真。  
   - 实验证明在 128K 上下文中仍能保持较高准确率。  

3. **Dynamic NTK / Interpolation 方法**  
   - **插值（Linear / Exponential Interpolation）**：通过在位置索引和角度映射间做插值，保证长距离仍有可区分度。  
   - **Dynamic NTK**：动态调整旋转角度分布，使得在超长序列时仍然保留相对位置信息。  

4. **Hybrid 方法**  
   - **ALiBi + RoPE**：在 Attention Score 中引入线性偏置，结合 RoPE 使用，进一步提升长距离建模。  
   - **LogN Attention / LongRoPE**：对旋转角度做对数/分段映射，使超长距离下角度增长更平滑。  

**3. 总结对比**  
| 方法              | 核心思想                           | 优点                  | 应用 |
|-------------------|----------------------------------|---------------------|------|
| **NTK-aware RoPE** | 调整角度分布（核等效）                | 简单高效，兼容原RoPE      | LLaMA 系列 |
| **YaRN**          | 分段缩放，近距精确+远距平滑             | 超长（128K）性能优      | YaRN 系列 |
| **插值 / Dynamic NTK** | 位置索引插值或动态调整频率             | 灵活，适配不同长度        | 多模型实验 |
| **ALiBi + RoPE**  | 结合线性偏置与旋转编码                 | 长距离鲁棒，泛化更好      | 部分长文本 LLM |


<mark>**总结**：RoPE 提供了相对位置编码的自然机制，但在超长上下文下会退化。主流优化方向包括 **NTK-aware Scaling、YaRN、插值/动态频率调整、混合方法（如 RoPE+ALiBi）**，显著扩展了上下文窗口（从 2K → 128K+），让大模型在长文本任务中表现更稳健。<mark>

### Q. LLMs 中的位置编码是可以学习的吗？
> **Company**: 蔚来 ｜ **Round**: 大模型算法工程师 一面 ｜ **Date**: 2025-04-08 ｜ **Tags**: [Position Embedding, Learnable, Transformer]

**1. 为什么需要位置编码**  
- Transformer 本身没有顺序感，需要显式引入位置信息。  
- 常见方式分为：**固定型** 和 **可学习型**。  

**2. 常见位置编码方式**  
- **Sinusoidal（正弦余弦）**：基于固定函数计算，**不可学习**。  
- **Learnable Embedding**：每个位置一个向量参数，**可学习**，训练中会更新。  
- **RoPE（Rotary Position Embedding）**：通过旋转变换引入相对位置信息，**不可学习**（旋转角度由固定公式决定）。  
- **ALiBi**：在注意力得分中加入线性偏置，**不可学习**，系数通常手动设定。  
- **Relative Position Embedding**：基于相对位置的可训练向量，**可学习**。  

**3. 总结**  
<mark>位置编码既有固定的（如 Sinusoidal、RoPE、ALiBi），也有可学习的（如 Learnable Embedding、部分 Relative Encoding）。当前 LLM 多采用 RoPE 或 ALiBi，它们本身不含可训练参数，稳定性更好；而 Learnable Embedding 在 BERT、GPT-2 等模型中使用，参数会随训练更新。</mark>