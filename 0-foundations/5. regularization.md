# Regularization
> 本节记录模型正则化相关的面试题目，包含但不限于 Dropout、Weight Decay、数据增强、早停等技术。

### Q. LayerDrop和Dropout，两者的区别及各自适用场景
> **Company**: 美团 ｜ **Round**: 大模型算法工程师 一面 ｜ **Date**: 2025-08-31 ｜ **Tags**: [LayerDrop, Dropout, 正则化]

**Dropout**  
- **原理**：在训练过程中随机屏蔽部分神经元的激活，打破特征共适应，提升泛化能力。  
- **特点**：细粒度（作用在神经元/连接级别），推理时关闭。  
- **适用场景**：  
  - **分类/回归模型**：提升泛化性能，尤其在数据量有限时有效。  
  - **图像/文本模型的全连接层**：常规正则化手段。  
  - **小规模任务**：防止模型过拟合。  

**LayerDrop**  
- **原理**：在训练过程中随机跳过整个 Transformer 层或子层，让模型学会在缺少层的情况下仍保持稳定。  
- **特点**：粗粒度（作用在层级别），训练出的模型天然支持裁剪。  
- **适用场景**：  
  - **大规模预训练模型**：训练时启用 LayerDrop，推理时可按需求裁剪层数，加速推理。  
  - **多设备部署**：例如在服务器上用完整模型，在移动端只保留部分层。  
  - **多阶段任务**：需要兼顾高性能和轻量化推理的应用（如搜索、推荐、NLP在线服务）。  

<mark>**总结**：Dropout 适合小模型和数据有限场景，用于泛化正则化；LayerDrop 主要用于大模型，增强鲁棒性并支持推理时的动态裁剪，适合工业级部署和资源受限场景。<mark>
