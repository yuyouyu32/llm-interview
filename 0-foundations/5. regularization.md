# Regularization
> 本节记录模型正则化相关的面试题目，包含但不限于 Dropout、Weight Decay、数据增强、早停等技术。

### Q. L1和L2正则化的区别
> **Company**: 字节跳动 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-27 ｜ **Tags**: [L1正则化, L2正则化, 权重衰减]

**公式形式**  
- **L1 正则化**：在损失函数中加入参数绝对值和  
\[
L_{reg} = \lambda \sum_i |w_i|
\]  
- **L2 正则化**：在损失函数中加入参数平方和  
\[
L_{reg} = \lambda \sum_i w_i^2
\]  

**几何直观**  
- L1 的约束区域是菱形（稀疏解更容易出现在坐标轴上）。  
- L2 的约束区域是圆形（解倾向于均匀收缩，而非置零）。  

**效果差异**  
- **L1 正则化**：  
  - 产生稀疏权重（很多参数被压到 0）。  
  - 有特征选择的效果，适合高维稀疏数据。  
  - 对异常值更鲁棒。  
- **L2 正则化**：  
  - 不会强制置零，而是整体缩小权重。  
  - 避免过拟合，保持所有特征都有一定贡献。  
  - 在优化时更稳定，常见于权重衰减 (Weight Decay)。  

**在深度学习中的应用**  
- L2 正则化（权重衰减）更常见，因为能提高数值稳定性。  
- L1 更常见于需要稀疏性的模型（如特征选择、压缩）。  

<mark>总结：L1 强调“稀疏和特征选择”，L2 强调“整体收缩和稳定性”。实际应用中，深度模型更常用 L2（权重衰减），而 L1 常用于需要稀疏性的场景。</mark>


### Q. LayerDrop和Dropout，两者的区别及各自适用场景
> **Company**: 美团 ｜ **Round**: 大模型算法工程师 一面 ｜ **Date**: 2025-08-31 ｜ **Tags**: [LayerDrop, Dropout, 正则化]

**Dropout**  
- **原理**：在训练过程中随机屏蔽部分神经元的激活，打破特征共适应，提升泛化能力。  
- **特点**：细粒度（作用在神经元/连接级别），推理时关闭。  
- **适用场景**：  
  - **分类/回归模型**：提升泛化性能，尤其在数据量有限时有效。  
  - **图像/文本模型的全连接层**：常规正则化手段。  
  - **小规模任务**：防止模型过拟合。  

**LayerDrop**  
- **原理**：在训练过程中随机跳过整个 Transformer 层或子层，让模型学会在缺少层的情况下仍保持稳定。  
- **特点**：粗粒度（作用在层级别），训练出的模型天然支持裁剪。  
- **适用场景**：  
  - **大规模预训练模型**：训练时启用 LayerDrop，推理时可按需求裁剪层数，加速推理。  
  - **多设备部署**：例如在服务器上用完整模型，在移动端只保留部分层。  
  - **多阶段任务**：需要兼顾高性能和轻量化推理的应用（如搜索、推荐、NLP在线服务）。  

<mark>**总结**：Dropout 适合小模型和数据有限场景，用于泛化正则化；LayerDrop 主要用于大模型，增强鲁棒性并支持推理时的动态裁剪，适合工业级部署和资源受限场景。<mark>
