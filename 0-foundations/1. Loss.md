# LLMs Loss Functions
> 本章节记录 LLMs 训练、推理中损失函数相关的面试题目。



### Q. 大模型预测 token 的损失是怎么算的？有哪几种常见的损失函数？
> **Company**: 快手 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-01 ｜ **Tags**: [损失函数, 交叉熵, KL散度]

**1. Token 预测的基本损失计算**  
- 大模型在训练时，会根据上下文预测下一个 token 的概率分布：  

$$
P_\theta(y_t \mid y_{<t})
$$  

- 真实 token 是 one-hot 向量，损失函数通常度量 **真实分布 $q$ 与预测分布 $p$ 的差异**。  

**2. 常见的损失函数**

1. **交叉熵损失（Cross-Entropy Loss）**  
   定义：  
   $$
   L = - \sum_t \sum_i q(y_t=i)\,\log p_\theta(y_t=i \mid y_{<t})
   $$  

   one-hot 情况下可简化为：  
   $$
   L = - \sum_t \log p_\theta(y_t = y_t^{\text{true}} \mid y_{<t})
   $$  

   本质上等价于最小化 **负对数似然（NLL）**。  

2. **KL 散度（Kullback–Leibler Divergence）**  
   定义：  
   $$
   D_{KL}(q \parallel p) = \sum_i q(i)\,\log \frac{q(i)}{p(i)}
   $$  

   在 token 预测任务中，$q$ 为 one-hot 分布，此时 KL 与交叉熵只差一个常数。  

3. **标签平滑交叉熵（Label Smoothing Cross-Entropy）**  
   真实标签分配概率 $(1-\epsilon)$，其余类别均分 $\epsilon/(V-1)$（$V$ 为词表大小）：  
   $$
   q'(i) =
   \begin{cases}
   1-\epsilon & i = y_t^{\text{true}} \\
   \frac{\epsilon}{V-1} & i \neq y_t^{\text{true}}
   \end{cases}
   $$  

   可以缓解过拟合并提升泛化。  

4. **其他变种**  
   - Margin-based Loss：在分类边界上加入间隔约束。  
   - Focal Loss：降低简单样本权重，聚焦难样本。  
   - InfoNCE / 对比学习损失：用于对比学习或检索类预训练。  



<mark>
- 主流大模型预训练时几乎都用 **交叉熵损失**（= NLL）。 
- **KL 散度** 常用于蒸馏和分布对齐。  
- **标签平滑** 是常见 trick，能提升泛化。  
- 特殊任务下会扩展到对比损失、Focal Loss 等。  
</mark>


### Q. 常见的对比学习 Loss 有哪些？
> **Company**: 科大讯飞 ｜ **Round**: 飞星计划算法工程师 一面 ｜ **Date**: 2025-04-01 ｜ **Tags**: [对比学习, Contrastive Loss, InfoNCE]

**1. InfoNCE Loss（最常用）**  
- **定义**：最大化正样本相似度，最小化正样本与所有负样本的相似度。  
- **公式**：  
  $$
  L = -\log \frac{\exp(\text{sim}(q,k^+)/\tau)}{\sum_{i=1}^N \exp(\text{sim}(q,k_i)/\tau)}
  $$
  其中 $\tau$ 是温度参数。  
- **特点**：广泛用于 SimCLR、CLIP 等模型，适合大规模预训练。  

**2. Triplet Loss**  
- **定义**：利用三元组 (Anchor, Positive, Negative)，希望正样本比负样本更接近 Anchor，且差距大于 margin。  
- **公式**：  
  $$
  L = \max(0, d(a,p) - d(a,n) + m)
  $$
- **特点**：直观，但对负样本选择较敏感，常需 hard negative mining。  

**3. Margin-based Contrastive Loss（对比损失）**  
- **定义**：直接用 margin 约束正样本距离小，负样本距离大。  
- **公式**：  
  $$
  L = y \cdot d(x_1,x_2)^2 + (1-y) \cdot \max(0, m - d(x_1,x_2))^2
  $$
- **特点**：用于孪生网络（Siamese Network），常见于人脸验证。  

**4. NT-Xent Loss（Normalized Temperature-scaled Cross Entropy）**  
- **定义**：InfoNCE 的归一化版本，强调样本间对称性。  
- **应用**：SimCLR 的核心 loss，提升训练稳定性。  

**5. Multi-Similarity Loss**  
- **定义**：综合考虑多种相似度度量（cosine、euclidean），对 hard positive/negative 赋予更大权重。  
- **应用**：在度量学习和细粒度分类任务中常用。  

**6. Supervised Contrastive Loss**  
- **定义**：扩展 InfoNCE，允许多个正样本，适合监督学习场景。  
- **应用**：在图像分类、文本表示中广泛应用。  

**7. Other Variants**  
- **ArcFace / CosFace / SphereFace**：在人脸识别中常用，加入角度 margin，使类间间隔更大。  
- **Hinge-based Loss**：类似于 SVM，使用 margin 约束。  
- **Debiased / Hard Negative Loss**：解决负样本分布偏差问题。  

✅ **总结**  
<mark>常见的对比学习 loss 包括 InfoNCE、Triplet、Contrastive、NT-Xent、Multi-Similarity、Supervised Contrastive，以及 ArcFace 等变体。InfoNCE 及其扩展是当前主流，Triplet 与 Contrastive 多用于经典度量学习，人脸识别领域常用 margin-based 变体。</mark>


## 1. 交叉熵损失（Cross Entropy Loss）

### Q. CE Loss的公式和代码实现
> **Company**: bilibili ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-23 ｜ **Tags**: [交叉熵, 损失函数]

- **交叉熵损失公式**  
  对于真实标签分布 $y$ 和预测概率分布 $\hat{y}$：  
  $$
  L = - \sum_{i=1}^C y_i \log \hat{y}_i
  $$

  其中概率用**softmax**归一化表示：
    $$
    \hat{y}_i = \frac{\exp(z_i - \max_k z_k)}{\sum_{j=1}^C \exp(z_j - \max_k z_k)}
    $$
**减最大值**是为了数值稳定，防止指数函数爆炸。

```python
  import numpy as np

  def softmax(logits):
    """
    softmax(logits) = exp(logits - max) / sum(exp(logits - max))
    """
    exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # 数值稳定
    return exps / np.sum(exps, axis=1, keepdims=True)

  def cross_entropy_loss(logits, labels):
    """
    logits: shape (batch_size, num_classes)
    labels: shape (batch_size,)  每个元素是类别索引
    """
    probs = softmax(logits)
    batch_size = logits.shape[0]
    # 取出正确类别的概率
    correct_probs = probs[np.arange(batch_size), labels]
    # 计算交叉熵损失
    loss = -np.mean(np.log(correct_probs + 1e-12))  # 加上1e-12防止log(0)
    return loss

  # 示例
  logits = np.array([[2.0, 0.5, 0.3],
                     [0.1, 0.2, 1.5]])  # 预测logits
  labels = np.array([0, 2])             # 正确标签
  loss = cross_entropy_loss(logits, labels)
  print("CE Loss:", loss)
```
<mark>交叉熵损失就是取出预测分布中正确类别的概率取 $-\log$，numpy 实现时记得先做 softmax，再按标签索引取值。**错误类别的概率不用单独加 loss，它们自然会被 softmax 约束（总和=1），正确类别概率升高，错误类别概率自然下降。**</mark>


### Q.为什么大模型损失函数采用交叉熵损失函数，而不是MSE损失函数？
> **Company**: None | **Round**: None | **Date**: None | **Tags**: [交叉熵, 损失函数, MSE]

- **任务本质不同**  
  - 分类任务是“选对标签”，关注的是 **概率分布的正确性**；  
  - MSE 衡量的是“数值距离”，适合回归任务，比如预测房价。  
  - 但“猫”和“狗”之间没有数值距离，所以 MSE 不适合分类。

- **交叉熵的优势**  
  1. **直击概率分布**：交叉熵专门为分类任务设计，直接衡量预测分布和真实分布的差异。  
  2. **错得越离谱，罚得越狠**：如果正确类概率很低，交叉熵会产生巨大的 loss，迫使模型快速调整。  
  3. **梯度表现更好**：  
     - 交叉熵的梯度与预测概率挂钩，错误预测时梯度大，更新更快；  
     - MSE 在概率低时梯度非常小，容易导致 **梯度消失**，训练停滞。  

- **举例说明**  
  - 模型输出 (猫=0.9, 狗=0.05, 苹果=0.05)，真实标签是猫 (1,0,0)。  
    - **交叉熵**：只盯“猫”，$-\log(0.9)$，loss 很小；若猫=0.1，则 $-\log(0.1)$ 巨大，梯度猛。  
    - **MSE**：算整体差异 $(0.9-1)^2 + (0.05-0)^2 + (0.05-0)^2$，即使猫预测 0.1，loss 也不会太大，梯度偏弱。  

- **一句话总结**  
  <mark>交叉熵是为分类量身定做的，能直接优化概率分布并避免梯度消失；MSE 更适合回归任务，硬用在分类上会导致训练缓慢甚至失败。</mark>

### Q. 在 LLMs 训练中，交叉熵损失 (CE Loss) 是如何计算的？  
> **Company**: None ｜ **Round**: None ｜ **Date**: None ｜ **Tags**: [交叉熵, 损失函数]

- **本质任务**：语言模型训练是一个 **下一个 token 的分类问题**。  
  - 词表大小通常几十万（50K+）。  
  - 每个位置预测下一个 token 的概率分布 $P_\theta(x_t|x_{<t})$。  

- **CE Loss 公式**：  
  $$
  L = -\frac{1}{T}\sum_{t=1}^T \log P_\theta(x_t \mid x_{<t})
  $$  
  👉 即在每个时间步只取出 **正确 token 的概率**，计算 $-\log(p)$，再对整个序列求平均。  

- **为什么只算正确 token？**  
  - 标签是 one-hot，只有目标 token 为 1，其他为 0。  
  - softmax 确保总概率=1，正确 token 概率上升 → 其他 token 概率自然下降。  

- **特殊情况**：  
  1. **Label Smoothing**：把 one-hot “抹平”，正确 token 概率从 1 变 $1-\epsilon$，其他类别分摊 $\epsilon$，loss 涉及全 vocab。  
  2. **Padding Mask**：padding 位置不计算 loss。  
  3. **Teacher Forcing**：训练时用真实序列对齐，逐 token 计算 loss；推理时才逐步采样生成。  

---

**一句话总结：**  
<mark>LLMs 的 CE Loss 就是“每个时间步取正确 token 的 $-\log(p)$，对序列和 batch 求平均”，本质上是大规模多分类任务的交叉熵。</mark>


## 2. KL 散度

### Q. KL散度和交叉熵之间的关系？
> **Company**: 阿里国际 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01 ｜ **Tags**: [KL散度, 交叉熵, 损失函数]

- **交叉熵 (Cross Entropy)**  
  度量两个分布 $P$ 和 $Q$ 之间的差异：  
  $$
  H(P, Q) = - \sum_x P(x) \log Q(x)
  $$
  直观理解：如果真实分布是 $P$，用模型 $Q$ 来编码，需要多少比特。

- **KL 散度 (Kullback–Leibler Divergence)**  
  描述分布 $P$ 相对于 $Q$ 的“额外信息量”：  
  $$
  D_{KL}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
  $$

- **两者的关系**  
  展开 KL 散度公式：  
  $$
  D_{KL}(P \| Q) = \sum_x P(x)\log P(x) - \sum_x P(x)\log Q(x)
  $$
  其中：  
  - $\sum_x P(x)\log P(x) = -H(P)$ 是分布 $P$ 的熵（与 $Q$ 无关，固定值）；  
  - $\;-\sum_x P(x)\log Q(x) = H(P,Q)$，就是交叉熵。  

  所以：  
  $$
  D_{KL}(P \| Q) = H(P, Q) - H(P)
  $$

- **直观理解**  
  - KL 散度 = 交叉熵 − 熵。  
  - 因为 $H(P)$ 固定，**最小化 KL 散度 ⇔ 最小化交叉熵**。  
  - 在分类任务中，我们常用的 **交叉熵损失** 本质就是在最小化 KL 散度。

| 维度       | KL 散度                                   | 交叉熵                                   |
|------------|-------------------------------------------|------------------------------------------|
| **本质**   | 衡量分布的相对差异（信息损失）            | 衡量预测分布的绝对编码代价               |
| **对称性** | 非对称（方向敏感）                        | 非对称，但在分类中对固定 $P$ 单向优化    |
| **取值范围** | ≥0，且 $P=Q$ 时为 0                      | 可能大于真实分布的熵，优化时等价于 KL 散度 |
| **应用侧重点** | 分布差异量化、无监督学习                | 直接优化预测概率，监督学习分类任务        |
| **数值稳定性** | 需处理 $Q(x)=0$ 的极端情况              | 计算更高效，仅需 $\log Q(x)$             |

- **实际选择建议**  
  - **优先使用 KL 散度**：  
    1. 需要精确量化分布差异（如 VAE 正则化、知识蒸馏）。  
    2. 需要对称性优化的任务（如让模型逼近某个分布）。  
  - **优先使用 交叉熵**：  
    1. 监督学习分类任务（标签固定，优化目标明确）。  
    2. 需要高效计算梯度的场景（如神经网络反向传播）。  


<mark>交叉熵是 KL 散度的组成部分，分类任务里优化交叉熵等价于优化 KL 散度；KL 更适合分布建模，交叉熵更适合分类与高效训练。</mark>

### Q. KL 散度和交叉熵的区别？能用通俗例子解释吗？  
> **Company**: None ｜ **Round**: None ｜ **Date**: None ｜ **Tags**: [KL散度, 交叉熵, 损失函数]

- **生活类比：考试答题**  
  - **真实分布 P（标准答案）**：假设选择题答案是 A → $P = (1, 0, 0, 0)$  
  - **预测分布 Q（学生答题策略）**：学生给出概率分布 → $Q = (0.7, 0.2, 0.05, 0.05)$  

- **交叉熵 (Cross-Entropy)**  
  - 只关心 **学生对正确答案 A 的信心**。  
  - 计算：$-\log(0.7)$  
  - 通俗理解：**你对正确答案押了几分？**

- **KL 散度 (KL Divergence)**  
  - 衡量 **整体分布差异**，不仅看 A，还惩罚在 B/C/D 上分的概率。  
  - 通俗理解：**你的整张答卷和标准答案有多大差距？**

- **为什么分类任务常用交叉熵？**  
  - 标签是 one-hot，$H(P)=0$，所以 CE 和 KL 只差一个常数。  
  - **最小化 CE = 最小化 KL**，计算更简单高效。  

<mark>交叉熵像是“看你对正确答案押了几分”，KL 散度则是“整份答卷和标准答案的差距”；分类任务里交叉熵更直接，分布建模任务才用 KL。</mark>
