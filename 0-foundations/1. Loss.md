# LLMs Loss Functions
> 本章节记录 LLMs 训练、推理中损失函数相关的面试题目。

## 1. 交叉熵损失（Cross Entropy Loss）

### Q. CE Loss的公式和代码实现
> **Company**: bilibili ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-23 ｜ **Tags**: [交叉熵, 损失函数]

- **交叉熵损失公式**  
  对于真实标签分布 $y$ 和预测概率分布 $\hat{y}$：  
  $$
  L = - \sum_{i=1}^C y_i \log \hat{y}_i
  $$

  其中概率用**softmax**归一化表示：
    $$
    \hat{y}_i = \frac{\exp(z_i - \max_k z_k)}{\sum_{j=1}^C \exp(z_j - \max_k z_k)}
    $$
**减最大值**是为了数值稳定，防止指数函数爆炸。

```python
  import numpy as np

  def softmax(logits):
    """
    softmax(logits) = exp(logits - max) / sum(exp(logits - max))
    """
    exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # 数值稳定
    return exps / np.sum(exps, axis=1, keepdims=True)

  def cross_entropy_loss(logits, labels):
    """
    logits: shape (batch_size, num_classes)
    labels: shape (batch_size,)  每个元素是类别索引
    """
    probs = softmax(logits)
    batch_size = logits.shape[0]
    # 取出正确类别的概率
    correct_probs = probs[np.arange(batch_size), labels]
    # 计算交叉熵损失
    loss = -np.mean(np.log(correct_probs + 1e-12))  # 加上1e-12防止log(0)
    return loss

  # 示例
  logits = np.array([[2.0, 0.5, 0.3],
                     [0.1, 0.2, 1.5]])  # 预测logits
  labels = np.array([0, 2])             # 正确标签
  loss = cross_entropy_loss(logits, labels)
  print("CE Loss:", loss)
```
<mark>交叉熵损失就是取出预测分布中正确类别的概率取 $-\log$，numpy 实现时记得先做 softmax，再按标签索引取值。**错误类别的概率不用单独加 loss，它们自然会被 softmax 约束（总和=1），正确类别概率升高，错误类别概率自然下降。**</mark>


### Q.为什么大模型损失函数采用交叉熵损失函数，而不是MSE损失函数？
> **Company**: None | **Round**: None | **Date**: None | **Tags**: [交叉熵, 损失函数, MSE]

- **任务本质不同**  
  - 分类任务是“选对标签”，关注的是 **概率分布的正确性**；  
  - MSE 衡量的是“数值距离”，适合回归任务，比如预测房价。  
  - 但“猫”和“狗”之间没有数值距离，所以 MSE 不适合分类。

- **交叉熵的优势**  
  1. **直击概率分布**：交叉熵专门为分类任务设计，直接衡量预测分布和真实分布的差异。  
  2. **错得越离谱，罚得越狠**：如果正确类概率很低，交叉熵会产生巨大的 loss，迫使模型快速调整。  
  3. **梯度表现更好**：  
     - 交叉熵的梯度与预测概率挂钩，错误预测时梯度大，更新更快；  
     - MSE 在概率低时梯度非常小，容易导致 **梯度消失**，训练停滞。  

- **举例说明**  
  - 模型输出 (猫=0.9, 狗=0.05, 苹果=0.05)，真实标签是猫 (1,0,0)。  
    - **交叉熵**：只盯“猫”，$-\log(0.9)$，loss 很小；若猫=0.1，则 $-\log(0.1)$ 巨大，梯度猛。  
    - **MSE**：算整体差异 $(0.9-1)^2 + (0.05-0)^2 + (0.05-0)^2$，即使猫预测 0.1，loss 也不会太大，梯度偏弱。  

- **一句话总结**  
  <mark>交叉熵是为分类量身定做的，能直接优化概率分布并避免梯度消失；MSE 更适合回归任务，硬用在分类上会导致训练缓慢甚至失败。</mark>

### Q. 在 LLMs 训练中，交叉熵损失 (CE Loss) 是如何计算的？  
> **Company**: None ｜ **Round**: None ｜ **Date**: None ｜ **Tags**: [交叉熵, 损失函数]

- **本质任务**：语言模型训练是一个 **下一个 token 的分类问题**。  
  - 词表大小通常几十万（50K+）。  
  - 每个位置预测下一个 token 的概率分布 $P_\theta(x_t|x_{<t})$。  

- **CE Loss 公式**：  
  $$
  L = -\frac{1}{T}\sum_{t=1}^T \log P_\theta(x_t \mid x_{<t})
  $$  
  👉 即在每个时间步只取出 **正确 token 的概率**，计算 $-\log(p)$，再对整个序列求平均。  

- **为什么只算正确 token？**  
  - 标签是 one-hot，只有目标 token 为 1，其他为 0。  
  - softmax 确保总概率=1，正确 token 概率上升 → 其他 token 概率自然下降。  

- **特殊情况**：  
  1. **Label Smoothing**：把 one-hot “抹平”，正确 token 概率从 1 变 $1-\epsilon$，其他类别分摊 $\epsilon$，loss 涉及全 vocab。  
  2. **Padding Mask**：padding 位置不计算 loss。  
  3. **Teacher Forcing**：训练时用真实序列对齐，逐 token 计算 loss；推理时才逐步采样生成。  

---

**一句话总结：**  
<mark>LLMs 的 CE Loss 就是“每个时间步取正确 token 的 $-\log(p)$，对序列和 batch 求平均”，本质上是大规模多分类任务的交叉熵。</mark>


## 2. KL 散度

### Q. KL散度和交叉熵之间的关系？
> **Company**: 阿里国际 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01 ｜ **Tags**: [KL散度, 交叉熵, 损失函数]

- **交叉熵 (Cross Entropy)**  
  度量两个分布 $P$ 和 $Q$ 之间的差异：  
  $$
  H(P, Q) = - \sum_x P(x) \log Q(x)
  $$
  直观理解：如果真实分布是 $P$，用模型 $Q$ 来编码，需要多少比特。

- **KL 散度 (Kullback–Leibler Divergence)**  
  描述分布 $P$ 相对于 $Q$ 的“额外信息量”：  
  $$
  D_{KL}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
  $$

- **两者的关系**  
  展开 KL 散度公式：  
  $$
  D_{KL}(P \| Q) = \sum_x P(x)\log P(x) - \sum_x P(x)\log Q(x)
  $$
  其中：  
  - $\sum_x P(x)\log P(x) = -H(P)$ 是分布 $P$ 的熵（与 $Q$ 无关，固定值）；  
  - $\;-\sum_x P(x)\log Q(x) = H(P,Q)$，就是交叉熵。  

  所以：  
  $$
  D_{KL}(P \| Q) = H(P, Q) - H(P)
  $$

- **直观理解**  
  - KL 散度 = 交叉熵 − 熵。  
  - 因为 $H(P)$ 固定，**最小化 KL 散度 ⇔ 最小化交叉熵**。  
  - 在分类任务中，我们常用的 **交叉熵损失** 本质就是在最小化 KL 散度。

| 维度       | KL 散度                                   | 交叉熵                                   |
|------------|-------------------------------------------|------------------------------------------|
| **本质**   | 衡量分布的相对差异（信息损失）            | 衡量预测分布的绝对编码代价               |
| **对称性** | 非对称（方向敏感）                        | 非对称，但在分类中对固定 $P$ 单向优化    |
| **取值范围** | ≥0，且 $P=Q$ 时为 0                      | 可能大于真实分布的熵，优化时等价于 KL 散度 |
| **应用侧重点** | 分布差异量化、无监督学习                | 直接优化预测概率，监督学习分类任务        |
| **数值稳定性** | 需处理 $Q(x)=0$ 的极端情况              | 计算更高效，仅需 $\log Q(x)$             |

- **实际选择建议**  
  - **优先使用 KL 散度**：  
    1. 需要精确量化分布差异（如 VAE 正则化、知识蒸馏）。  
    2. 需要对称性优化的任务（如让模型逼近某个分布）。  
  - **优先使用 交叉熵**：  
    1. 监督学习分类任务（标签固定，优化目标明确）。  
    2. 需要高效计算梯度的场景（如神经网络反向传播）。  


<mark>交叉熵是 KL 散度的组成部分，分类任务里优化交叉熵等价于优化 KL 散度；KL 更适合分布建模，交叉熵更适合分类与高效训练。</mark>

### Q. KL 散度和交叉熵的区别？能用通俗例子解释吗？  
> **Company**: None ｜ **Round**: None ｜ **Date**: None ｜ **Tags**: [KL散度, 交叉熵, 损失函数]

- **生活类比：考试答题**  
  - **真实分布 P（标准答案）**：假设选择题答案是 A → $P = (1, 0, 0, 0)$  
  - **预测分布 Q（学生答题策略）**：学生给出概率分布 → $Q = (0.7, 0.2, 0.05, 0.05)$  

- **交叉熵 (Cross-Entropy)**  
  - 只关心 **学生对正确答案 A 的信心**。  
  - 计算：$-\log(0.7)$  
  - 通俗理解：**你对正确答案押了几分？**

- **KL 散度 (KL Divergence)**  
  - 衡量 **整体分布差异**，不仅看 A，还惩罚在 B/C/D 上分的概率。  
  - 通俗理解：**你的整张答卷和标准答案有多大差距？**

- **为什么分类任务常用交叉熵？**  
  - 标签是 one-hot，$H(P)=0$，所以 CE 和 KL 只差一个常数。  
  - **最小化 CE = 最小化 KL**，计算更简单高效。  

<mark>交叉熵像是“看你对正确答案押了几分”，KL 散度则是“整份答卷和标准答案的差距”；分类任务里交叉熵更直接，分布建模任务才用 KL。</mark>

