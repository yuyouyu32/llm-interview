# LLMs' Tokenizer
> 本章节整理大模型常用的 Tokenizer 相关知识。

### Q. LLMs 是怎么把文本转成 token 的？
> **Company**: 快手 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-01 ｜ **Tags**: [Tokenizer, Tokenization, BPE, BBPE]

**1. 基本过程**  
- 大模型不能直接处理自然语言字符，而是需要将文本拆分为 **token**（子词或字符片段）。  
- 这个过程由 **Tokenizer** 完成，最终输出 token ID 序列，作为模型的输入。  

**2. 常见的切分方法**  
1. **基于词典（Word-level Tokenization）**  
   - 直接将词作为基本单元，简单直观。  
   - 缺点：词表过大（几十万甚至百万），对未登录词（OOV）无能为力。  
2. **子词级方法（Subword Tokenization）**  
   - **BPE（Byte Pair Encoding）**：  
     - 初始以字符为单位；  
     - 迭代合并出现频率最高的相邻 token；  
     - 最终得到一个平衡的子词词表。  
     - 优点：压缩词表、支持 OOV（未登录词会被拆解成更小的子词）。  
   - **Unigram LM**：通过概率模型选择最优的子词切分，代表工具为 SentencePiece。  
3. **字节级方法（Byte-level BPE, BBPE）**  
   - 以字节为基础再做 BPE，例如 GPT 系列使用的 GPT-2 tokenizer。  
   - 好处：不依赖具体语言字符集，对各种 Unicode 文本都稳健（包括 emoji、罕见符号）。  
4. **字符级（Character-level）**  
   - 每个字符对应一个 token。  
   - 词表极小，但序列长度过长，训练效率差。  

**3. 编码流程举例（以 GPT-2 BBPE 为例）**  
- 输入："unhappiness"  
- 初始字节：`u n h a p p i n e s s`  
- BPE 合并：  
  - `un` → `un`  
  - `happiness` → `happi` + `ness`  
- 最终 token 序列：[`un`, `happi`, `ness`]  

**4. 解码流程**  
- 将 token ID 映射回 token 字符串。  
- 将 token 拼接还原为原始文本（可能需要处理空格等）。
如果是 BBPE，则将字节序列还原为字符。

**5. 工程实践要点**  
- **词表规模**：常见在 30K–100K 之间（如 GPT-2 是 50K）。  
- **跨语言支持**：BBPE 适合多语言；中文模型通常结合字符 + BPE。  
- **压缩率与效率**：token 越少 → 序列更短 → 推理/训练效率更高。  
- **一致性**：训练和推理必须使用同一个 tokenizer，否则 token 对齐出错。  

<mark>核心：LLMs 通过 Tokenizer 将文本切分成子词或字节单元并映射成 ID 序列，从而能被模型理解与处理。</mark>

### Q. BPE 和 BBPE 的区别是什么？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [BPE, BBPE, 分词, Tokenization]

- **BPE (Byte Pair Encoding)**  
  - **基本思想**：从最小的单位（通常是字符/字节）开始，统计语料中最频繁的相邻符号对，不断合并成新 token。  
  - **优点**：  
    - 高频词可被整体编码 → 压缩长度。  
    - 高频子词组合能有效覆盖低频词。  
  - **缺点**：  
    - 依赖语料统计 → 跨域时可能 OOV（需要退回字符级）。  
    - 固定合并规则，缺乏上下文感知。  

- **BBPE (Byte-level BPE)**  
  - **基本思想**：直接在 **字节 (byte)** 层面应用 BPE，而不是字符层面。  
  - **特点**：  
    - 覆盖所有输入（任何语言、符号、表情），不会 OOV。  
    - 保证 tokenizer 的通用性，特别适合多语言和跨域输入。  
    - 典型应用：GPT-2 / GPT-3 使用的 **BBPE 分词**。  

- **区别总结**  
  | 特性 | BPE | BBPE |
  |------|-----|------|
  | **操作单位** | 字符/子词 | 字节 (byte) |
  | **OOV 处理** | 可能退回字符级 | 不存在 OOV，任何输入都能拆成 byte |
  | **跨语言能力** | 依赖训练语料 | 通用性更强 |
  | **应用场景** | 单语/特定领域 | 多语言、大模型通用 tokenizer |

<mark>核心：BPE 在字符层面合并，可能有 OOV 问题；BBPE 在字节层面合并，彻底消除 OOV，更适合多语言和通用大模型。</mark>


### ### Q. 现在的大模型使用的tokenizer是什么？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [Tokenizer, BPE, BBPE, LLM]

- **早期 LLMs**  
  - **GPT-2 / GPT-3**：确实用的是 **BBPE (Byte-level BPE)**，起点是字节，保证无 OOV。  
  - 这是最经典的分词方式之一，现在还在不少模型里用（如 HuggingFace GPT2Tokenizer）。  

- **近期主流模型**  
  - **GPT-4 / GPT-4o / Claude / Gemini 等**：  
    - 内部实现细节未完全公开，但大多仍基于 **字节级 BPE** 或类似思想（保证通用性）。  
  - **LLaMA 系列**：使用 **SentencePiece + BPE**（实际上也是一种 BBPE 的变体），保证跨语言通用。  
  - **Mistral / Falcon / Bloom 等**：多数也基于 **SentencePiece BPE/Unigram**，本质还是 **子词级 + 字节兜底**。  
  - **中文模型（如 ChatGLM, Qwen）**：  
    - 采用 **中英文混合词表**，常用 SentencePiece（BPE/Unigram），汉字/拼音/字节结合，保证中文可切分。  

- **核心点**  
  - ✅ 现在的 LLM **几乎都不是纯“词级”分词**，而是 **子词级 (subword)**。  
  - ✅ 大多数模型采用 **字节级兜底 (BBPE or Unigram with byte-fallback)** → **无 OOV**。  
  - ✅ 不同模型的差异主要在于 **词表设计**（偏英文还是多语言，是否单独优化中文）。  

**结论**：  
现在的 LLMs 基本都还是基于 **BBPE 或 SentencePiece (带字节兜底)** 的方案，不会 OOV。区别在于：  
- 英文模型 → BBPE / BPE。  
- 多语言模型 → SentencePiece + byte fallback。  
- 中文模型 → 特殊优化（汉字 + 拼音 + byte fallback）。  

<mark>核心：现代 LLMs 分词方案依旧是 **子词级别 + 字节兜底**，不是传统“词级切分”，所以能稳定处理任何符号和语言。</mark>

### Q. Tokenizer 遇到词表外（OOV）的数据会怎么办？

- **传统 BPE**：回退到更细的子词或字符切分，保证能切分，但 token 数量会增加。  
- **BBPE / SentencePiece (byte fallback)**：直接拆成字节序列 → 不会 OOV，但序列变长。  



### Q. 什么是 Tokenizer 的压缩率？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [Tokenizer, OOV, 压缩率]

- 定义：**压缩率 = 原始文本长度（字符/字节数） ÷ Token 数**。  
- 举例：100 字符文本切成 50 个 token → 压缩率 = 2。  
- 含义：衡量每个 token 平均能表示多少原始信息。  



### Q. 压缩率越大越好还是越小越好？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [Tokenizer, OOV, 压缩率]

- **压缩率过大（词级切分）**：序列短、效率高，但词表大，OOV 严重。  
- **压缩率过小（字符级切分）**：无 OOV，词表小，但序列过长，计算量大。  
- 因此：压缩率既不能太大也不能太小。  

### Q. 压缩率与 OOV 之间如何折中？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [Tokenizer, OOV, 压缩率]

- **子词级分词 (Subword Tokenization)**：  
  - 高频词 → 整体 token，保证压缩率。  
  - 低频词 → 拆成子词/字节，保证泛化。  
- 实际工程中：  
  - 词表大小通常 3万–10万，压缩率一般在 **2~4** 之间。  
  - 兼顾了效率和跨领域泛化能力。  

<mark>核心：OOV 用更细粒度切分兜底；压缩率是效率和泛化的折中指标，实际通过子词级分词控制在中间范围。</mark>


### Q. 为什么有的模型词表大小是 5 万，而有的是 10 万？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [Tokenizer, 词表大小, 压缩率]

- **词表大小的决定因素**  
  1. **训练语料规模和多样性**：  
     - 多语言/跨领域 → 需要更大词表覆盖更多 token。  
     - 单一语言/领域 → 较小词表即可。  
  2. **压缩率需求**：  
     - 大词表 → 高频词更可能整体成 token，序列更短（高压缩率）。  
     - 小词表 → 更多回退到子词/字节，序列更长（低压缩率）。  
  3. **显存和效率**：  
     - 词表越大，embedding 层和 softmax 层参数量越大，推理/训练显存开销更高。  
     - 词表较小则 embedding 层更轻，但序列更长，计算也会增加。  
  4. **目标任务**：  
     - **英文类模型**（如 GPT 系列）通常 50k 左右就够。  
     - **中英文/多语种模型**（如 LLaMA、Qwen）往往 100k 甚至更多，保证覆盖。  

- **5 万 vs 10 万的取舍**  
  - **5 万词表**：  
    - 优点：embedding 层更小，推理/存储开销低。  
    - 缺点：更多拆分为子词 → 序列更长。  
  - **10 万词表**：  
    - 优点：压缩率更高，序列更短，吞吐率更好。  
    - 缺点：embedding 层大，显存占用高。  

- **折中原则**  
  - 工程上一般选 **3万–10万** 之间，取决于：  
    - **目标语言数**（单语/多语）。  
    - **硬件资源**（是否能承受更大的 embedding）。  
    - **目标任务**（更看重效率还是更看重泛化）。  

<mark>核心：词表大小是压缩率与显存开销的折中结果。5 万更轻，10 万更紧凑；取决于语料覆盖需求和硬件约束。</mark>

### Q. 有3个binary的特征和3个连续特征，怎么做tokenizer？
**Company**: 途游游戏 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-03-12 ｜ **Tags**: [Tokenizer, 特征工程, 离散化]

**1. 背景问题**  
- 在构建 LLM 或 RAG 系统时，非文本特征（如 binary、数值型特征）也需要转化为“token-like” 表示，才能与序列建模架构对齐。  
- 问题：如何对 **3 个二进制特征** 和 **3 个连续特征** 做 tokenizer 处理？

**2. Binary 特征的处理方式**  
1. **直接离散化为 token**  
   - 每个 binary 特征只有 0/1 两种取值，可以映射为两个固定 token（如 `[f1_0]` / `[f1_1]`）。  
   - 最终 3 个 binary 特征共 6 个离散 token。  
2. **组合映射**  
   - 将 3 个 binary 特征拼成一个 3-bit 状态（000–111），一共 8 种组合，每种对应一个 token。  
   - 优点：表达联合特征；缺点：维度指数增长。  

**3. 连续特征的处理方式**  
1. **分桶 (Discretization/Binning)**  
   - 将连续特征按区间划分（如分为 10 桶），每个桶对应一个 token。  
   - 适合范围有限、对数值精度要求不高的场景。  
2. **量化 (Quantization)**  
   - 对连续特征进行等距/对数刻度量化，将数值映射到有限集合。  
   - 比分桶更灵活，能控制误差。  
3. **Embedding Projection**  
   - 直接将连续值通过 MLP/线性层映射到 embedding 向量，再拼接到 token embedding 序列中。  
   - 更适合深度学习场景，可保留更多精度信息。  

**4. 工程实践方案**  
- **简单方案**：  
  - Binary → 独立 0/1 token；  
  - Continuous → 分桶成 10 个离散 token。  
- **进阶方案**：  
  - Binary → 组合成一个整体 token（8 类）；  
  - Continuous → 使用小网络映射到 embedding，再作为连续特征 token 注入。  
- **选择依据**：  
  - 如果模型对结构化特征要求高 → 分桶/量化离散化。  
  - 如果模型有足够参数 → 直接用 embedding 映射更灵活。  

**5. 一句话总结**
<mark>Binary 特征可直接映射为 0/1 token 或组合编码，连续特征可分桶、量化或投影到 embedding，最终都能转化为序列 token 以供模型处理。</mark>

### Q. LLMs 训练和推理中如果词表特别大 (10w+ token), Softmax 加速通常有哪些实现？
> **Company**: 快手 ｜ **Round**: 算法工程师 二面 ｜ **Date**: 2025-09-27 ｜ **Tags**: [Tokenizer, Softmax, 加速, 大词表]

1. **层次化 Softmax（Hierarchical Softmax）**  
   将词表组织为一棵二叉树或多叉树，预测时只需沿路径计算节点概率，复杂度从 \( O(V) \) 降为 \( O(\log V) \)。

2. **采样近似 Softmax（Sampled Softmax / NCE）**  
   仅计算目标词和少量负样本的概率，用采样估计替代完整归一化，常用于训练阶段的加速。

3. **自适应 Softmax（Adaptive Softmax）**  
   将高频词与低频词分组，高频词完整计算，低频词使用分层结构，兼顾精度与效率。

4. **稀疏/低秩近似（Sparsemax, Low-rank Approximation）**  
   对输出分布或权重矩阵进行稀疏化或低秩分解，减少计算和存储开销。

5. **混合专家输出层（MoE Output / Clustered Softmax）**  
   将词表划分到不同专家中，每次仅激活部分专家，实现计算并行与稀疏更新。

6. **推理阶段优化**  
   在解码时使用 **Top-k / Top-p 采样** 仅计算概率前若干候选，或采用 **缓存 logits 重用** 及 **半精度 Softmax** 减少显存和计算量。

<mark>**一句话总结：大词表下的 Softmax 加速主要依靠分层结构、采样近似和稀疏化策略，在保证精度的同时显著降低计算复杂度。**</mark>

