# LLMs Acceleration Kits
> 本章节记录 LLMs 加速相关工具的面试题目。

## 1. DeepSpeed

### Q. DeepSpeed ZeRO 的核心机制是什么？
> **Company**: 字节topseed | **Round**: 算法工程师 一面 | **Date**: 2025-08-04 | **Tags**: [RL, GRPO, MOE]

ZeRO 的核心机制  
ZeRO（Zero Redundancy Optimizer）通过**分片 (partitioning)**，把训练三大块（参数、梯度、优化器状态）拆开分布到不同 GPU，只在需要时通信/聚合，从而大幅降低显存。

- **Stage 1 – 优化器状态分片**：Adam 的动量变量分片到各 GPU，每卡只保留一部分。  
- **Stage 2 – 梯度分片**：每卡只存自己负责参数的梯度，而不是全量梯度。  
- **Stage 3 – 参数分片**：连参数本身也分片存储，前向/反向时再 all-gather。  

📊 以 7B 模型为例的显存占用对比  
假设 7B 参数，FP16 权重（2B），Adam 优化器状态（8B/参数），8 卡训练。

| 模式                | 参数 (GB) | 梯度 (GB) | 优化器状态 (GB) | 总显存 (GB/卡) | 相对 DP |
|---------------------|-----------|-----------|-----------------|----------------|---------|
| Data Parallel (DP)  | 14        | 14        | 56              | **84**         | 1.0×    |
| ZeRO Stage 1        | 14        | 14        | 7               | **35**         | 2.4×    |
| ZeRO Stage 2        | 14        | 1.75      | 7               | **22.75**      | 3.7×    |
| ZeRO Stage 3        | 1.75      | 1.75      | 7               | **10.5**       | 8.0×    |

**直观理解**  
- **DP**：每卡都要放“全套家具”（参数+梯度+优化器状态） → 84 GB，直接爆显存。  
- **ZeRO**：把这三块“大家合租分担”，每人只放一部分 → Stage 3 只需 ~10.5 GB/卡。  


**效果**  
- 7B 模型：DP 需要每卡 84 GB，ZeRO Stage 3 只要 10.5 GB。  
- 节省近 8 倍显存，让大模型可以在 24 GB 甚至更小的 GPU 上训练。  
- 与混合精度、流水并行结合后，可扩展至万亿参数模型。

<mark>DeepSpeed ZeRO 的核心机制是“分片存储参数、梯度和优化器状态”，逐阶段把冗余消除干净，以 7B 模型为例，显存占用从 84 GB 降到 10.5 GB/卡，节省近 8 倍，使大规模训练成为可能。</mark>

## FlashAttention

