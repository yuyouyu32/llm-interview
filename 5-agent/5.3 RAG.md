# RAG (Retrieval-Augmented Generation)
> 此章节记录 RAG 相关的知识点和面试题目，RAG 是结合检索和生成的技术，提升模型回答准确性和信息量，常用于问答系统和对话系统中。


### Q. 什么是 RAG，它是怎么提升生成质量的？与传统检索 + 模型生成的流程有何不同？
> **Company**: 淘天 ｜ **Round**: Agent智能体 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [RAG, 检索增强生成]

1. **RAG 的定义**  
   - RAG（Retrieval-Augmented Generation, 检索增强生成）是 **外部知识库检索 + 大模型生成** 的一体化方法。  
   - 在生成时，模型会检索相关文档，并把它们作为条件输入拼接进 prompt，让生成“带着证据”进行。  

2. **如何提升生成质量**  
   - **减少幻觉**：答案基于外部文档，而非完全依赖模型参数中的记忆。  
   - **知识可更新**：知识库能随时扩展，不需要重新训练模型。  
   - **提升专业性**：长尾知识、领域知识（医学、金融、法律）都能被补充进生成。  

3. **与传统“检索 + 生成”流程的区别**  
   - **传统流水线**：  
     - 检索和生成是两个割裂的模块。  
     - 检索引擎返回一堆文档 → 直接拼给生成模型。  
     - 模型可能忽略文档内容，依旧依赖自身知识，结果容易幻觉。  
   - **RAG**：  
     - 检索和生成是端到端设计并训练的。  
     - 检索结果会被挑选、压缩、格式化后拼进 prompt，模型学会在生成时 **条件化在检索证据上**。  
     - 换句话说，RAG 是“证据驱动生成”，而传统方式只是“把文档扔给模型参考”。  

4. **举例**  
   - 问题：“爱因斯坦提出了什么理论？”  
     - **传统流水线**：检索返回一篇讲相对论，一篇讲量子力学 → 模型可能混着说“他提出了量子力学”。  
     - **RAG**：检索直接抽取“爱因斯坦提出相对论”的段落，拼到 prompt：  
       > **检索证据**：爱因斯坦提出相对论。  
       > **问题**：爱因斯坦提出了什么理论？  
       → 模型生成：“爱因斯坦提出了相对论。”  

<mark>传统检索 + 生成只是“文档拼接，模型不一定用”；RAG 是“端到端训练的证据驱动生成”，生成过程被检索结果强约束，更可靠。</mark>


### Q. 你是如何评估一个RAG系统是否work的？
> **Company**: 淘天 ｜ **Round**: Agent智能体 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [RAG, 评估指标]

1. **检索质量评估**  
   - **召回率 (Recall)**：能否检索到包含答案的文档。  
   - **精确率 (Precision)**：返回的文档是否相关，是否减少噪声。  
   - **Top-K 命中率 (Hit@K)**：答案是否在前 K 条检索结果中。  
   - **指标工具**：可以用标准信息检索指标 (MRR、nDCG) 来衡量。  

2. **生成质量评估**  
   - **真实性 (Faithfulness)**：生成内容是否忠实于检索文档，减少幻觉。  
   - **相关性 (Relevance)**：生成是否紧扣用户 query。  
   - **流畅性 (Fluency)**：语言是否自然，符合用户阅读习惯。  
   - **多维打分**：人工标注 + LLM-as-a-judge（比如 GPT-4/Claude 判分）。  

3. **整体系统指标**  
   - **端到端答案准确率 (Answer Accuracy)**：最终答案是否正确。  
   - **覆盖率 (Coverage)**：复杂问题是否能结合多个文档信息。  
   - **延迟与成本 (Latency & Cost)**：检索+生成的耗时、token 成本是否在可接受范围。  

4. **对比实验**  
   - **A/B Test**：RAG vs 不带检索的纯生成 → 看提升幅度。  
   - **领域 Benchmark**：如 QA 数据集（NaturalQuestions, HotpotQA）测试效果。  
   - **业务场景验证**：比如电商 Agent 能否回答“店铺优惠政策”这类长尾问题。  

<mark>评估 RAG 系统是否 work，要同时看“检索准不准 + 生成真不真 + 系统用不用得起”，最终以端到端答案准确率和用户体验为核心。</mark>



### Q. 文档问答中，如何构建索引，提升对用户问题的泛化能力？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-14 ｜ **Tags**: [RAG, 文档问答, 索引构建]


- **索引构建方式**：  
  - **分块切分**：对文档做语义合理的 chunk 切分（保持上下文完整性，避免过大/过小）。  
  - **Embedding 表征**：采用强语义覆盖的向量模型，支持跨领域语义匹配。  
  - **多粒度索引**：同时建立段落级、句子级甚至关键词级索引，保证灵活检索。  
  - **倒排+向量混合索引**：倒排保证精准匹配，向量检索保证语义泛化。  

- **提升泛化能力的策略**：  
  - **Query 扩展/改写**：利用同义词、知识库或 LLM 改写用户问题，增加召回覆盖面。  
  - **多视角表征**：为文档和 Query 生成多个 embedding（语义、关键词、实体），提升匹配鲁棒性。  
  - **跨文档上下文融合**：在召回后做 rerank 时引入跨文档关联信息。  
  - **领域自适应**：对 embedding 模型做领域微调，减少长尾和行业专有词遗漏。  

- **优化点**：  
  - **检索架构**：支持 hybrid search（BM25 + 向量检索）。  
  - **批处理 & 缓存**：高频查询缓存 embedding 和召回结果。  
  - **高效存储**：ANN 索引结构（HNSW/FAISS），兼顾精度与速度。  

<mark>核心：构建索引时要兼顾 **粒度、语义覆盖、混合检索**；提升泛化能力则依赖 **Query 扩展、多视角表征、领域自适应**；最终通过 hybrid search + 高效 ANN 索引实现召回全面且快速。</mark>

### Q. 多路检索如何实现？如何合并多路检索的结果，对它们做排序？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-14 ｜ **Tags**: [RAG, 多路检索, 结果合并]

- **多路检索实现**：  
  - **Hybrid Search**：同时使用 **向量检索**（语义匹配）和 **倒排检索**（关键词/精确匹配）。  
  - **多模态检索**：文本 + 表格 + 图片等，分别检索后合并。  
  - **多索引并行**：基于不同粒度索引（句子级、段落级、文档级）并行召回。  
  - **多模型路由**：不同领域或任务调用不同 embedding 模型，提升覆盖。

- **结果合并策略**：  
  - **分数归一化**：对不同检索通道结果的分数做归一化（如 Min-Max、Z-score）。  
  - **加权融合**：设定不同通道的权重（如向量检索权重大，倒排权重次之），线性加权。  
  - **学习排序 (Learning to Rank)**：使用训练好的排序模型，对多路结果进行 rerank。  
  - **多因素排序**：结合 BM25 分数、embedding 相似度、文档质量/新鲜度，综合打分。  

- **优化点**：  
  - **动态权重调整**：根据 Query 类型自适应调整通道权重（例：事实类问题更依赖倒排）。  
  - **多阶段排序**：先快速合并 → 再用重模型做 rerank。  
  - **结果去重/聚合**：相似内容合并，避免重复展示。  

<mark>核心：多路检索通过 **Hybrid / 多索引 / 多模型** 并行召回；合并时做 **归一化 + 加权融合 + rerank**；排序需结合语义相似度、精确匹配和文档质量，多阶段处理以兼顾召回率与精度。</mark>

### Q. Reranker 精排如何构建重排序模型的微调数据？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-14 ｜ **Tags**: [RAG, Reranker, 微调数据]

- **数据来源**：  
  - **用户交互日志**：点击/停留时长/转化率，用于构造正负样本。  
  - **人工标注**：人工判断 Query-Doc 相关性，生成高质量对照集。  
  - **弱监督/自动生成**：利用现有召回结果 + 规则或大模型判断，快速生成大规模训练数据。  
  - **知识库/FAQ**：结构化问答对转化为训练样本。  

- **数据构造方式**：  
  - **Pointwise**：Query-Doc 打分（相关/不相关，多级标签）。  
  - **Pairwise**：正负文档成对（正例 > 负例），训练排序函数。  
  - **Listwise**：给定 Query 的多个候选，优化整体排序质量（NDCG、MAP）。  
  - **难负样本挖掘**：在相似度高但不相关的样本中构造负例，提升模型判别力。  

- **优化策略**：  
  - **多样性采样**：覆盖不同类型 Query（长尾、口语化、模糊查询）。  
  - **数据增强**：同义改写、噪声注入，提升模型泛化能力。  
  - **领域自适应**：在特定领域（电商、金融等）做再标注或再采样。  
  - **在线反馈闭环**：利用在线点击/用户反馈不断更新微调数据。  

<mark>核心：Reranker 微调数据可来自 **交互日志、人工标注、弱监督生成**；构造方式有 **Pointwise / Pairwise / Listwise**，并通过 **难负样本、数据增强、领域适配** 提升判别力和泛化能力。</mark>

### Q. Reranker 一般是句对打分的回归吗？负样本具体怎么用于训练？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-14 ｜ **Tags**: [RAG, Reranker, 回归, 负样本]

- **模型形态（推理阶段）**  
  - 典型 **Reranker = Cross-Encoder**：输入 `(Query, Doc)` 句对，拼接后由同一编码器计算 **实数分数 `s(q,d)`**，按分数排序。  
  - 因此在推理上看是 **“句对打分的回归/打分任务”**（点式评分，最后按分数降序）。

- **训练目标（如何把负样本用进去）**  
  - **Pointwise 回归/分类**（最贴近你说的“给 AB 两句话打分”）：  
    - 标注相关度 `r ∈ {0..4}` 或二元标签；损失用 **MSE**（回归到 `r` 或归一化 `r/4`），或 **BCE/Logistic**（`sigmoid(s)` 逼近相关概率）。  
    - **负样本**直接记为低分/0 分，参与同一损失。  
  - **Pairwise 排序**（训练用对比，推理仍点式打分）：  
    - 三元组 `(q, d⁺, d⁻)`，优化 **`s(q,d⁺) > s(q,d⁻)`**，常用 **Margin Ranking/Hinge**：`max(0, m - s⁺ + s⁻)`。  
    - 通过 **难负样本** 放大梯度，学到更清晰的判别边界。  
  - **Listwise / InfoNCE（批内对比）**（训练更高效，推理仍点式）：  
    - 一个 `q` 搭配 `1 正 + K 负`，对相似度向量做 **Softmax 交叉熵**，或用 **InfoNCE**。  
    - 批内文档互为 **in-batch negatives**，无需额外采样；**难负样本**效果更佳。

- **负样本构造与采样策略**  
  - **来源**：BM25/向量检索 TopK 未点击、编辑生成的“近义但不相关”、历史错匹配、跨域干扰项。  
  - **类型**：  
    - **随机负**（易）：收敛稳但提升有限。  
    - **难负**（Hard）：与正样本相似但不相关，强提升判别力。  
    - **半难负**（Semi-hard）：介于两者之间，更稳定。  
  - **采样/挖掘**：  
    - **动态难负挖掘**（periodic hard mining）：用当前模型从召回库再挖最混淆的负例。  
    - **批内对比**：N 个 query × (1 正 + K 负)，共享一轮前向，训练高效。  
    - **比例**：常见 **1:4 ~ 1:16（正:负）**，结合课程学习（由易→难）。  
    - **去噪**：点击数据用 **倾向校正/IPS**、阈值过滤，避免“伪负/伪正”。

- **训练细节与工程实践**  
  - **标签与分布**：分级相关度做 **归一化** 或 **温度缩放**；BCE 用 **label smoothing**（例如负样本 0.05）抗噪。  
  - **稳定性**：梯度裁剪、混合精度、冻结词嵌入前几轮、学习率分段/余弦退火。  
  - **蒸馏**：用 **强 Cross-Encoder** 产生成对分数，蒸馏给 **轻量模型**（保持低延迟）。  
  - **评测指标**：NDCG@k、MRR、Recall@k（离线），上线做 A/B（CR、CTR、首答时延）。  
  - **融合打分**：`S_final = λ · s_rerank + (1-λ) · s_retriever`，对不同 query 类型做 **动态 λ**（事实类↑检索权重）。

<mark>核心：Reranker 推理确实是“句对打分”的点式回归/打分；但训练时应把 **负样本**系统性引入（Pointwise=低分，Pairwise=成对对比，Listwise/InfoNCE=批内对比），优先使用 **难负样本 + 动态挖掘 + in-batch negatives**，既保证判别力又兼顾效率与稳定性。</mark>