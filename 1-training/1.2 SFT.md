# LLM SFT Stage
> 本章节记录 LLMs SFT训练相关工具的面试题目。

## 1. PEFT (Parameter-Efficient Fine-Tuning)

### Q. Prompt Tuning、Prefix Tuning、LoRA 和 Adapter，结合实际场景谈优劣
> **Company**: 美团 | **Round**: 大模型算法工程师 一面 | **Date**: 2025-08-31 | **Tags**: [Prompt Tuning, Prefix Tuning, LoRA, Adapter, PEFT, 参数高效微调]

参考链接：https://zhuanlan.zhihu.com/p/675231376
**Prompt Tuning**  
- **方法**：在输入端拼接可训练的虚拟 prompt embedding，引导模型完成特定任务。  
- **优点**：参数量极小（通常 <0.1%），训练与部署成本低，适合**大规模任务快速适配**。  
- **缺点**：表达能力有限，对复杂任务或跨领域泛化能力不足。  
- **场景**：在搜索、广告等需要对**成千上万小任务**快速适配时常用。  

**Prefix Tuning**  
- **方法**：在 Transformer 每层的 Key/Value 前加一组可训练前缀向量，相当于在注意力机制里“插队”影响注意力分布。  
- **优点**：能直接作用在注意力计算，建模能力更强，比 Prompt Tuning 更能控制模型的生成。  
- **缺点**：参数量和推理开销比 Prompt Tuning 略大。  
- **场景**：适合**需要更强控制力的生成类任务**，如对话系统、代码生成。  

**Adapter**  
- **方法**：在 Transformer 每层插入小型瓶颈网络（降维→非线性→升维），训练这些额外层参数。  
- **优点**：模块化设计，可在同一模型里加载多个 Adapter，快速切换任务；训练稳定性高。  
- **缺点**：推理时需要额外前向计算，会增加延迟；参数量通常**比 Prefix Tuning 大**，但比 LoRA 小。  
- **场景**：常用于**多任务和跨领域应用**，如一个基础模型服务多个业务，只需切换不同的 Adapter 模块即可。  

**LoRA (Low-Rank Adaptation)**  
- **方法**：对权重矩阵进行低秩分解，仅训练低秩部分，并与原权重相加。  
- **优点**：性能接近全参数微调，适合复杂下游任务；可结合 SFT/DPO 等训练方法。  
- **缺点**：显存和推理开销比 Prompt/Prefix 大，需要管理多个权重矩阵。  
- **场景**：常用于**工业级场景**，如金融、医疗、代码生成。  

**Adapter vs Prefix Tuning 的区别**  
- **作用位置不同**：  
  - Prefix Tuning 只影响注意力层（Key/Value 输入），属于“修改注意力输入”。  
  - Adapter 是独立的小层，插在 Transformer 层内部，属于“增加新的计算模块”。  
- **参数规模**：  
  - Prefix 参数较少，更轻量；  
  - Adapter 参数较多，但能单独保存/切换，支持多任务管理。  
- **推理影响**：  
  - Prefix 几乎不增加额外计算，推理更快；  
  - Adapter 会增加一部分计算，延迟更高。  

<mark>**总结**：Prompt Tuning 极简轻量；Prefix Tuning 强调注意力控制；LoRA 兼顾性能与效率，是工业落地主流；Adapter 模块化强，适合多任务/跨领域切换。Prefix 是“加前缀影响注意力”，Adapter 是“插模块影响表示”，两者定位不同。</mark>



### Q. LORA的原理
> **Company**: 阿里国际 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01 ｜ **Tags**: [LORA, PEFT, 参数高效微调]

LORA 的核心思想是：**把大模型的权重更新限制在一个低秩矩阵上**。  
具体来说，大模型权重 $W$ 不直接更新，而是写成：  
$$
W' = W + \Delta W,\quad \Delta W = A B
$$
其中 $A \in \mathbb{R}^{d \times r}$，$B \in \mathbb{R}^{r \times k}$，$r \ll d,k$。  
这样只需训练 $A, B$ 两个低秩矩阵，参数量比全量微调小几个数量级，同时能保留大模型原有能力。

### Q. 啥是矩阵的秩？秩和LORA有啥关系？
> **Company**: 快手 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-01 ｜ **Tags**: [LORA, PEFT, 参数高效微调]

**1. 矩阵的秩**  
- **定义**：矩阵的秩（Rank）是矩阵中 **线性无关行向量或列向量的最大个数**。  
- **直观理解**：秩代表了矩阵中包含多少“有效信息”或“自由度”。  
- **性质**：  
  - 秩 = 行空间/列空间的维度。  
  - 秩 ≤ min(行数, 列数)。  
  - 秩越大，矩阵能表示的线性组合越丰富。  

**2. 秩与 LORA 的关系**  
- **LORA (Low-Rank Adaptation)**  
  - 对预训练模型中的大权重矩阵 \(W \in \mathbb{R}^{d \times k}\)，冻结其原始参数。  
  - 只在训练时引入一个 **低秩分解**：  
    \[
    W' = W + A B
    \]
    其中 \(A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{r \times k}\)，且 \(r \ll \min(d, k)\)。  
  - 本质：用两个低秩矩阵的乘积近似一个高秩更新，从而减少训练参数量。  

**3. 为什么能行？**  
- 自然语言任务的参数更新通常存在 **低秩结构**（即有效更新空间远小于权重矩阵本身的维度）。  
- LORA 通过低秩分解捕捉主要变化方向，实现 **参数高效微调 (PEFT)**。  
- 秩 \(r\) 越小，训练参数更少，但表达能力可能不足；\(r\) 越大，表达能力更强，但开销增加。  

<mark>核心：矩阵的秩表示矩阵的自由度，LORA 就是利用低秩分解来高效近似权重更新，从而在保持性能的同时大幅减少可训练参数。</mark>


### Q. LORA矩阵是如何初始化的？
> **Company**: 阿里国际 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01 ｜ **Tags**: [LORA, PEFT, 参数高效微调]

- 一般做法是：  
  - **$A$**：用高斯/均匀分布随机初始化；  
  - **$B$**：初始化为 **全零**。  
- 这样一开始 $\Delta W = A B = 0$，相当于模型初始输出与原模型完全一致，不会被低秩部分扰动。  
- 随着训练进行，$B$ 学到方向，$A$ 负责缩放，逐步逼近合适的更新。

**为什么这样设计：**
  1. **保证稳定性**：一开始不引入扰动，避免模型性能骤降。  
  2. **$B$ 学方向**：由于 $B=0$，训练时小的梯度更新能立即决定 $\Delta W$ 的方向，因此 $B$ 扮演“方向学习”的角色。  
  3. **$A$ 负责缩放**：$A$ 是随机低秩基底，对 $B$ 学到的方向进行线性投影和缩放，相当于“放大/缩小器”。  
  4. **梯度直观解释**：当 $B=0$ 时，$\nabla_A \Delta W = 0$，而 $\nabla_B \Delta W \neq 0$，因此更新首先发生在 $B$，方向由 $B$ 决定，$A$ 再对其进行缩放。

<mark>在 LORA 中，$B$ 决定低秩更新的方向，$A$ 提供缩放与投影；这种初始化方式既保证初始等价于原模型，又能让训练快速收敛。</mark>


### Q. QLORA相对于LORA改进是什么？
> **Company**: 阿里国际 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-01 ｜ **Tags**: [QLORA, LORA, PEFT, 参数高效微调]

QLORA 可以理解为 **“量化后的 LORA”**，在保持 LORA 参数高效微调优势的同时，进一步通过 **4-bit 量化** 来降低显存占用。核心改进有三个方面：

1. **4-bit 量化（NF4 格式）**  
   - 普通 LORA 需要加载原始 FP16 权重（7B 模型 ≈ 14GB）。  
   - QLORA 将预训练权重压缩为 **4-bit (NF4)**，大小直接缩小 **4 倍**。  
   - NF4（Normalized Float 4）是一种专门设计的 4 位量化格式，比普通 int4 更精确。  
   - **好处**：显存消耗骤降，可以在单张 24GB GPU 上跑 65B 模型。

2. **LORA 适配器保持 FP16/FP32 训练**  
   - 在量化的模型上插入 LORA 低秩矩阵，训练时只更新这部分参数（依旧是少量）。  
   - 主模型权重保持 4-bit 存储，但在计算时动态反量化为更高精度，保证训练精度。  
   - **好处**：微调精度接近全精度微调。

3. **工程优化：Double Quantization + Paged Optimizer**  
   - **Double Quantization**：连量化参数（scaling factors）也再次量化，进一步压缩存储。  
   - **Paged Optimizer**：像操作系统内存分页一样，优化器状态按需加载，避免一次性占满显存。  
   - **好处**：减少峰值显存，避免 OOM，支持大模型在消费级 GPU 上训练。

**通俗总结：**  
- **LORA**：只训练小矩阵，参数量省，但模型本身还是 FP16 → 显存大。  
- **QLORA**：先把大模型权重压缩到 **4-bit**，再加 LORA 适配器 → 显存小 + 参数省。  

**一句话**：  
<mark>LORA 解决了“要训练的参数太多”的问题，而 QLORA 进一步解决了“模型本身太占显存”的问题，使得在消费级显卡上也能微调超大规模模型。</mark>

### Q. 为什么在参数高效微调中使用低秩矩阵分解？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-14 ｜ **Tags**: [LORA, PEFT, 参数高效微调]

- **背景**：  
  - 大模型参数量巨大，完整微调成本极高（显存/存储/训练时间）。  
  - 参数高效微调（PEFT）方法目标是在保持性能的同时，减少需要更新的参数量。  

- **低秩矩阵分解的思想（LoRA 为例）**：  
  - 假设原始权重矩阵 $W \in \mathbb{R}^{d \times k}$。  
  - LoRA 冻结 $W$，只学习一个低秩分解矩阵 $W_{update} = A B$，其中 $A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{r \times k}$，$r \ll \min(d, k)$。  
  - 训练时仅更新 $A, B$，推理时实际用 $W + A B$。  

- **为什么有效**：  
  1. **低秩近似假设**：大模型在特定任务上的参数更新通常处于一个低秩子空间，不需要完整自由度。  
  2. **显著降参**：只需训练 $O(r(d+k))$ 个参数，远小于 $O(dk)$。  
  3. **可叠加/可组合**：不同任务的低秩更新矩阵可以叠加，便于多任务和增量学习。  
  4. **效率与存储**：大幅减少显存开销与存储需求，训练快、部署轻。  

- **对比全量微调**：  
  - 全量微调：更新全部参数，开销大，不易迁移。  
  - 低秩分解：冻结大部分参数，仅在低秩子空间做任务适配，保持了基础模型能力。  

<mark>核心：低秩矩阵分解利用“大模型参数更新处于低秩子空间”的规律，把高维参数变化压缩到低秩矩阵中，从而大幅减少训练参数量和显存开销，实现高效微调。</mark>


### Q. MLA 和 LoRA 有哪些相似之处？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [MLA, LoRA, 参数高效]

- **MLA (Multi-Head Latent Attention)**  
  - 在注意力中引入 **低维潜在空间 (latent)**，将高维的 Q/K/V 投影到一个较小的维度，再恢复回去。  
  - 本质：利用 **低秩表示/压缩** 来减少存储和计算。  

- **LoRA (Low-Rank Adaptation)**  
  - 在参数高效微调中，将权重更新矩阵分解为 **低秩矩阵 $A \cdot B$**。  
  - 本质：假设参数更新分布在低秩子空间，通过低秩分解减少训练参数。  

- **相似之处**  
  1. **低秩思想**：二者都利用低秩近似，把高维映射压缩到低秩空间，再还原。  
  2. **减少计算/存储**：MLA 降低推理的 KV 存储和计算；LoRA 降低训练参数量和显存。  
  3. **保持模型性能**：都尝试在减少开销的同时，尽量保留模型表达能力。  
  4. **可叠加/可组合**：二者都能与现有 Transformer 架构结合，不改变主干结构。  

- **差异点**  
  - **应用阶段**：  
    - MLA → 主要用于 **推理加速、显存压缩**。  
    - LoRA → 主要用于 **训练微调、参数高效适配**。  
  - **优化目标**：  
    - MLA → 通过 latent 压缩减少 KV Cache 存储。  
    - LoRA → 通过低秩分解减少更新参数量。  

<mark>核心：MLA 和 LoRA 的共通点是都基于“低秩近似”思想，目标是**在低秩子空间中高效表示**，前者偏向推理优化，后者偏向训练微调。</mark>



## Full-Size Fine-Tuning

### Q. 简要介绍一下 SFT的核心流程，以及数据集的构建策略，SFT之后常见的 Post-Training 还有哪些？它们之间的目的有何区别？
> **Company**: 淘天 ｜ **Round**: Agent智能体 一面 ｜ **Date**: 2025-08-26 ｜ **Tags**: [SFT, 数据集, Post-Training]

1. **SFT 核心流程**  
   - **目标**：让大模型学会“遵循指令、生成合理回答”。  
   - **步骤**：  
     1. 收集 **高质量指令–回答对 (prompt–response)**。  
     2. 将这些数据转成监督学习格式：输入为 prompt，输出为目标答案。  
     3. 在预训练模型上微调，优化交叉熵损失，使模型在给定 prompt 时尽可能生成目标答案。  

2. **数据集构建策略**  
   - **多样性**：覆盖问答、对话、代码、推理等多任务，避免过拟合单一场景。  
   - **高质量**：人工标注 + LLM 生成 + 自动清洗结合，保证回答准确性与安全性。  
   - **平衡性**：控制不同任务和领域的比例，避免模型偏向某类指令。  
   - **难度递进**：可以采用 Curriculum Learning，从简单问答到复杂推理逐步训练。  

3. **SFT 之后的常见 Post-Training**  
   - **RLHF (Reinforcement Learning with Human Feedback)**  
     - 通过人类偏好排序训练奖励模型，再用 PPO 等方法优化。  
     - 目标：提升输出的 **对齐性**（礼貌、安全、偏好一致）。  
   - **DPO (Direct Preference Optimization)**  
     - 直接基于偏好数据优化，不需要训练奖励模型。  
     - 目标：简化流程，稳定高效地对齐人类偏好。  
   - **RLAIF (RL with AI Feedback)**  
     - 由另一个强模型提供偏好反馈，降低人工成本。  
   - **对比学习 / 约束优化**  
     - 如使用规则、知识库、稀疏奖励，引导模型在特定维度（事实性、安全性、推理链）更优。

4. **目的区别**  
   - **SFT**：让模型“听懂指令”，具备基础可用性。  
   - **RLHF / DPO 等 Post-Training**：让模型“更符合人类偏好”，解决安全性、礼貌性、稳定性等问题。  
   - **类 RL 方法 vs 对比学习方法**：前者通过奖励信号强化，后者直接基于 pairwise 偏好优化。  

<mark>SFT 教会模型“照着做”，Post-Training 则让模型“做得更符合人类想要的样子” or 更强的推理能力？</mark>

### Q. 指令微调（Instruct-tuning） 和 提示学习（Prompting）的区别是什么？
> **Company**: 阿里 | **Round**: 算法工程师 一面 ｜ **Date**: 2025-08-14 ｜ **Tags**: [指令微调, 提示学习, LLM]

- **提示学习（Prompting）**：  
  - **定义**：直接在推理时，通过人为设计 prompt，引导大模型完成任务。  
  - **特点**：  
    - 无需修改模型参数，仅依赖输入文本。  
    - 灵活，适合零样本/小样本（Zero-shot/Few-shot）。  
    - 效果受 prompt 工程影响大，稳定性差。  

- **指令微调（Instruct-tuning）**：  
  - **定义**：在预训练模型上，使用人工编写或合成的大规模 **指令-响应对** 数据集，进行监督微调。  
  - **特点**：  
    - 改变模型参数，使模型内化“理解指令→执行”的能力。  
    - 在真实任务中更稳健，对 prompt 设计依赖小。  
    - 数据与训练成本较高。  

- **核心区别**：  
  1. **是否修改参数**：Prompting 不改参数；Instruct-tuning 会更新模型参数。  
  2. **效果稳定性**：Prompting 效果依赖 prompt 工程；Instruct-tuning 更鲁棒。  
  3. **适用场景**：Prompting 更快更便宜；Instruct-tuning 更适合部署和大规模应用。  

<mark>核心：Prompting = “靠输入提示驱动”；Instruct-tuning = “靠微调内化指令能力”。前者轻量灵活但不稳定，后者成本更高但效果更稳健。</mark>

### Q. LLM 训练中遇到 Loss 突增如何解决？
> **Company**: 同花顺 ｜ **Round**: 大模型算法工程师 一面 ｜ **Date**: 2025-04-15 ｜ **Tags**: [训练稳定性, Loss 突增]

**1. 学习率问题**  
- **现象**：过大学习率往往导致 Loss 突然飙升甚至 NaN。  
- **解决**：  
  - 使用学习率 warmup（逐步升高到设定值）。  
  - 尝试调小初始学习率或使用余弦退火等动态调度策略。  

**2. 数值稳定性问题**  
- **现象**：Loss 变为 NaN 或梯度爆炸。  
- **解决**：  
  - 使用 **梯度裁剪 (Gradient Clipping)**，限制梯度范数。  
  - 启用 **混合精度 (FP16/BF16)** 时检查溢出，可用 `GradScaler`。  
  - 确保 log/softmax/除法等操作避免数值溢出。  

**3. 数据问题**  
- **现象**：部分 batch 输入异常，导致 Loss 异常跳高。  
- **解决**：  
  - 检查数据清洗，避免异常 token / 长度过长序列。  
  - 使用 batch 正则化（padding/truncation 一致化）。  
  - 开启数据校验，丢弃异常 batch。  

**4. 模型与优化器设置**  
- **现象**：优化器状态不稳定或初始化不合理。  
- **解决**：  
  - 更换优化器（如 AdamW → Lion/Adafactor）。  
  - 检查权重初始化，避免方差过大。  
  - 考虑更稳定的归一化方式（RMSNorm 替代 LayerNorm）。  

**5. 分布式与硬件问题**  
- **现象**：Loss 在多卡训练时突然异常，但单卡正常。  
- **解决**：  
  - 检查 NCCL/通信设置，确保梯度同步无误。  
  - 确认不同 GPU/TPU 上 batch size、seed 一致。  

**调试与排查流程**  
1. **先降学习率**，看 Loss 是否稳定。  
2. **开启梯度裁剪与混合精度溢出检测**。  
3. **检查数据 pipeline**，确保 batch 输入合法。  
4. **缩小模型/数据规模** 做 sanity check，排除硬件或实现问题。  

<mark>**总结**：LLM 训练中 Loss 突增常由学习率过大、数值溢出、数据异常或分布式训练问题引起。常见解法是调学习率 + 梯度裁剪 + 数据清洗，并结合日志与断点排查定位问题。</mark>
