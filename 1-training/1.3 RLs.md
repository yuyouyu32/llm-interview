# Reinforcement Learning (RLs) in LLMs

> 本章节记录与 **大语言模型 (LLMs)** 相关的强化学习方法，不仅包含常见的 **PPO** 等标准强化学习算法，还涵盖一些 **类强化学习 (RL-like)** 的方法，例如 **Direct Preference Optimization (DPO)**、**Rejection Sampling Fine-Tuning (RFT)**等。这些方法虽然未必严格遵循传统的 RL 框架，但在实践中承担了类似的“基于偏好/反馈优化模型”的角色，因此一并整理在此。


## 1. [RLHF](https://arxiv.org/abs/2204.05862) (Reinforcement Learning from Human Feedback)

### Q. RLHF的完整流程有哪些？
> **Company**: None | **Round**: None | **Date**: None | **Tags**: [RL, RLHF]

1. Step 0（可选）：SFT 作为起点
- **做什么**：用指令数据对基座模型做一次 **监督微调（SFT）**。
- **怎么做**：`(prompt, response*)` 的有监督交叉熵训练，得到 **参考策略** π_ref（也可直接拷贝 SFT 权重作为 π_ref）。
- **为什么**：把模型先对齐到“能按指令作答”的分布，降低后续 RL 的探索难度与不稳定性。
2. Step 1：从当前策略采样多候选响应（数据收集）
- **做什么**：对每个 prompt \(x\) 从策略 π_θ 采样 K 个响应 \(y_{1..K}\)。
- **怎么做**：
  - 采样策略：温度/Top-p/Top-k，限制最大长度与停用词；去重（哈希/近似去重）。
  - 过滤：简单安全/质量规则（脏词、模板化、空答），保留可分辨质量的候选。
  - 记录元信息：log-prob、长度、采样参数，以便后续分析 length bias 和 KL。
- **为什么**：必须有“好坏对比”的候选，后续才能产生偏好信号。

> **常见坑**：全部“很差”或“很像”的候选会削弱后续信号。

3. Step 2：人类偏好标注/排序（偏好蒸馏）
- **做什么**：对同一 prompt 下的候选按质量 **成对比较或排序**：$(y^+ \succ y^-)$ 或 $(y_{(1)} \succ y_{(2)} \succ \dots)$。
- **怎么做**：
  - **Pairwise** 是主流：更稳、更一致；也可用 **k-wise 排序**（Plackett–Luce）。
  - 制定**评分准则**（有用/无害/真实/风格），做标注前校准标注者；允许“并列/无法比较”以减少噪声。
  - 可用 **AI-feedback（RLAIF）** 或 规则库辅助，降低人工成本（注意引入系统性偏差）。
- **为什么**：把人类偏好转为可学习信号，是后续训练奖励模型（RM）的依据。

> **常见坑**：标签噪声、长度偏好、领域偏置；需抽样复核与一致性检查（如 κ 系数）。

4. Step 3：训练奖励模型（Reward Model, RM）
- **做什么**：学一个打分函数 $(r_\phi(x, y) \in \mathbb{R})$，拟合“哪个回答更好”的偏好。
- **怎么做**（以 pairwise 为例）：
  - **Bradley–Terry 损失**  
    $$
    \mathcal{L}_{\text{RM}} = - \mathbb{E}_{(x, y^+, y^-)}
    \big[\log \sigma\big(r_\phi(x, y^+) - r_\phi(x, y^-)\big)\big]
    $$
  - 模型结构：复用基座 Transformer，取 **EOS 向量/mean-pool** 过一个线性头输出标量。
  - 正则化：长度惩罚/奖励归一化，dropout，权重衰减；监控 **pairwise-acc/AUC**。
- **为什么**：RM 提供可微分的“人类偏好代理”，供 RL 优化目标使用。

> **常见坑**：RM **过拟合**导致“奖励黑客”（policy 专门投机 RM 的缺陷）；需做独立验证与漂移监控。

5. Step 4：基于 RM 的策略优化（PPO 等）
- **做什么**：用 RL 优化策略 π_θ，使其在 RM 下得分更高，同时**约束**不偏离 π_ref。
- **怎么做**：
  - 目标函数（KL-正则化）：
    $$
    \max_{\theta}\ \mathbb{E}_{x,y\sim \pi_\theta}
    \big[r_\phi(x,y)\big]\ -\ \beta\, \mathrm{KL}\!\left(\pi_\theta(\cdot|x)\ \Vert\ \pi_{\text{ref}}(\cdot|x)\right)
    $$
  - **奖励塑形**（token-level）：把序列奖励分摊到 token，并将 KL 惩罚写成逐 token 的“惩罚项”，结合 **GAE** 计算 advantage。
  - **PPO 细节**：剪切比值 $\$( \text{clip}(\cdot, 1\pm\epsilon))$、多 epoch 小批量更新、value/entropy 系数调参；on-policy 采样与 **新鲜轨迹**。
  - **迭代**：收集 → 更新 → 评估；必要时回到 Step 1/2 扩充数据，或重训 RM。
- **为什么**：在“对齐人类偏好”的同时保持语言分布稳定（KL 约束防止退化或风格飘移）。

> **常见坑**：
>- **KL 系数 β 失衡**：β 太小→发散/风格塌陷；太大→进步有限。  
>- **探索不足/过早收敛**：采样温度、长度上限、奖励归一化需要联动调。  
>- **训练不稳定**：on-policy 噪声大；可用 **replay mixing/渐进 β** 缓解。

<mark>**Note**: 1) 采样多候选 → 2) 偏好标注 → 3) 训练 RM（pairwise BT 损失） →  4) RL（PPO：最大化 RM 奖励 + KL 约束对 π_ref 的偏移） →  5) 评测/回流/再训练（必要时重训 RM & 扩充偏好数据）。</mark>


### Q. xxx



## 2. [DPO]() (Direct Preference Optimization)



## 3. [RFT]() (Rejection Sampling Fine-Tuning)



## 4. [GRPO]((https://arxiv.org/pdf/2402.03300)) (Group Relative Policy Optimization)
图片：
![GRPO](./imgs/GRPO.png)

### Q. GRPO 是什么？它与 PPO 有何区别？
> **Company**: None | **Round**: None | **Date**: None | **Tags**: [RL, GRPO, PPO]

**GRPO（Group Relative Policy Optimization）** 是一种用于大语言模型 (LLMs) 对齐训练的强化学习变体。  
它的核心思想是：**不再依赖价值函数 (Critic) 来估计优势，而是通过同一提示下多条响应的组内相对比较来计算 advantage**。  
这使得算法结构更简洁，避免了训练一个额外的价值模型。

**GRPO 的工作流程**：

1. **采样组响应**，给定提示 $p$，从当前策略 $\pi_\theta$ 采样一组 $N$ 条响应  $G=\{r_1, r_2, \dots, r_N\}$。

2. **奖励打分**，用奖励模型 or 规则奖励 $R_\phi$ 为每条响应打分：  


3. **组内归一化优势（代替 Critic）**，对每条响应计算 advantage（相对于组均值的标准化奖励）：
$$
A_i = \frac{R_\phi(r_i) - \mathrm{mean}(G)}{\mathrm{std}(G)}\,,
$$
其中 $\mathrm{mean}(G)$、$\mathrm{std}(G)$ 为该组奖励的均值与标准差。

4. **策略更新（剪裁替代损失 + KL 约束）**，使用与 PPO 相同的剪裁替代损失：
$$
L_{\text{clip}}(\theta) = \frac{1}{N}\sum_{i=1}^{N}
\min\!\left(
\frac{\pi_\theta(r_i\mid p)}{\pi_{\theta_{\text{old}}}(r_i\mid p)}A_i,\;
\operatorname{clip}\!\left(
\frac{\pi_\theta(r_i\mid p)}{\pi_{\theta_{\text{old}}}(r_i\mid p)},
1-\varepsilon,\,1+\varepsilon
\right)A_i
\right).
$$
再加入 KL 惩罚（约束与初始/参考策略的偏移）得到 GRPO 目标：
$$
L_{\text{GRPO}}(\theta)=L_{\text{clip}}(\theta)-w_1\, D_{\mathrm{KL}}\!\big(\pi_\theta \,\|\, \pi_{\mathrm{orig}}\big).
$$

> 注：GRPO 通常**不再显式使用熵奖励**，因为同一提示下的组内多样化采样已带来探索性。

**与 PPO 的对比表格**：
| 维度 | PPO | GRPO |
|---|---|---|
| **优势估计 (Advantage)** | 依赖 **Critic/Value**，可逐 token/状态做 credit assignment | **组内相对标准化奖励**（sequence-level），无需 Critic |
| **额外网络** | 需要训练 Value/Critic（Actor–Critic） | 不需要，结构更简洁 |
| **训练信号** | 序列奖励需 reward shaping 到 token，并配合 Critic 稳定学习 | 直接用组均值/方差作基线，避免 reward shaping |
| **Advantage 粒度** | per-token advantage（依赖 Critic 做 baseline） | per-sequence advantage（基于组比较） |
| **探索机制** | 依赖熵奖励 + on-policy 采样 | 组采样本身促进多样性，常不再加熵奖励 |
| **优点** | 信号细粒度，长序列 credit assignment 更稳定 | 计算/实现更简单，资源友好、扩展性好 |
| **常见缺点** | 训练/部署开销较大 | 信号更粗糙；同组“全优/全劣”会无梯度；存在 length bias 等 |

- **PPO**：像家长实时给每次作业打分（需要持续的价值估计，开销大但细粒度）。  
- **GRPO**：像让学生做多份模拟卷，再把平均分作为“基线”，如果超过平均分就奖励，否则惩罚。省去了家长逐次批改的麻烦，但评估更粗糙。  

<mark>**Note**：**GRPO** 用“组内相对奖励”替代 Critic 的优势估计，结构更**简单高效**；  
**PPO** 通过 Critic 提供细粒度的价值基线，通常在**长序列与稳定性**上更有优势；  
实践中可根据资源与任务选择：若追求训练简洁/吞吐，GRPO 友好；若在复杂对话/长 horizon 任务中要求稳定性，PPO 往往更稳。<mark>

***

### Q. GRPO 的主要贡献是什么？它在 RL 训练中解决了哪些问题？

> **Company**: None | **Round**: None | **Date**: None | **Tags**: [RL, GRPO]

**主要贡献**  
- 提出了一种 **无需 Critic 的 Advantage 估计方法**，用组内相对奖励归一化代替 Value Function。  
- 将复杂的 **Actor–Critic 框架**简化为仅依赖奖励模型 (RM) 的结构，降低了训练与计算成本。  
- 保持了与 PPO 相似的目标函数（clip + KL 惩罚），因此实现上兼容，迁移容易。

**解决的问题**  
1. **计算资源消耗高**：去掉 Critic，减少额外模型训练与显存占用。  
2. **训练不稳定**：通过组内归一化，天然引入基线，降低了方差。  
3. **探索效率低**：多样化组采样（同一提示多响应）在一定程度上代替了熵奖励，促进探索。 

<mark>**Note**：GRPO 通过组内相对奖励归一化，简化了 RL 训练流程，降低了计算开销，同时保持了与 PPO 相似的目标函数结构。解决了 Critic 训练开销大、训练不稳定、探索效率低等问题。<mark>


***

### Q. GRPO 的缺陷是什么？

> **Company**: 百度 | **Round**: 暑期二面 | **Date**: 2025-04-15 | **Tags**: [RL, GRPO]

GRPO（Group Relative Policy Optimization）通过在同一 group 内比较样本相对好坏来计算 advantage，从而省去了 Critic 模型，降低了计算开销。但这种设计也带来了以下缺陷：  
1. **缺少 Critic（Value Function） → 优化信号粗糙**  
   - **原因**：GRPO 不引入显式的 Value/Critic 网络，只依赖组内样本的相对得分来计算 advantage。  
   - **后果**：  
     - 优化信号只能在 **sequence-level** 粒度上提供，而缺少 per-token 的 credit assignment。  
     - 模型在长序列上难以判断“哪一部分 token 导致回答好/坏”，更新更盲目，训练稳定性下降。  
2. **全对或全错的 group → 无梯度**  
   - **原因**：GRPO 的梯度完全依赖样本间的相对差异。如果一个 group 中的候选答案质量完全相同（全对或全错），则样本间没有区分度。  
   - **后果**：  
     - 此时 advantage = 0，无法反向传播。  
     - 等于浪费了一整个 group 的训练样本，降低了数据利用效率。  
3. **长度偏差（Length Bias） → 更新方向被扭曲**  
   - **原因**：在比较时，长答案更容易积累 reward 的波动（可能更高也可能更低），而 GRPO 缺少 Critic 去对长度进行归一化或基线修正。  
   - **后果**：  
     - 长序列的样本可能被系统性偏好或惩罚，产生 **length bias**。  
     - PPO 的 Critic 至少能部分减弱这个偏差，而 GRPO 完全暴露在这个问题下。  
4. **优化依赖组内相对差异 → 对噪声敏感**  
   - **原因**：GRPO 的训练信号完全基于同组样本的对比，如果 prompt 或样本分布含噪声，就可能错误地放大某些无关特征。  
   - **后果**：  
     - 更新方向可能受 group 内异常值影响，导致训练不稳定。  
     - 当样本质量差异很小，梯度信号也会变得极弱。  
5. **缺乏长期 credit assignment 能力 → 难以扩展到多轮任务**  
   - **原因**：GRPO 不建模 token-level 或 step-level 的价值传播，只能在单个回答序列上做整体比较。  
   - **后果**：  
     - 更适合单轮推理/数学问题等短序列场景。  
     - 对需要多轮对话、长 horizon 的 Agent 任务不够适用。

<mark>**Note**：GRPO 的优势是 **简洁高效**（去掉 Critic，降低内存与算力），但缺点在于 **优化信号过粗、梯度利用不足、对噪声敏感、存在长度偏差**。<mark>

## 5. [GAPO]() (Group Advantage Policy Optimization)