# Pre-Training
> 本章节记录大模型预训练的面试题目。

### Q. LLMs Pre-Training 与 SFT 的区别？
> **Company**: 快手 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-05 ｜ **Tags**: [LLMs, Pre-Training, SFT]

**1. Pre-Training（预训练）**  
- **目标**：让模型学习通用的语言知识与语义表示。  
- **数据**：规模极大（TB 级），多为无标注的通用语料（网页、书籍、维基百科等）。  
- **任务**：语言建模（LM）任务，例如自回归预测下一个 token，或掩码预测（MLM）。  
- **结果**：得到一个具备通用语言理解与生成能力的基础模型。  
- **特点**：  
  - 数据量最大，训练成本最高。  
  - 模型学到的是“通用知识”，但缺乏任务/指令对齐。  

**2. SFT（Supervised Fine-Tuning，监督微调）**  
- **目标**：让模型在特定任务或指令场景中表现更符合需求。  
- **数据**：人工构造或半自动生成的 **高质量指令-响应对**。  
- **任务**：在给定输入指令下生成符合人类期望的输出。  
- **结果**：模型学会遵循指令、提升对话可控性。  
- **特点**：  
  - 数据规模较小，但质量高。  
  - 偏重“对齐”，而非通用知识学习。  

**3. 区别总结**  
| 维度 | Pre-Training | SFT |
|------|--------------|-----|
| 数据 | 海量无标注语料 | 高质量指令-响应对 |
| 目标 | 学习通用语言建模能力 | 学习任务/指令对齐能力 |
| 成本 | 高（万亿 token 级） | 低（百万级样本即可） |
| 输出 | 基础大语言模型 | 可遵循指令的对齐模型 |

<mark>核心：Pre-Training 注重“通用语言能力”，SFT 注重“任务与指令对齐”，二者结合让 LLM 既有知识储备又能听懂人话。</mark>

### Q. LLMs Pre-Training 与 SFT 他们在训练本质上有差异吗？
> **Company**: 快手 ｜ **Round**: 算法工程师 一面 ｜ **Date**: 2025-09-05 ｜ **Tags**: [LLMs, Pre-Training, SFT]

**1. 训练目标差异**  
- **Pre-Training**：核心是 **语言建模目标**，通常是自回归的下一个 token 预测 (Next Token Prediction) 或 MLM。  

$$
\mathcal{L}_{\text{pretrain}} = - \sum_{t} \log P_\theta(x_t \mid x_{<t})
$$  

- **SFT**：本质还是 **条件语言建模**，但数据来自人工构造的 **指令-响应对**。  

$$
\mathcal{L}_{\text{SFT}} = - \sum_{t} \log P_\theta(y_t \mid x, y_{<t})
$$  

其中 \(x\) 是指令，\(y\) 是期望响应。  

**2. 数据分布差异**  
- Pre-Training：无标注的自然文本 → 分布广，覆盖知识面大。  
- SFT：高质量人工标注数据 → 分布集中，更贴近下游任务与人类偏好。  

**3. 参数更新本质**  
- 两者在 **优化算法**（如 AdamW）、**损失函数形式**（交叉熵）上几乎一致。  
- 本质区别：**训练数据的来源与目标函数的条件约束不同**。  

**4. 差异总结**  
| 维度 | Pre-Training | SFT |
|------|--------------|-----|
| 优化目标 | 语言建模（预测下一个 token） | 条件语言建模（预测响应） |
| 数据 | 海量无标注 | 少量高质量指令对 |
| 本质差异 | 学语言分布 | 学任务/对齐 |

<mark>核心：在训练机制上 Pre-Training 和 SFT 都是基于交叉熵的语言建模，差异不在优化方式，而在 **数据分布与训练目标的语义约束**。</mark>
